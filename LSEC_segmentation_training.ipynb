{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/LSEC_segmentation_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for training a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECS)."
      ],
      "metadata": {
        "id": "-aHjwiD8IkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "J-Z80u6TN3Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a network, first connect to a GPU (**Runtime -> Change runtime time -> Hardware accelerator -> GPU**).\n",
        "\n",
        "If you are using a pretrained network for inference and not training, being connected only to a **CPU** is slower, but possible."
      ],
      "metadata": {
        "id": "NUZeORlUN_LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "-gq1-hflPdMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "LHteKyDySYvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5QvbqMfiA4o"
      },
      "outputs": [],
      "source": [
        "!pip install wandb\n",
        "# !pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import os\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Probably no point in running this, if the gpu is not connected\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# **2. Utils**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Data utils**\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask /= 255\n",
        "        mask[mask == 255.0] = 1\n",
        "\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def normalize_hist(img):\n",
        "    clahe = cv.createCLAHE(10, tileGridSize=(11, 11))\n",
        "    img = clahe.apply(img)\n",
        "    img = cv.medianBlur(img, 3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_loaders(img_train, mask_train, img_val, mask_val, batch_size, num_workers=0, pin_memory=True):\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_data = MyDataset(\n",
        "        image_dir=img_train,\n",
        "        mask_dir=mask_train,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_data = MyDataset(\n",
        "        image_dir=img_val,\n",
        "        mask_dir=mask_val,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Affine(scale=(0.9, 1.1)),\n",
        "            A.Normalize(\n",
        "                mean = 0.5,\n",
        "                std = 0.5,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.5,\n",
        "                std = 0.5,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "        mean = 0.5,\n",
        "        std = 0.5,\n",
        "        max_pixel_value=255.0,\n",
        "        ),\n",
        "            ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    print('merging masks')\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - (border_size+bottom_edge):height, :])\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - (border_size+right_edge):width])\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path):\n",
        "    window_size = 224\n",
        "    oh, ow = 20, 20\n",
        "\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    weighting_window = top_left\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            square_section = normalize_hist(square_section)\n",
        "            square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch and channel dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    threshold = int(255*0.4)\n",
        "    output_mask = np.where(output_probs > threshold, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    return output_mask\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = normalize_hist(image)\n",
        "    return image\n",
        "\n",
        "def create_train_val_patches(train_image_folder, train_mask_folder, val_image_folder, val_mask_folder, output_folder, patch_size, reduction):\n",
        "    train_image_patches_path, train_mask_patches_path = create_image_patches(train_image_folder, train_mask_folder, output_folder, patch_size, reduction, img_type='train', )\n",
        "    val_image_patches_path, val_mask_patches_path = create_image_patches(val_image_folder, val_mask_folder, output_folder, patch_size, reduction, img_type='val')\n",
        "    return train_image_patches_path, train_mask_patches_path, val_image_patches_path, val_mask_patches_path\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size, reduction, img_type):\n",
        "    image_patches_path = os.path.join(output_folder, img_type+'_image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, img_type+'_mask_patches')\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "        k = 0\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                if k % reduction == 0: # this reduces the number of patches if the training takes too long(1 for no reduction)\n",
        "                    img_patch = img_strided[i, j]\n",
        "                    mask_patch = mask_strided[i, j]\n",
        "                    # Compute the percentage of white pixels\n",
        "                    fenestration_area = np.sum(mask_patch == 255)\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                k += 1\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    num_ellipses = 0\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "                num_ellipses += 1\n",
        "            else:\n",
        "                ellipses.append((None, None, None))\n",
        "        else:\n",
        "            ellipses.append((None, None, None))\n",
        "    # print(len(filtered_contours), len(ellipses))\n",
        "    return ellipses, num_ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Training utils**\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_dice_score = 0.0\n",
        "    total_samples = 0\n",
        "    eps = 1e-8\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            out = model(x)\n",
        "            loss = get_loss(out, y, loss_fn)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            if WANDB_CONNECTED:\n",
        "                wandb.log({\"val/batch loss\": loss.item()})\n",
        "\n",
        "            predicted_probs = torch.sigmoid(out)\n",
        "            predicted = (predicted_probs > 0.5).float()\n",
        "            intersection = torch.sum(predicted * y)\n",
        "            dice_score = (2.0 * intersection + eps) / (torch.sum(predicted) + torch.sum(y) + eps)\n",
        "            total_dice_score += dice_score.item() * x.size(0)\n",
        "\n",
        "            total_samples += x.size(0)\n",
        "    model.train()\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    average_dice_score = total_dice_score / total_samples\n",
        "\n",
        "    return average_loss, average_dice_score\n",
        "\n",
        "\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def build_optimizer(model, config, beta1=None, beta2=None):\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=config.learning_rate,\n",
        "                              weight_decay=config.weight_decay,\n",
        "                              momentum=config.momentum)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=config.learning_rate,\n",
        "                            #    betas=(config.beta1, config.beta2),\n",
        "                               weight_decay=config.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def build_dataloaders(config):\n",
        "    train_image_patches_path = config.train_image_patches_path\n",
        "    train_mask_patches_path = config.train_mask_patches_path\n",
        "    val_image_patches_path = config.val_image_patches_path\n",
        "    val_mask_patches_path = config.val_mask_patches_path\n",
        "\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        train_image_patches_path,\n",
        "        train_mask_patches_path,\n",
        "        val_image_patches_path,\n",
        "        val_mask_patches_path,\n",
        "        config.batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return train_loader, val_loader # this is the simplest way to do it, wandb train cannot take any arguments\n",
        "\n",
        "class EarlyStopper():\n",
        "    def __init__(self, patience):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > self.min_validation_loss:\n",
        "            self.counter += 1\n",
        "            print(self.counter)\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "    losses = []\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = get_loss(predictions, targets, loss_fn)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "        if WANDB_CONNECTED:\n",
        "            wandb.log({\"train/batch loss\": loss.item()})\n",
        "\n",
        "    mean_loss = total_loss / total_samples\n",
        "    model.eval()\n",
        "    return mean_loss\n",
        "\n",
        "def build_model(model_name):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    if '+' in model_name:\n",
        "        name_parts = model_name.split('+')\n",
        "        encoder = name_parts[-2]\n",
        "        if name_parts[-1] == 'imagenet' or name_parts[-1] == 'ssl':\n",
        "            weights = name_parts[-1]\n",
        "        else:\n",
        "            weights = None\n",
        "    out_activation = None\n",
        "\n",
        "    if 'Unet++' in model_name:\n",
        "        model = smp.UnetPlusPlus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'Linknet' in model_name:\n",
        "        model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'FPN' in model_name:\n",
        "        model = smp.FPN(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'DeepLabV3' in model_name:\n",
        "        model = smp.DeepLabV3(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    else:\n",
        "        model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def get_loss(pred, target, func_name):\n",
        "    loss_func = None\n",
        "    if func_name == 'dice':\n",
        "        loss_func = smp.losses.DiceLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'jaccard':\n",
        "        loss_func = smp.losses.JaccardLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'weighted_bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'focal':\n",
        "        loss_func = smp.losses.FocalLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'dice+bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == '5dice+95bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.05*loss1 + 0.95*loss2\n",
        "    elif func_name == '20dice+80bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.2*loss1 + 0.8*loss2\n",
        "    elif func_name == 'dice+focal':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = smp.losses.FocalLoss(mode='binary')\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == 'tversky':\n",
        "        loss_func = smp.losses.TverskyLoss(mode='binary', alpha=0.7, beta=0.3)\n",
        "        loss = loss_func(pred, target)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, val_loader = build_dataloaders(config)\n",
        "        model = build_model(config.model_type)\n",
        "        optimizer = build_optimizer(model, config)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "        # best_dice_score = 0\n",
        "        smallest_val_loss = 1000.0\n",
        "        early_stopper = EarlyStopper(patience=10)\n",
        "        for epoch in range(config.epochs):\n",
        "            print(f'Epoch {epoch}')\n",
        "            avg_loss = train_epoch(model, train_loader, optimizer, config.loss_function)#, loss_fn)\n",
        "            metrics = {\"train/loss\": avg_loss, \"train/epoch\": epoch}\n",
        "            val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if early_stopper.early_stop(val_loss):\n",
        "                print(f\"early stop on epoch {epoch}\")\n",
        "                with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                    file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "                break\n",
        "\n",
        "            if val_loss < smallest_val_loss:\n",
        "                torch.save(model.state_dict(), os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "            smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "            val_metrics = {\"val/val_loss\": val_loss,\n",
        "                           \"val/dice_score\": dice_score}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "class DictObject:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "def train(config, loaded_model=None):\n",
        "    if WANDB_CONNECTED:\n",
        "        wandb.init(\n",
        "            project=\"LSEC_segmentation\",\n",
        "            config=config)\n",
        "    config = DictObject(**config)\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "    if loaded_model is None:\n",
        "        model = build_model(config.encoder+'+'+config.model)\n",
        "    else:\n",
        "        model = loaded_model\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    smallest_val_loss = 1000.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "    early_stopper = EarlyStopper(patience=10)\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.loss_function)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "        scheduler.step(val_loss)\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(\"Current learning rate:\", current_lr)\n",
        "\n",
        "        if early_stopper.early_stop(val_loss):\n",
        "            print(f\"early stop on epoch {epoch}\")\n",
        "            with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "            break\n",
        "        if val_loss < smallest_val_loss:\n",
        "            print(config.model_path)\n",
        "            torch.save(model.state_dict(), config.model_path)\n",
        "        smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "        dice_scores.append(dice_score)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Dice score: {round(dice_score, 2)}')\n",
        "        # view_prediction(val_loader, model, device = DEVICE)\n",
        "        print(f'train loss: {train_loss}, val loss: {val_loss}')\n",
        "        if WANDB_CONNECTED:\n",
        "            wandb.log({\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": epoch,\n",
        "                       \"val/val_loss\": val_loss,\n",
        "                       \"val/dice_score\":dice_score,\n",
        "                       })\n",
        "    if WANDB_CONNECTED:\n",
        "        wandb.finish()\n",
        "\n",
        "    return train_losses, val_losses, dice_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Insert training images**"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "#@markdown If you want to create new 224x224 patches, check the following box.\n",
        "#@markdown If you already have image patches, insert the folders below, uncheck the box, and leave output_patches_folder_empty.\n",
        "create_patches = True # @param {type:\"boolean\"}\n",
        "\n",
        "# training_images = './gdrive/MyDrive/lsecs/masks_added_small/train_images' #@param {type:\"string\"}\n",
        "# training_masks = './gdrive/MyDrive/lsecs/masks_added_small/train_masks' #@param {type:\"string\"}\n",
        "# validation_images = './gdrive/MyDrive/lsecs/masks_added_small/val_images' #@param {type:\"string\"}\n",
        "# validation_masks = './gdrive/MyDrive/lsecs/masks_added_small/val_masks' #@param {type:\"string\"}\n",
        "training_images = './gdrive/MyDrive/lsecs/cropped_cells/train_images' #@param {type:\"string\"}\n",
        "training_masks = './gdrive/MyDrive/lsecs/cropped_cells/train_masks' #@param {type:\"string\"}\n",
        "validation_images = './gdrive/MyDrive/lsecs/cropped_cells/val_images' #@param {type:\"string\"}\n",
        "validation_masks = './gdrive/MyDrive/lsecs/cropped_cells/val_masks' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "output_patches_folder = './gdrive/MyDrive/lsecs/cropped_cells/patches' #@param {type:\"string\"}\n",
        "#@markdown This can be used to reduce the number of patches when making them (1 means no reduction, 2 means half the number of patches will be saved...)\n",
        "reduction_rate = 2 # @param {type:\"number\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "training_masks = training_masks.strip()\n",
        "validation_images = validation_images.strip()\n",
        "validation_masks = validation_masks.strip()\n",
        "\n",
        "output_patches_folder = output_patches_folder.strip()\n",
        "\n",
        "if not os.path.exists(training_images):\n",
        "    print(f'{training_images} does not exist.')\n",
        "if not os.path.exists(training_masks):\n",
        "    print(f'{training_masks} does not exist.')\n",
        "if not os.path.exists(validation_images):\n",
        "    print(f'{validation_images} does not exist.')\n",
        "if not os.path.exists(validation_masks):\n",
        "    print(f'{validation_masks} does not exist.')\n",
        "\n",
        "SAVE_PATCHES_TO_DISK = True\n",
        "patch_size = 224\n",
        "\n",
        "if create_patches:\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        # output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "        print(f'Saving patches to {output_patches_folder}')\n",
        "    else:\n",
        "        output_patches_folder = os.getcwd()\n",
        "    train_img_patches_path, train_mask_patches_path, val_img_patches_path, val_mask_patches_path = create_train_val_patches(training_images, training_masks, validation_images, validation_masks, output_patches_folder, patch_size, reduction_rate)\n",
        "else: # The patches will be read from disk\n",
        "    train_img_patches_path = training_images\n",
        "    train_mask_patches_path = training_masks\n",
        "    val_img_patches_path = validation_images\n",
        "    val_mask_patches_path = validation_masks\n",
        "\n",
        "print(f'Training image patches are located in {train_img_patches_path}, {len(os.listdir(train_img_patches_path))} patches.')\n",
        "print(f'Training mask patches are located in {train_mask_patches_path}')\n",
        "print(f'Validation image patches are located in {val_img_patches_path}, {len(os.listdir(val_img_patches_path))} patches.')\n",
        "print(f'Validation mask patches are located in {val_mask_patches_path}')"
      ],
      "metadata": {
        "id": "4qR6zmj4U-pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "KPwZ2wIG8htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown You can load model weights and retrain them, otherwise ImageNet weights will be loaded.\n",
        "load_model = False # @param {type:\"boolean\"}\n",
        "model_path = './gdrive/MyDrive/lsecs/model_weights.pth' #@param {type:\"string\"}\n",
        "#@markdown Check, if you want to use wandb for training evaluation:\n",
        "use_wandb = False # @param {type:\"boolean\"}\n",
        "WANDB_CONNECTED = use_wandb\n",
        "#@markdown Where to save the trained model:\n",
        "out_model_path = './gdrive/MyDrive/lsecs/new_model_weights.pth' #@param {type:\"string\"}\n",
        "\n",
        "config = {\n",
        "    'num_epochs' : 2,\n",
        "    'model': 'unet',\n",
        "    'loss_function': 'bce',\n",
        "    'encoder': 'resnet34',\n",
        "    'batch_size' : 32,\n",
        "    'optimizer' : 'sgd',\n",
        "    'learning_rate' : 0.04,\n",
        "    'weight_decay' : 0.01,\n",
        "    'momentum' : 0.07,\n",
        "    'train_image_patches_path': train_img_patches_path,\n",
        "    'train_mask_patches_path': train_mask_patches_path,\n",
        "    'val_image_patches_path': val_img_patches_path,\n",
        "    'val_mask_patches_path': val_mask_patches_path,\n",
        "    'model_path':out_model_path,\n",
        "}\n",
        "\n",
        "if load_model:\n",
        "    model = build_model('resnet34+none')\n",
        "    loaded_state_dict = torch.load(model_path)\n",
        "    model.load_state_dict(loaded_state_dict)\n",
        "    model.eval()\n",
        "else:\n",
        "    model = None\n",
        "train_losses, val_losses, dice_scores = train(config, model)"
      ],
      "metadata": {
        "id": "-0Al6T1fdX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# 4. **Training log**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En"
      },
      "outputs": [],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "WAJp45Xo8p_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This can be used to train multiple networks in one run\n",
        "\n",
        "# Choose training parameters\n",
        "output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "\n",
        "# wandb sweep config\n",
        "sweep_config = {\n",
        "    'method': 'grid'#'grid'#\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'val/dice_score',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        # 'values': ['adam', 'sgd']\n",
        "        'value': 'sgd'\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'value': 0.04,\n",
        "        # a flat distribution between min and max\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.001,\n",
        "        # 'max': 0.01\n",
        "      },\n",
        "    'weight_decay': {\n",
        "        # 'value': 0.0189,\n",
        "        'value': 0.01\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max' : 0.02,\n",
        "    },\n",
        "    # sgd parameters\n",
        "    'momentum':{\n",
        "        'value': 0.07,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.06,\n",
        "        # 'max' : 0.09,\n",
        "    },\n",
        "\n",
        "    # 'dropout': {\n",
        "    #     'value': 0.5,\n",
        "    #     #   'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    #     },\n",
        "    'epochs': {\n",
        "        'value': 1,\n",
        "        },\n",
        "\n",
        "    # Dataloader params\n",
        "    'train_image_patches_path': {\n",
        "        'value': train_img_patches_path\n",
        "        },\n",
        "    'train_mask_patches_path': {\n",
        "        'value': train_mask_patches_path\n",
        "        },\n",
        "    'val_image_patches_path': {\n",
        "        'value': val_img_patches_path\n",
        "        },\n",
        "    'val_mask_patches_path': {\n",
        "        'value': val_mask_patches_path\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'value': 32,\n",
        "        # # integers between min and max\n",
        "        # # with evenly-distributed logarithms\n",
        "        # 'distribution': 'q_log_uniform_values',\n",
        "        # 'q': 2, # the discrete step of the distribution\n",
        "        # 'min': 4,\n",
        "        # 'max': 8,\n",
        "      },\n",
        "    # Adam parameters\n",
        "    # 'beta1': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "    # 'beta2': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "        # 'fc_layer_size': {\n",
        "    #     'values': [128, 256, 512]\n",
        "    #     },\n",
        "    'image_denoising_methods': {\n",
        "        'value': '11_10_indiv',\n",
        "        # 'values': ['no_denoise', 'med5']\n",
        "        # 'values': ['nlm', 'med5']\n",
        "        # 'values': ['clahe+median5', 'med7', 'median5', 'median5+clahe', 'wave1_5+med3', 'wave2_5', 'wave2_5+med5'],#['wavelet', 'wavelet+median', 'advanced median'] # k waveletu jeste pridat ruzne thresholdy\n",
        "    },\n",
        "    'loss_function':{\n",
        "        # 'value': 'bcelog',\n",
        "        # 'values': ['dice', 'dice+bce', 'dice+focal', 'tversky'],#['dice', 'bcelog', 'jaccard', 'weighted_bce', 'focal'],#, 'tversky', 'hausdorff']\n",
        "        # 'values': ['dice', 'dice+bce', 'focal','bcelog', 'dice+focal'],\n",
        "        'values': ['bce'],\n",
        "\n",
        "\n",
        "    },\n",
        "    'model_type':{\n",
        "        # 'values': ['plain_unet', 'resnet34+imagenet', 'resnet50+imagenet', 'inceptionv4+imagenet', 'efficientnet-b7+imagenet', 'resnet18+swsl', 'resnet18+imagenet','vgg11+imagenet'],\n",
        "        # 'values': ['vgg11+imagenet', 'vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl','resnet34+imagenet','resnet50+ssl', 'resnext50_32x4d+ssl'],\n",
        "        # 'values': ['vgg11+imagenet','vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl',  'resnet34+imagenet','resnet50+ssl', 'efficientnet-b7+imagenet'],\n",
        "        # 'value': 'vgg11+imagenet',\n",
        "        # 'values':['vgg11+imagenet','vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', 'efficientnet-b7+imagenet'],\n",
        "        # 'values':['vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', [\n",
        "        'values': ['resnet34+imagenet']\n",
        "\n",
        "    },\n",
        "    'model_path':{\n",
        "        'value': './gdrive/MyDrive/lsecs',\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LSEC_segmentation\")\n",
        "\n",
        "WANDB_CONNECTED = True\n",
        "wandb.agent(sweep_id, wandb_train, count=510)"
      ],
      "metadata": {
        "id": "Y851Q6gvcd8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Inference evaluation**"
      ],
      "metadata": {
        "id": "RxiOKWQb6aFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert folders with cell images and their ground truth masks for comparison:\n",
        "images_path = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "ground_truth_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/ground_truth_masks_1205' #@param {type:\"string\"}\n",
        "semiauto_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks' #@param {type:\"string\"}\n",
        "\n",
        "models_path = './gdrive/MyDrive/lsecs/model_weights/model_weights.pth' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "\n",
        "# filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "\n",
        "remove_false_fenestrations = True # @param {type:\"boolean\"}\n",
        "pixel_size_nm = 9.28 #@param {type:\"number\"}\n",
        "min_diameter_nm = 50 #@param {type:\"number\"}\n",
        "max_diameter_nm = 350 #@param {type:\"number\"}\n",
        "min_roundness = 0.4 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "log_file_path = './gdrive/MyDrive/lsecs/dice_score_test/log.txt'\n",
        "\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "import scipy.stats\n",
        "\n",
        "ground_truth_mask_folder = ground_truth_mask_folder.strip()\n",
        "images_path = images_path.strip()\n",
        "models_path = models_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "semiauto_mask_folder = semiauto_mask_folder.strip()\n",
        "\n",
        "\n",
        "# model_names = sorted([f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f)) and 'pt' in f])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "# print(model_names)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(images_path):\n",
        "    print(\"Images folder does not exist\")\n",
        "    # exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    print(\"Folder with ground truth masks does not exist\")\n",
        "    # exit()\n",
        "\n",
        "ground_truth_images = sorted([f for f in os.listdir(ground_truth_mask_folder) if os.path.isfile(os.path.join(ground_truth_mask_folder, f))])\n",
        "images = sorted([f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))])\n",
        "semiauto_images = sorted([f for f in os.listdir(semiauto_mask_folder) if os.path.isfile(os.path.join(semiauto_mask_folder, f))])\n",
        "\n",
        "if len(ground_truth_images) != len(images) or len(images) != len(semiauto_images):\n",
        "    print('The number of ground truths and images differs.')\n",
        "\n",
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "def remove_fenestrations(mask, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse != (None, None, None) and ellipse is not None:\n",
        "            center, axes, _ = ellipse\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length:\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "                if (diameter < min_d or diameter > max_d) or  (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    mask = remove_contour_from_mask(contour, mask)\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "            else:\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    return mask\n",
        "\n",
        "def compute_dice_score(image1, image2):\n",
        "    eps = 1e-8\n",
        "    image1[image1 == 255] = 1\n",
        "    image2[image2 == 255] = 1\n",
        "    intersection_sum = np.logical_and(image1, image2).sum()\n",
        "    dice_score = (2*intersection_sum+eps)/(image1.sum() + image2.sum() + eps)\n",
        "    return dice_score\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "# dice_scores = []\n",
        "# with open(log_file_path, \"a+\") as file:\n",
        "#     file.write(f'{len(images)} images\\n')\n",
        "#     for ground_truth_mask_name, image_name, cell_name  in zip(ground_truth_images, images, cells):\n",
        "#         print(f'Compare: {ground_truth_mask_name} - {image_name} - {cell_name}')\n",
        "#         file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "#         ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "#         image_path = os.path.join(images_path, image_name)\n",
        "#         cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "#         cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "#         ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask[cell == 0] = 0\n",
        "#         current_dice_score = compute_dice_score(ground_truth_mask, image_mask)\n",
        "#         print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "#         file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "#         dice_scores.append(current_dice_score)\n",
        "\n",
        "#     dice_scores = np.array(dice_scores)\n",
        "#     mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "#     std_dice = round(np.std(dice_scores)*100, 1)\n",
        "\n",
        "#     print(f'Semiautomatic Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "#     file.write(f'Semiautomatic Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "\n",
        "def get_fenestrations_from_image(mask):\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    # fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    return ellipses, num_ellipses, contours, len(contours)\n",
        "\n",
        "def get_image_stats(contours, ellipses, pixel_size_nm, min_d=0, max_d=100000, min_roundness=0):\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None and ellipse != (None, None, None):\n",
        "            # print(ellipse)\n",
        "            center, axes, _ = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length: # The fitting algorithm can fail sometimes\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                diameter_pix = equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "                diameter = pixel_size_nm * diameter_pix\n",
        "                # print(contour)\n",
        "                # print(diameter)\n",
        "                if (diameter < min_d or diameter > max_d) or (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    # mask = remove_contour_from_mask(contour, mask)\n",
        "                    continue\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter_pix**2)/4*math.pi)\n",
        "    return equivalent_diameters, roundness_of_ellipses, fenestration_areas_from_ellipses\n",
        "\n",
        "import seaborn as sns\n",
        "# Get stats\n",
        "\n",
        "save_plots = False\n",
        "save_image_masks = False\n",
        "plot_path = './gdrive/MyDrive/lsecs/plots/'\n",
        "\n",
        "# for model_name in model_names:\n",
        "all_my_diameters = []\n",
        "all_gt_diameters = []\n",
        "all_s_diameters = []\n",
        "\n",
        "all_my_means = []\n",
        "all_gt_means = []\n",
        "all_s_means = []\n",
        "\n",
        "all_my_roundness = []\n",
        "all_gt_roundness = []\n",
        "all_s_roundness = []\n",
        "\n",
        "num_all_my_ellipses = []\n",
        "num_all_gt_ellipses = []\n",
        "num_all_s_ellipses = []\n",
        "\n",
        "my_dice_scores = []\n",
        "s_dice_scores = []\n",
        "\n",
        "all_gt_porosities_pix = []\n",
        "all_s_porosities_pix = []\n",
        "all_my_porosities_pix = []\n",
        "\n",
        "all_gt_porosities_ell = []\n",
        "all_s_porosities_ell = []\n",
        "all_my_porosities_ell = []\n",
        "\n",
        "all_gt_frequencies = []\n",
        "all_s_frequencies = []\n",
        "all_my_frequencies = []\n",
        "\n",
        "# file.write(f'{model_name}\\n')\n",
        "# print(model_name)\n",
        "model = build_model('resnet34+none')\n",
        "loaded_state_dict = torch.load(models_path) # TODO:these models do not include sigmoid and preprocessing yet\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "# dice_scores = []\n",
        "# dice_scores_filt = []\n",
        "for ground_truth_mask_name, image_name, semiauto_image_name, cell_name  in zip(ground_truth_images, images, semiauto_images, cells):\n",
        "    print(f'Compare: {ground_truth_mask_name} - {image_name} - {semiauto_image_name} -{cell_name}')\n",
        "    ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "    image_path = os.path.join(images_path, image_name)\n",
        "    semiauto_image_path = os.path.join(semiauto_mask_folder, semiauto_image_name)\n",
        "    cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "    ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    semiauto_mask = cv.imread(semiauto_image_path, cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "    new_mask = inference_on_image_with_overlap(model, image_path)\n",
        "\n",
        "    new_mask[cell == 0] = 0\n",
        "    semiauto_mask[cell == 0] = 0\n",
        "    ground_truth_mask[cell == 0] = 0\n",
        "\n",
        "\n",
        "    # I need to fill the cell nucleus to compute the area\n",
        "    contours, hierarchy = cv.findContours(\n",
        "        cell, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Get the largest object in the image(the cell)\n",
        "    empty_mask = np.zeros_like(cell)\n",
        "    areas = []\n",
        "    for cnt in contours:\n",
        "        area = cv.contourArea(cnt)\n",
        "        areas.append(area)\n",
        "    areas = np.array(areas)\n",
        "    max_area_idx = np.argmax(areas)\n",
        "\n",
        "    # Fill this object and compute its area\n",
        "    c = 0\n",
        "    for cnt in contours:\n",
        "        if c == max_area_idx:\n",
        "            cv.drawContours(empty_mask, [cnt], -1, 1, thickness=cv.FILLED)\n",
        "            cell_contour, hierarchy = cv.findContours(\n",
        "                empty_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "        c += 1\n",
        "    cell_area = float(np.sum(empty_mask))\n",
        "\n",
        "\n",
        "    # Remove fenestrations\n",
        "    new_mask_filt = remove_fenestrations(new_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    s_mask_filt = remove_fenestrations(semiauto_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    gt_mask_filt = remove_fenestrations(ground_truth_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "\n",
        "    if save_image_masks:\n",
        "        cv.imwrite(os.path.join(plot_path, image_name), new_mask_filt)\n",
        "\n",
        "    # Save filtered masks\n",
        "        merge = np.zeros((new_mask_filt.shape[0], new_mask_filt.shape[1], 3))\n",
        "        merge = merge.astype('uint8')\n",
        "        merge[:, :, 2][gt_mask_filt == 255.0] = 255 #R\n",
        "        merge2 = merge.copy()\n",
        "        merge3 = merge.copy()\n",
        "        merge[:, :, 0][s_mask_filt == 255.0] = 255 #B\n",
        "        merge[:, :, 1][s_mask_filt == 255.0] = 100 #G, so the blue is more visible\n",
        "\n",
        "        merge[:, :, 1][(s_mask_filt == 255.0) & (gt_mask_filt == 255.0)] = 255 #G\n",
        "\n",
        "        cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare_semiautomatic\"+'.png'), merge)\n",
        "\n",
        "        merge2[:, :, 1][new_mask_filt == 255.0] = 255 #G\n",
        "        merge2[:, :, 0][(new_mask_filt == 255.0) & (gt_mask_filt == 255.0)] = 255 #B\n",
        "        cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare_automatic\"+'.png'), merge2)\n",
        "\n",
        "        # merge3[:, :, 0][s_mask_filt == 255.0] = 255 #B\n",
        "        # merge3[:, :, 1][s_mask_filt == 255.0] = 100 #G, so the blue is more visible\n",
        "        # merge3[:, :, 1][new_mask_filt == 255.0] = 255 #G\n",
        "        # cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare\"+'.png'), merge3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    gt_cell_area_pix = np.sum(gt_mask_filt/255)\n",
        "    s_cell_area_pix = np.sum(s_mask_filt/255)\n",
        "    my_cell_area_pix = np.sum(new_mask_filt/255)\n",
        "\n",
        "    gt_fen_area = float(gt_cell_area_pix)\n",
        "    s_fen_area = float(s_cell_area_pix)\n",
        "    my_fen_area = float(my_cell_area_pix)\n",
        "\n",
        "    gt_porosity_pix = round(gt_fen_area/cell_area*100, 1)\n",
        "    s_porosity_pix = round(s_fen_area/cell_area*100, 1)\n",
        "    my_porosity_pix = round(my_fen_area/cell_area*100, 1)\n",
        "\n",
        "    all_gt_porosities_pix.append(gt_porosity_pix)\n",
        "    all_s_porosities_pix.append(s_porosity_pix)\n",
        "    all_my_porosities_pix.append(my_porosity_pix)\n",
        "\n",
        "    # print(f'gt: {gt_porosity_pix}, s: {s_porosity_pix}, my: {my_porosity_pix}')\n",
        "\n",
        "\n",
        "\n",
        "    # Compute Dice scores\n",
        "    my_current_dice = compute_dice_score(gt_mask_filt, new_mask_filt)\n",
        "    s_current_dice = compute_dice_score(gt_mask_filt, s_mask_filt)\n",
        "\n",
        "    my_dice_scores.append(my_current_dice)\n",
        "    s_dice_scores.append(s_current_dice)\n",
        "    # print(f'Image Dice score: {round(my_current_dice*100, 1)}')\n",
        "    #\n",
        "\n",
        "    # my data\n",
        "    my_ellipses, num_my_ellipses, my_objects, num_all_my_objects = get_fenestrations_from_image(new_mask)\n",
        "    my_equivalent_diameters, my_roundness_of_ellipses, my_fenestration_areas_from_ellipses = get_image_stats(my_objects, my_ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "    my_ell_area = np.sum(np.array(my_fenestration_areas_from_ellipses))\n",
        "    all_my_porosities_ell.append(round(my_ell_area/cell_area*100, 1))\n",
        "    # ground truth\n",
        "    ellipses, num_ellipses, objects, num_all_objects = get_fenestrations_from_image(ground_truth_mask)\n",
        "    equivalent_diameters, roundness_of_ellipses, fenestration_areas_from_ellipses = get_image_stats(objects, ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "    gt_ell_area = np.sum(np.array(fenestration_areas_from_ellipses))\n",
        "    all_gt_porosities_ell.append(round(gt_ell_area/cell_area*100, 1))\n",
        "    # semiautomatic_data\n",
        "    s_ellipses, s_num_ellipses, s_objects, s_num_all_objects = get_fenestrations_from_image(semiauto_mask)\n",
        "    s_equivalent_diameters, s_roundness_of_ellipses, s_fenestration_areas_from_ellipses = get_image_stats(s_objects, s_ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "    s_ell_area = np.sum(np.array(s_fenestration_areas_from_ellipses))\n",
        "    all_s_porosities_ell.append(round(s_ell_area/cell_area*100, 1))\n",
        "\n",
        "    all_my_diameters.extend(my_equivalent_diameters)\n",
        "    all_gt_diameters.extend(equivalent_diameters)\n",
        "    all_s_diameters.extend(s_equivalent_diameters)\n",
        "\n",
        "    all_my_roundness.extend(my_roundness_of_ellipses)\n",
        "    all_gt_roundness.extend(roundness_of_ellipses)\n",
        "    all_s_roundness.extend(s_roundness_of_ellipses)\n",
        "\n",
        "\n",
        "    n_my_ellipses = len(my_equivalent_diameters)\n",
        "    n_s_ellipses = len(s_equivalent_diameters)\n",
        "    n_gt_ellipses = len(equivalent_diameters)\n",
        "\n",
        "    gt_freq = n_gt_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "    s_freq = n_s_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "    my_freq = n_my_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "\n",
        "    all_gt_frequencies.append(round(gt_freq, 1))\n",
        "    all_s_frequencies.append(round(s_freq, 1))\n",
        "    all_my_frequencies.append(round(my_freq, 1))\n",
        "\n",
        "    num_all_my_ellipses.append(n_my_ellipses)\n",
        "    num_all_gt_ellipses.append(n_gt_ellipses)\n",
        "    num_all_s_ellipses.append(n_s_ellipses)\n",
        "\n",
        "    gt_mean = round(np.mean(np.array(equivalent_diameters)))\n",
        "    gt_std = round(np.std(np.array(equivalent_diameters)))\n",
        "    my_mean = round(np.mean(np.array(my_equivalent_diameters)))\n",
        "    my_std = round(np.std(np.array(my_equivalent_diameters)))\n",
        "    s_mean = round(np.mean(np.array(s_equivalent_diameters)))\n",
        "    s_std = round(np.std(np.array(s_equivalent_diameters)))\n",
        "\n",
        "\n",
        "    all_my_means.append(my_mean)\n",
        "    all_gt_means.append(gt_mean)\n",
        "    all_s_means.append(s_mean)\n",
        "\n",
        "    fig, ax = plt.subplots(3, 2, figsize=(15, 23))\n",
        "    ax = ax.flatten()\n",
        "    # plt.subplots(1, 2, figsize=(15, 7))\n",
        "    sns.set_theme()  # This changes the look of plots.\n",
        "    nbins = 20\n",
        "    # Calculate density for each dataset\n",
        "    hist_x, bin_edges_x  = np.histogram(equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_y, bin_edges_y = np.histogram(my_equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_z, bin_edges_z = np.histogram(s_equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_x = hist_x*np.diff(bin_edges_x)\n",
        "    hist_y = hist_y*np.diff(bin_edges_y)\n",
        "    hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "    bin_width = (max_diameter_nm - min_diameter_nm)/3/nbins\n",
        "    shift1 = bin_width/2\n",
        "    shift2 = bin_width/2 + bin_width\n",
        "    shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "\n",
        "    ax[0].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "    ax[0].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "    ax[0].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "    # ax[0].legend([f'Ground truth, mean: {gt_mean} nm\\n{len(equivalent_diameters)} fenestrations', f'Automatic, mean: {my_mean} nm\\n{len(my_equivalent_diameters)} fenestrations', f'Semiautomatic, mean: {s_mean} nm\\n{len(s_equivalent_diameters)} fenestrations'], fontsize='small')\n",
        "    ax[0].legend([f'Ground truth\\nmean d = {gt_mean} nm', f'Automatic\\nmean d = {my_mean} nm', f'Semiautomatic\\nmean d = {s_mean} nm'], fontsize='medium')\n",
        "\n",
        "    # Add title and axis labels\n",
        "    # ax[0].set_title(image_name)\n",
        "    ax[0].set_xlabel('Equivalent diameter (nm)', fontsize=16)\n",
        "    ax[0].set_ylabel('Probability', fontsize=16)\n",
        "    ax[0].set_xlim(min_diameter_nm-20, max_diameter_nm+20)\n",
        "\n",
        "    # x_ticks = [50, 100, 150, 200, 250, 300, 350, 400]  # Define which x ticks to show\n",
        "    x_ticks = list(range(min_diameter_nm, max_diameter_nm+1, 50))\n",
        "    ax[0].set_xticks(x_ticks)  # Set the x ticks\n",
        "    ax[0].set_xticklabels(x_ticks)  # Set the labels for the x ticks\n",
        "    ax[0].tick_params(axis='x', labelsize=12)\n",
        "    ax[0].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "\n",
        "    # Calculate density for each dataset\n",
        "    nbins = 12\n",
        "    hist_x, bin_edges_x  = np.histogram(roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_y, bin_edges_y = np.histogram(my_roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_z, bin_edges_z = np.histogram(s_roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_x = hist_x*np.diff(bin_edges_x)\n",
        "    hist_y = hist_y*np.diff(bin_edges_y)\n",
        "    hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "    bin_width = (1 - min_roundness)/3/nbins\n",
        "    shift1 = bin_width/2\n",
        "    shift2 = bin_width/2 + bin_width\n",
        "    shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "    ax[1].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "    ax[1].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "    ax[1].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "    # # i += 1\n",
        "    # plt.subplot(1, 2)\n",
        "    # ax[1].hist([roundness_of_ellipses, my_roundness_of_ellipses, s_roundness_of_ellipses], color=['g','r','b'], alpha=0.8, density=True)\n",
        "    ax[1].legend(['Ground truth', 'Automatic', 'Semiautomatic'], fontsize='medium')\n",
        "    # Add title and axis labels\n",
        "    # ax[1].set_title(image_name)\n",
        "    ax[1].set_xlabel('Roundness of fitted ellipses', fontsize=16)\n",
        "    ax[1].set_ylabel('Probability', fontsize=16)\n",
        "    ax[1].set_xlim(min_roundness - 0.02, 1.02)\n",
        "    ax[1].tick_params(axis='x', labelsize=12)\n",
        "    ax[1].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "    # Show image patch\n",
        "    r = slice(3600,4200)\n",
        "    c = slice(2600,3200)\n",
        "    image_patch = image[r, c]\n",
        "    show_size = (300, 300)\n",
        "    image_patch = cv.resize(image_patch, show_size)\n",
        "    ax[2].imshow(image_patch, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "    ax[2].set_title('Image patch example (600x600 pixels)', fontsize=16)\n",
        "    ax[2].axis('off')\n",
        "\n",
        "    ax[3].axis('off')\n",
        "\n",
        "    gt_patch = ground_truth_mask[r, c]\n",
        "    s_patch = semiauto_mask[r, c]\n",
        "    my_patch = new_mask[r, c]\n",
        "    # print(gt_patch.shape)\n",
        "\n",
        "    gt_patch = remove_fenestrations(gt_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    s_patch = remove_fenestrations(s_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    my_patch = remove_fenestrations(my_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "\n",
        "    # image_patch = cv.resize(image_patch, show_size, interpolation=cv.INTER_NEAREST)\n",
        "\n",
        "    merge = np.zeros((gt_patch.shape[0], gt_patch.shape[1], 3))\n",
        "    merge[:, :, 0][gt_patch == 255] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    ax[4].imshow(merge, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "    ax[4].set_title('Ground truth mask', fontsize=16)\n",
        "    ax[4].axis('off')\n",
        "\n",
        "    merge[:, :, 2][s_patch == 255] = 255 # B channel\n",
        "    merge[:, :, 1][s_patch == 255] = 100 # G channel\n",
        "    merge[:, :, 1][my_patch == 255] = 255 # G channel\n",
        "    merge = merge.astype('uint8')\n",
        "\n",
        "    ax[5].imshow(merge, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "    ax[5].set_title('Mask comparison', fontsize=16)\n",
        "    ax[5].axis('off')\n",
        "    leg = f'R = automatic FN & semiautomatic FN\\nG = automatic FP\\nB = semiautomatic FP'\n",
        "    leg += '\\nC = automatic FN & semiautomatic FN'\n",
        "    leg += '\\nM = automatic FN & semiautomatic TP'\n",
        "    leg += '\\nY  = automatic TP & semiautomatic FN'\n",
        "    leg += '\\nW = automatic TP & semiautomatic TP'\n",
        "    ax[5].text(0, merge.shape[0]+210, leg, ha='left', fontsize=16)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, f'stats_{image_name}.svg'), format='svg')\n",
        "\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Stats for whole dataset\n",
        "gt_mean = round(np.mean(np.array(all_gt_diameters)))\n",
        "gt_std = round(np.std(np.array(all_gt_diameters)))\n",
        "my_mean = round(np.mean(np.array(all_my_diameters)))\n",
        "my_std = round(np.std(np.array(all_my_diameters)))\n",
        "s_mean = round(np.mean(np.array(all_s_diameters)))\n",
        "s_std = round(np.std(np.array(all_s_diameters)))\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(13, 7))\n",
        "ax = ax.flatten()\n",
        "sns.set_theme()\n",
        "# Calculate density for each dataset\n",
        "nbins=20\n",
        "hist_x, bin_edges_x  = np.histogram(all_gt_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "hist_y, bin_edges_y = np.histogram(all_my_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "hist_z, bin_edges_z = np.histogram(all_s_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "hist_x = hist_x*np.diff(bin_edges_x)\n",
        "hist_y = hist_y*np.diff(bin_edges_y)\n",
        "hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "bin_width = (max_diameter_nm - min_diameter_nm)/3/nbins\n",
        "shift1 = bin_width/2\n",
        "shift2 = bin_width/2 + bin_width\n",
        "shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "ax[0].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "ax[0].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "ax[0].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "# ax[0].legend([f'Ground truth, mean: {gt_mean} nm\\n{len(all_gt_diameters)} fenestrations', f'Automatic, mean: {my_mean} nm\\n{len(all_my_diameters)} fenestrations', f'Semiautomatic, mean: {s_mean} nm\\n{len(all_s_diameters)} fenestrations'], fontsize='small')\n",
        "ax[0].legend([f'Ground truth\\nmean d = {gt_mean} nm', f'Automatic\\nmean d = {my_mean} nm', f'Semiautomatic\\nmean d = {s_mean} nm'], fontsize='medium')\n",
        "\n",
        "# Add title and axis labels\n",
        "ax[0].set_title('Whole dataset', fontsize=22)\n",
        "ax[0].set_xlabel('Equivalent diameter (nm)', fontsize=16)\n",
        "ax[0].set_ylabel('Probability', fontsize=16)\n",
        "ax[0].set_xlim(min_diameter_nm-20, max_diameter_nm+20)\n",
        "\n",
        "# x_ticks = [50, 100, 150, 200, 250, 300, 350, 400]  # Define which x ticks to show\n",
        "x_ticks = list(range(min_diameter_nm, max_diameter_nm+1, 50))\n",
        "ax[0].set_xticks(x_ticks)  # Set the x ticks\n",
        "ax[0].set_xticklabels(x_ticks)  # Set the labels for the x ticks\n",
        "ax[0].tick_params(axis='x', labelsize=12)\n",
        "ax[0].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "\n",
        "# Stats for whole dataset\n",
        "gt_mean = round(np.mean(np.array(all_gt_roundness)))\n",
        "gt_std = round(np.std(np.array(all_gt_roundness)))\n",
        "my_mean = round(np.mean(np.array(all_my_roundness)))\n",
        "my_std = round(np.std(np.array(all_my_roundness)))\n",
        "s_mean = round(np.mean(np.array(all_s_roundness)))\n",
        "s_std = round(np.std(np.array(all_s_roundness)))\n",
        "\n",
        "# Calculate density for each dataset\n",
        "\n",
        "\n",
        "nbins=12\n",
        "hist_x, bin_edges_x  = np.histogram(all_gt_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "hist_y, bin_edges_y = np.histogram(all_my_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "hist_z, bin_edges_z = np.histogram(all_s_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "hist_x = hist_x*np.diff(bin_edges_x)\n",
        "hist_y = hist_y*np.diff(bin_edges_y)\n",
        "hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "bin_width = (1 - min_roundness)/3/nbins\n",
        "shift1 = bin_width/2\n",
        "shift2 = bin_width/2 + bin_width\n",
        "shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "ax[1].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "ax[1].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "ax[1].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "# ax[1].legend([f'Ground truth\\nmean roundness = {gt_mean}', f'Automatic\\nmean roundness = {my_mean}', f'Semiautomatic\\nmean roundness = {s_mean}'], fontsize='small')\n",
        "# Add title and axis labels\n",
        "ax[1].set_title('Whole dataset', fontsize=22)\n",
        "ax[1].set_xlabel('Roundness of fitted ellipses', fontsize=16)\n",
        "ax[1].set_ylabel('Probability', fontsize=16)\n",
        "ax[1].legend(['Ground truth', 'Automatic', 'Semiautomatic'], fontsize='medium')\n",
        "ax[1].set_xlim(min_roundness-0.02, 1.02)\n",
        "ax[1].tick_params(axis='x', labelsize=12)\n",
        "ax[1].tick_params(axis='y', labelsize=12)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'whole.svg'), format='svg')\n",
        "plt.show()\n",
        "\n",
        "# Correlation for the number of found ellipses\n",
        "num_all_my_ellipses = np.array(num_all_my_ellipses)\n",
        "num_all_gt_ellipses = np.array(num_all_gt_ellipses)\n",
        "num_all_s_ellipses =  np.array(num_all_s_ellipses)\n",
        "\n",
        "sorted_indices = np.argsort(num_all_gt_ellipses)\n",
        "num_all_gt_ellipses = num_all_gt_ellipses[sorted_indices]\n",
        "num_all_my_ellipses = num_all_my_ellipses[sorted_indices]\n",
        "num_all_s_ellipses = num_all_s_ellipses[sorted_indices]\n",
        "\n",
        "# Fit a linear function to the data\n",
        "my_coefficients = np.polyfit(num_all_gt_ellipses, num_all_my_ellipses, 1)\n",
        "my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "s_coefficients = np.polyfit(num_all_gt_ellipses, num_all_s_ellipses, 1)\n",
        "s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "my_r_squared = round(r2_score(num_all_gt_ellipses, my_fit_line(num_all_gt_ellipses)), 2)\n",
        "my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "s_r_squared = round(r2_score(num_all_gt_ellipses, s_fit_line(num_all_s_ellipses)), 2)\n",
        "s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "s_fitted_values = np.polyval(s_coefficients, num_all_gt_ellipses)\n",
        "s_residuals = num_all_s_ellipses - s_fitted_values\n",
        "s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "my_fitted_values = np.polyval(my_coefficients, num_all_gt_ellipses)\n",
        "my_residuals = num_all_my_ellipses - my_fitted_values\n",
        "my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "s_tg = np.polyval(s_derivative_coefficients, 1000)\n",
        "my_tg = np.polyval(my_derivative_coefficients, 1000)\n",
        "s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "s_residuals_sd = round(s_residuals_sd)\n",
        "my_residuals_sd = round(my_residuals_sd)\n",
        "\n",
        "\n",
        "print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "# ax = ax.flatten()\n",
        "sns.set_theme()\n",
        "ax.plot(num_all_gt_ellipses, my_fit_line(num_all_gt_ellipses), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}{my_residuals_sd}')\n",
        "ax.plot(num_all_gt_ellipses, s_fit_line(num_all_gt_ellipses), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}{s_residuals_sd}')\n",
        "ax.scatter(num_all_gt_ellipses, num_all_my_ellipses, color='g', marker='o', label=f'Automatic: R = {my_r_squared}', alpha=np.ones_like(num_all_gt_ellipses))\n",
        "ax.scatter(num_all_gt_ellipses, num_all_s_ellipses, color='b', marker='s', label=f'Semiautomatic: R = {s_r_squared}', alpha=np.ones_like(num_all_gt_ellipses))\n",
        "\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Ground truth number of fenestrations per image', fontsize=16)\n",
        "ax.set_ylabel('Detected number of fenestrations per image', fontsize=16)\n",
        "ax.set_title(f'Correlation plot of the number of detected fenestrations\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "ax.grid(True)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'corr_num.svg'), format='svg')\n",
        "plt.show()\n",
        "\n",
        "# print(r2_score(num_all_gt_ellipses,num_all_s_ellipses), r2_score(num_all_gt_ellipses,num_all_my_ellipses))\n",
        "# print(scipy.stats.pearsonr(num_all_gt_ellipses, num_all_s_ellipses)[0]**2, scipy.stats.pearsonr(num_all_gt_ellipses, num_all_my_ellipses)[0]**2)\n",
        "\n",
        "\n",
        "print('num ellipses')\n",
        "print(f'gt {np.sum(num_all_gt_ellipses)}, my {np.sum(num_all_my_ellipses)}, s {np.sum(num_all_s_ellipses)}')\n",
        "print(my_dice_scores)\n",
        "print(s_dice_scores)\n",
        "print('my')\n",
        "for dice in my_dice_scores:\n",
        "    print(round(dice, 2))\n",
        "my_dice_scores = np.array(my_dice_scores)\n",
        "my_mean_dice = round(np.mean(my_dice_scores), 2)\n",
        "my_std_dice = round(np.std(my_dice_scores), 2)\n",
        "print(f'Auto mean dice: {my_mean_dice} +- {my_std_dice}')\n",
        "print('s')\n",
        "for dice in s_dice_scores:\n",
        "    print(round(dice, 2))\n",
        "s_dice_scores = np.array(s_dice_scores)\n",
        "s_mean_dice = round(np.mean(s_dice_scores), 2)\n",
        "s_std_dice = round(np.std(s_dice_scores), 2)\n",
        "print(f'Semiauto mean dice: {s_mean_dice} +- {s_std_dice}')\n",
        "\n",
        "# correlation of means\n",
        "\n",
        "\n",
        "all_my_means = np.array(all_my_means)\n",
        "all_gt_means = np.array(all_gt_means)\n",
        "all_s_means =  np.array(all_s_means)\n",
        "\n",
        "sorted_indices = np.argsort(all_gt_means)\n",
        "all_gt_means = all_gt_means[sorted_indices]\n",
        "all_my_means = all_my_means[sorted_indices]\n",
        "all_s_means = all_s_means[sorted_indices]\n",
        "\n",
        "# Fit a linear function to the data\n",
        "my_coefficients = np.polyfit(all_gt_means, all_my_means, 1)\n",
        "my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "s_coefficients = np.polyfit(all_gt_means, all_s_means, 1)\n",
        "s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "s_fitted_values = np.polyval(s_coefficients, all_gt_means)\n",
        "s_residuals = all_s_means - s_fitted_values\n",
        "s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "my_fitted_values = np.polyval(my_coefficients, all_gt_means)\n",
        "my_residuals = all_my_means - my_fitted_values\n",
        "my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "my_r_squared = round(r2_score(all_gt_means, my_fit_line(all_gt_means)), 2)\n",
        "my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "s_r_squared = round(r2_score(all_gt_means, s_fit_line(all_gt_means)), 2)\n",
        "s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "s_tg = np.polyval(s_derivative_coefficients, 150)\n",
        "my_tg = np.polyval(my_derivative_coefficients, 150)\n",
        "s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "s_residuals_sd = round(s_residuals_sd)\n",
        "my_residuals_sd = round(my_residuals_sd)\n",
        "\n",
        "print('mean values')\n",
        "print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "sns.set_theme()\n",
        "ax.plot(all_gt_means, my_fit_line(all_gt_means), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}{my_residuals_sd}')\n",
        "ax.plot(all_gt_means, s_fit_line(all_gt_means), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}{s_residuals_sd}')\n",
        "ax.scatter(all_gt_means, all_my_means, color='g', marker='o', label=f'Automatic: R = {my_r_squared}', alpha=np.ones_like(all_gt_means))\n",
        "ax.scatter(all_gt_means, all_s_means, color='b', marker='s', label=f'Semiautomatic: R = {s_r_squared}', alpha=np.ones_like(all_gt_means))\n",
        "\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Ground truth mean fenestration\\nequivalent diameter per image (nm)', fontsize=16)\n",
        "ax.set_ylabel('Detected mean fenestration\\nequivalent diameter per image (nm)', fontsize=16)\n",
        "ax.set_title(f'Correlation plot of the mean fenestration diameter\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "ax.grid(True)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "x_ticks = list(range(110, 180+1, 10))\n",
        "ax.set_xticks(x_ticks)\n",
        "ax.set_yticks(x_ticks)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'corr_diameter.svg'), format='svg')\n",
        "plt.show()\n",
        "print('gt means')\n",
        "for mean in all_gt_means:\n",
        "    print(mean)\n",
        "print('my means')\n",
        "for mean in all_my_means:\n",
        "    print(mean)\n",
        "print('s means')\n",
        "for mean in all_s_means:\n",
        "    print(mean)\n",
        "\n",
        "\n",
        "\n",
        "print('porosity')\n",
        "print('gt')\n",
        "for p in all_gt_porosities_pix:\n",
        "    print(p)\n",
        "print('s')\n",
        "for p in all_s_porosities_pix:\n",
        "    print(p)\n",
        "print('my')\n",
        "for p in all_my_porosities_pix:\n",
        "    print(p)\n",
        "\n",
        "\n",
        "# Porosity correlation\n",
        "all_gt_porosities_pix = np.array(all_gt_porosities_pix)\n",
        "all_s_porosities_pix = np.array(all_s_porosities_pix)\n",
        "all_my_porosities_pix =  np.array(all_my_porosities_pix)\n",
        "\n",
        "sorted_indices = np.argsort(all_gt_porosities_pix)\n",
        "all_gt_porosities_pix = all_gt_porosities_pix[sorted_indices]\n",
        "all_s_porosities_pix = all_s_porosities_pix[sorted_indices]\n",
        "all_my_porosities_pix = all_my_porosities_pix[sorted_indices]\n",
        "\n",
        "\n",
        "# Fit a linear function to the data\n",
        "my_coefficients = np.polyfit(all_gt_porosities_pix, all_my_porosities_pix, 1)\n",
        "my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "s_coefficients = np.polyfit(all_gt_porosities_pix, all_s_porosities_pix, 1)\n",
        "s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "s_fitted_values = np.polyval(s_coefficients, all_gt_porosities_pix)\n",
        "s_residuals = all_s_porosities_pix - s_fitted_values\n",
        "s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "my_fitted_values = np.polyval(my_coefficients, all_gt_porosities_pix)\n",
        "my_residuals = all_my_porosities_pix - my_fitted_values\n",
        "my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "my_r_squared = round(r2_score(all_gt_porosities_pix, my_fit_line(all_gt_porosities_pix)), 2)\n",
        "my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "s_r_squared = round(r2_score(all_gt_porosities_pix, s_fit_line(all_gt_porosities_pix)), 2)\n",
        "s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "s_residuals_sd = round(s_residuals_sd, 2)\n",
        "my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "# print('mean values')\n",
        "# print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "# print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "sns.set_theme()\n",
        "ax.plot(all_gt_porosities_pix, my_fit_line(all_gt_porosities_pix), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}{my_residuals_sd}')\n",
        "ax.plot(all_gt_porosities_pix, s_fit_line(all_gt_porosities_pix), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}{s_residuals_sd}')\n",
        "ax.scatter(all_gt_porosities_pix, all_my_porosities_pix, color='g', marker='o', label=f'Automatic: R = {my_r_squared}', alpha=np.ones_like(all_gt_porosities_pix))\n",
        "ax.scatter(all_gt_porosities_pix, all_s_porosities_pix, color='b', marker='s', label=f'Semiautomatic: R = {s_r_squared}', alpha=np.ones_like(all_gt_porosities_pix))\n",
        "\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Ground truth porosity per image (%)', fontsize=16)\n",
        "ax.set_ylabel('Detected porosity per image (%)', fontsize=16)\n",
        "ax.set_title(f'Correlation plot of cell porosity of fenestrations detected\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "ax.grid(True)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "# x_ticks = list(range(110, 180+1, 10))\n",
        "# ax.set_xticks(x_ticks)\n",
        "# ax.set_yticks(x_ticks)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'corr_porosity_pix.svg'), format='svg')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "print('porosity')\n",
        "print('gt')\n",
        "for p in all_gt_porosities_ell:\n",
        "    print(p)\n",
        "print('s')\n",
        "for p in all_s_porosities_ell:\n",
        "    print(p)\n",
        "print('my')\n",
        "for p in all_my_porosities_ell:\n",
        "    print(p)\n",
        "\n",
        "\n",
        "\n",
        "# Porosity correlation\n",
        "all_gt_porosities_ell = np.array(all_gt_porosities_ell)\n",
        "all_s_porosities_ell = np.array(all_s_porosities_ell)\n",
        "all_my_porosities_ell =  np.array(all_my_porosities_ell)\n",
        "\n",
        "sorted_indices = np.argsort(all_gt_porosities_ell)\n",
        "all_gt_porosities_ell = all_gt_porosities_ell[sorted_indices]\n",
        "all_s_porosities_ell = all_s_porosities_ell[sorted_indices]\n",
        "all_my_porosities_ell = all_my_porosities_ell[sorted_indices]\n",
        "\n",
        "my_r_squared = round(r2_score(all_gt_porosities_ell, my_fit_line(all_gt_porosities_ell)), 2)\n",
        "my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "s_r_squared = round(r2_score(all_gt_porosities_ell, s_fit_line(all_gt_porosities_ell)), 2)\n",
        "s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "my_coefficients = np.polyfit(all_gt_porosities_ell, all_my_porosities_ell, 1)\n",
        "my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "s_coefficients = np.polyfit(all_gt_porosities_ell, all_s_porosities_ell, 1)\n",
        "s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "s_fitted_values = np.polyval(s_coefficients, all_gt_porosities_ell)\n",
        "s_residuals = all_s_porosities_ell - s_fitted_values\n",
        "s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "my_fitted_values = np.polyval(my_coefficients, all_gt_porosities_ell)\n",
        "my_residuals = all_my_porosities_ell - my_fitted_values\n",
        "my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "s_residuals_sd = round(s_residuals_sd, 2)\n",
        "my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "# print('mean values')\n",
        "# print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "# print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "sns.set_theme()\n",
        "ax.plot(all_gt_porosities_ell, my_fit_line(all_gt_porosities_ell), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}{my_residuals_sd}')\n",
        "ax.plot(all_gt_porosities_ell, s_fit_line(all_gt_porosities_ell), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}{s_residuals_sd}')\n",
        "ax.scatter(all_gt_porosities_ell, all_my_porosities_ell, color='g', marker='o', label=f'Automatic: R = {my_r_squared}', alpha=np.ones_like(all_gt_porosities_ell))\n",
        "ax.scatter(all_gt_porosities_ell, all_s_porosities_ell, color='b', marker='s', label=f'Semiautomatic: R = {s_r_squared}', alpha=np.ones_like(all_gt_porosities_ell))\n",
        "\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Ground truth porosity per image (%)', fontsize=16)\n",
        "ax.set_ylabel('Detected porosity per image (%)', fontsize=16)\n",
        "ax.set_title(f'Correlation plot of cell porosity computed with fitted ellipses\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "ax.grid(True)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "# x_ticks = list(range(110, 180+1, 10))\n",
        "# ax.set_xticks(x_ticks)\n",
        "# ax.set_yticks(x_ticks)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'corr_porosity_ell.svg'), format='svg')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# fen freq\n",
        "print('frequency')\n",
        "print('gt')\n",
        "for p in all_gt_frequencies:\n",
        "    print(p)\n",
        "print('s')\n",
        "for p in all_s_frequencies:\n",
        "    print(p)\n",
        "print('my')\n",
        "for p in all_my_frequencies:\n",
        "    print(p)\n",
        "\n",
        "\n",
        "# Porosity correlation\n",
        "all_gt_frequencies = np.array(all_gt_frequencies)\n",
        "all_s_frequencies = np.array(all_s_frequencies)\n",
        "all_my_frequencies =  np.array(all_my_frequencies)\n",
        "\n",
        "sorted_indices = np.argsort(all_gt_frequencies)\n",
        "all_gt_frequencies = all_gt_frequencies[sorted_indices]\n",
        "all_s_frequencies = all_s_frequencies[sorted_indices]\n",
        "all_my_frequencies = all_my_frequencies[sorted_indices]\n",
        "\n",
        "\n",
        "# Fit a linear function to the data\n",
        "my_coefficients = np.polyfit(all_gt_frequencies, all_my_frequencies, 1)\n",
        "my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "# Fit a linear function to the data\n",
        "s_coefficients = np.polyfit(all_gt_frequencies, all_s_frequencies, 1)\n",
        "s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "s_fitted_values = np.polyval(s_coefficients, all_gt_frequencies)\n",
        "s_residuals = all_s_frequencies - s_fitted_values\n",
        "s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "my_fitted_values = np.polyval(my_coefficients, all_gt_frequencies)\n",
        "my_residuals = all_my_frequencies - my_fitted_values\n",
        "my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "my_r_squared = round(r2_score(all_gt_frequencies, my_fit_line(all_gt_frequencies)), 2)\n",
        "my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "s_r_squared = round(r2_score(all_gt_frequencies, s_fit_line(all_gt_frequencies)), 2)\n",
        "s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "\n",
        "s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "s_residuals_sd = round(s_residuals_sd, 2)\n",
        "my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "sns.set_theme()\n",
        "ax.plot(all_gt_frequencies, my_fit_line(all_gt_frequencies), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}{my_residuals_sd}')\n",
        "ax.plot(all_gt_frequencies, s_fit_line(all_gt_frequencies), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}{s_residuals_sd}')\n",
        "ax.scatter(all_gt_frequencies, all_my_frequencies, color='g', marker='o', label=f'Automatic: R = {my_r_squared}', alpha=np.ones_like(all_gt_frequencies))\n",
        "ax.scatter(all_gt_frequencies, all_s_frequencies, color='b', marker='s', label=f'Semiautomatic: R = {s_r_squared}', alpha=np.ones_like(all_gt_frequencies))\n",
        "\n",
        "ax.legend(fontsize='medium')\n",
        "ax.set_xlabel('Fenestration frequency per image\\n(fenestrations/m)', fontsize=16)\n",
        "ax.set_ylabel('Detected fenestration frequency per image\\n(fenestrations/m)', fontsize=16)\n",
        "ax.set_title(f'Correlation plot of fenestration frequency detected\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "ax.grid(True)\n",
        "ax.tick_params(axis='x', labelsize=12)\n",
        "ax.tick_params(axis='y', labelsize=12)\n",
        "# x_ticks = list(range(110, 180+1, 10))\n",
        "# ax.set_xticks(x_ticks)\n",
        "# ax.set_yticks(x_ticks)\n",
        "if save_plots:\n",
        "    plt.savefig(os.path.join(plot_path, 'corr_freq.svg'), format='svg')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#     # if remove_false_fenestrations:\n",
        "#     #     new_mask_filt = remove_fenestrations(new_mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "#     #     # current_dice_score_filt = compute_dice_score(ground_truth_mask, new_mask_filt)\n",
        "#     #     # dice_scores_filt.append(current_dice_score_filt)\n",
        "#     #     # print(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})')\n",
        "#     #     # file.write(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})\\n')\n",
        "#     # else:\n",
        "#     #     # print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "#     #     # file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "\n",
        "# # dice_scores = np.array(dice_scores)\n",
        "# # mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "# # std_dice = round(np.std(dice_scores)*100, 1)\n",
        "# # if remove_false_fenestrations:\n",
        "# #     dice_scores_filt = np.array(dice_scores_filt)\n",
        "# #     mean_dice_filt = round(np.mean(dice_scores_filt)*100, 1)\n",
        "# #     std_dice_filt = round(np.std(dice_scores_filt)*100, 1)\n",
        "# #     print(f'{model_name} Mean dice: {mean_dice} +- {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n')\n",
        "# #     file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n\\n')\n",
        "# # else:\n",
        "# #     print(f'{model_name} Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "# #     file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U1JDFWEQ6hbm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}