{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for training a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECS)."
      ],
      "metadata": {
        "id": "-aHjwiD8IkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "J-Z80u6TN3Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a network, first connect to a GPU (**Runtime -> Change runtime time -> Hardware accelerator -> GPU**).\n",
        "\n",
        "If you are using a pretrained network for inference and not training, being connected only to a **CPU** is slower, but possible."
      ],
      "metadata": {
        "id": "NUZeORlUN_LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "-gq1-hflPdMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "LHteKyDySYvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893dec0a-ec46-4072-d8ac-24e0837eef15"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N5QvbqMfiA4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d39ff8-f5b9-4bda-8f66-0d4be1073cff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.0.0-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.0.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.17.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=062c67200ef5c4bda2d88c3d848e90ef685df0a7f29156b2aa28516e18c7f1c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=30065260a6c84727deaf293ed0ef8eae7a61e84cab975205d349ba911e50dd36\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from numba import njit\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "\n",
        "# gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = False\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth.tar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transofrm=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transofrm\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask[mask == 255.0] = 1\n",
        "        mask /= 255\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_dir, mask_dir, split, batch_size, num_workers=4, pin_memory=True): # TODO: check these parameters\n",
        "    data = MyDataset(\n",
        "        image_dir=img_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        transofrm=None\n",
        "    )\n",
        "\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        range(len(data)),\n",
        "        test_size=split,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_data = TransformDataset(Subset(data, train_indices), train_transform)\n",
        "    val_data = TransformDataset(Subset(data, test_indices), val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_indices\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "        [\n",
        "            A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # A.Affine(shear=(0.5,1)),\n",
        "            # A.Affine(scale=(-10, 10)),\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0, # normalization to [0, 1]\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# test_transform = A.Compose(\n",
        "#     [\n",
        "#     A.Normalize(\n",
        "#       mean = 0.0,\n",
        "#       std = 1.0,\n",
        "#       max_pixel_value=255.0,\n",
        "#     ),\n",
        "#         ToTensorV2()\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations if needed\n",
        "])\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - border_size:height, :])\n",
        "    bottom_zeros = np.zeros((bottom_edge-border_size, width), dtype = image.dtype)\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border, bottom_zeros))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - border_size:width])\n",
        "    right_zeros = np.zeros((top_bottom_mirrored.shape[0], right_edge-border_size), dtype = image.dtype)\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border, right_zeros))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "    window_size = 512\n",
        "    oh, ow = 50, 50\n",
        "    # out_crop =\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    # bottom_edge = (image_height + oh) % (window_size - oh)\n",
        "    # right_edge = (image_height + ow) % (window_size - ow)\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    # print(mirrored_image.shape)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    # tryout = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "            # print(x, y)\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    # if original_height != window_size:\n",
        "                    weighting_window = top_left\n",
        "                    # print('top left')\n",
        "                elif y == image_width - window_size:\n",
        "                    # print('top right')\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "                    # print('top ')\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                    # print('bottom left')\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                    # print('bottom right')\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "                    # print('bottom')\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "                # print('left')\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "                # print('right')\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "                # print('middle')\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "            square_section = preprocess_image(square_section) # TODO: prehodit tohle, at se to dela jednou pro celej obrazek, ne pro patche?\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            # output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # cv2_imshow(output_pil)\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "    # Crop\n",
        "    # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    # weights = weights[:original_height, :original_width]*255\n",
        "    # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # Apply weights\n",
        "    # output_probs /= weights\n",
        "\n",
        "    # Create image from mask\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_mirrored\"+ext), mirrored_image)\n",
        "\n",
        "    # Merge image with created mask\n",
        "    out_mask_path = os.path.join(output_folder, filename+\"_new_mask\"+ext)\n",
        "    merge = merge_images(input_image, output_mask)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "    cv.imwrite(out_mask_path, output_mask)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "    return out_mask_path\n",
        "\n",
        "# def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "#     window_size = 512\n",
        "#     oh, ow = 124, 124\n",
        "#     input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#     image_height, image_width = input_image.shape\n",
        "#     original_height, original_width = image_height, image_width\n",
        "#     bottom_edge = image_height % (window_size - oh)\n",
        "#     right_edge = image_width % (window_size - ow)\n",
        "#     mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "#     mirrored_image[:image_height, :image_width] = input_image\n",
        "#     mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "#     mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "#     image_height += bottom_edge\n",
        "#     image_width += right_edge\n",
        "#     weights = np.zeros((image_height, image_width))\n",
        "#     # tryout = np.zeros((image_height, image_width))\n",
        "#     output_probs = np.zeros((image_height, image_width))\n",
        "#     output_mask = np.zeros((image_height, image_width))\n",
        "#     middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "#     for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "#         for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "#             # Choose weighting window\n",
        "#             if x == 0:\n",
        "#                 if y == 0:\n",
        "#                     if original_height != window_size:\n",
        "#                         weighting_window = top_left\n",
        "#                     else:\n",
        "#                         weighting_window = np.ones((window_size, window_size))\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = top_right\n",
        "#                 else:\n",
        "#                     weighting_window = top\n",
        "#             elif x == window_size - oh - 1:\n",
        "#                 if y == 0:\n",
        "#                     weighting_window = bottom_left\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = bottom_right\n",
        "#                 else:\n",
        "#                     weighting_window = bottom\n",
        "#             elif y == 0:\n",
        "#                 weighting_window = left\n",
        "#             elif y == window_size - ow - 1:\n",
        "#                 weighting_window = right\n",
        "#             else:\n",
        "#                 weighting_window = middle\n",
        "#             square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "#             weights[x:x + window_size, y:y + window_size] = weighting_window\n",
        "#             # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "#             square_section = preprocess_image(square_section)\n",
        "#             square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "#             # Forward pass through the model\n",
        "#             with torch.no_grad():\n",
        "#                 output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "#             # Scale the probablity to 0-255\n",
        "#             output = output*255\n",
        "#             output = output.to(torch.uint8)\n",
        "#             output_pil = output.squeeze(0).cpu().numpy()\n",
        "#             output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()*weighting_window\n",
        "#     # Crop\n",
        "#     output_probs = output_probs[:original_height, :original_width]\n",
        "#     # weights = weights[:original_height, :original_width]*255\n",
        "#     # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "#     # Apply weights\n",
        "#     # output_probs /= weights\n",
        "\n",
        "#     # Create image from mask\n",
        "#     output_mask = np.where(output_probs > 127, 255, 0)\n",
        "#     output_mask = output_mask.astype(np.uint8)\n",
        "#     filename_ext = os.path.basename(image_path)\n",
        "#     filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "#     # Merge image with created mask\n",
        "#     out_mask_path = os.path.join(output_folder, filename+\"_mask\"+ext)\n",
        "#     merge = merge_images(input_image, output_mask)\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_merge\"+ext), merge)\n",
        "\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "#     cv.imwrite(out_mask_path, output_mask)\n",
        "#     # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "#     return out_mask_path\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = nlm_filt(image)\n",
        "    # image = wavelet_denoise(image, threshold=1.5)\n",
        "    # image = apply_clahe(image)\n",
        "    # image = cv.medianBlur(image, 7)\n",
        "    return image\n",
        "\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "    image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    fenestration_area_thresh = 0.0 #0.01\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        # if image_name.endswith(\".tif\"): # TODO: tohle mozna odstranit\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "        # strides = (patch_size * width , patch_size)\n",
        "\n",
        "        # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "        #              strides=img.strides + img.strides, writeable=False)\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                img_patch = img_strided[i, j]\n",
        "                mask_patch = mask_strided[i, j]\n",
        "                # Compute the percentage of white pixels\n",
        "                fenestration_area = np.sum(mask_patch == 255)\n",
        "                # print(fenestration_area)\n",
        "                # fenestration_percentage = fenestration_area/patch_area\n",
        "                if fenestration_area >= fenestration_area_thresh:\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "                else:\n",
        "                    print(\"not writing patch\")\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "# This is the official implementation of BoundaryDOULoss https://arxiv.org/pdf/2308.00220.pdf\n",
        "# Taken from: https://github.com/sunfan-bvb/BoundaryDoULoss/tree/main\n",
        "class BoundaryDoULoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BoundaryDoULoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i\n",
        "            tensor_list.append(temp_prob.unsqueeze(1))\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _adaptive_size(self, score, target):\n",
        "        kernel = torch.Tensor([[0,1,0], [1,1,1], [0,1,0]])\n",
        "        padding_out = torch.zeros((target.shape[0], target.shape[-2]+2, target.shape[-1]+2))\n",
        "        padding_out[:, 1:-1, 1:-1] = target\n",
        "        h, w = 3, 3\n",
        "\n",
        "        Y = torch.zeros((padding_out.shape[0], padding_out.shape[1] - h + 1, padding_out.shape[2] - w + 1)).cuda()\n",
        "        for i in range(Y.shape[0]):\n",
        "            Y[i, :, :] = torch.conv2d(target[i].unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0).cuda(), padding=1)\n",
        "        Y = Y * target\n",
        "        Y[Y == 5] = 0\n",
        "        C = torch.count_nonzero(Y)\n",
        "        S = torch.count_nonzero(target)\n",
        "        smooth = 1e-5\n",
        "        alpha = 1 - (C + smooth) / (S + smooth)\n",
        "        alpha = 2 * alpha - 1\n",
        "\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        alpha = min(alpha, 0.8)  ## We recommend using a truncated alpha of 0.8, as using truncation gives better results on some datasets and has rarely effect on others.\n",
        "        loss = (z_sum + y_sum - 2 * intersect + smooth) / (z_sum + y_sum - (1 + alpha) * intersect + smooth)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "\n",
        "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            loss += self._adaptive_size(inputs[:, i], target[:, i])\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            preds = model(x)\n",
        "            # loss_fn = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(16))\n",
        "            # loss = loss_fn(preds, y)\n",
        "            loss = get_loss(preds, y, loss_fn)\n",
        "            running_loss += loss.cpu()\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "            # num_correct += (preds == y).sum()\n",
        "            # num_pixels += torch.numel(preds)\n",
        "            dice_score += (2*(preds*y).sum()) / (preds+y).sum() + 1e-8 # this is a better predictor\n",
        "    # print(\n",
        "    #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "    # )\n",
        "    dice_score = dice_score/(idx+1)\n",
        "    val_loss = running_loss/(idx+1)\n",
        "    # dice_score = dice_score/len(loader)\n",
        "    # val_loss = running_loss/len(loader) #TODO: not sure this is correct(dividing by batch size?)\n",
        "    # print(f\"Dice score is {dice_score}\")\n",
        "    # val_losses.append(running_loss/len(loader))\n",
        "    # dice_scores.append(dice_score.cpu())\n",
        "    model.train()\n",
        "    return val_loss, dice_score.cpu()\n",
        "\n",
        "\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            # output = torch.softmax(model(x), dim=1)\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# def getClassWeights(mask_path, train_indices):\n",
        "#     mask_dir_list = sorted(os.listdir(mask_path))\n",
        "#     class_count = np.zeros(2, dtype=int)\n",
        "#     for i in train_indices:\n",
        "#         mask = cv.imread(os.path.join(mask_path, mask_dir_list[i]), cv.IMREAD_GRAYSCALE) #np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "#         mask[mask == 255.0] = 1\n",
        "#         class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "#         class_count[1] += mask.sum()\n",
        "\n",
        "#     n_samples = class_count.sum()\n",
        "#     n_classes = 2\n",
        "\n",
        "#     class_weights = n_samples / (n_classes * class_count)\n",
        "#     return torch.from_numpy(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "            else:\n",
        "                ellipses.append(None)\n",
        "        else:\n",
        "            ellipses.append(None)\n",
        "    return ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "    # image = cv.cvtColor(seg_mask, cv.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    # min_contour_area = 10\n",
        "    # filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
        "    # return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    # print(contour_centers)\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01_original_mask.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "fenestration_areas_from_ellipses = []\n",
        "\n",
        "for ellipse in ellipses:\n",
        "    center, axes, angle = ellipse\n",
        "    # center_x, center_y = center\n",
        "    major_axis_length, minor_axis_length = axes\n",
        "    roundness = minor_axis_length/major_axis_length\n",
        "    roundness_of_ellipses.append(roundness)\n",
        "    # rotation_angle = angle\n",
        "    diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "    equivalent_diameters.append(diameter)\n",
        "    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "\n",
        "# show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtPrBpQBcsmn",
        "outputId": "bcc03d24-197a-4bc5-b78d-306b25f23fc1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fenestrations: 0\n",
            "Number of fitted ellipses: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wavelet filtering debug\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "images = os.listdir(image_folder)\n",
        "image_name = images[0]\n",
        "image = cv.imread(os.path.join(image_folder, image_name), cv.IMREAD_GRAYSCALE)\n",
        "# cv2_imshow(image)\n",
        "\n",
        "denoised_image = wavelet_denoise(image)\n",
        "# cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch, activation):\n",
        "    if activation == 'ReLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    elif activation == 'GeLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none'),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none')\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device, dropout_probability, activations, out_activation):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(p=dropout_probability)\n",
        "        self.activations = activations\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64, activation=activations)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128, activation=activations)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256, activation=activations)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512, activation=activations)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024, activation=activations)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512, activation=activations)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256, activation=activations)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128, activation=activations)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64, activation=activations)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "        self.out_activation = out_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        p2 = self.dropout(p2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        p3 = self.dropout(p3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        p4 = self.dropout(p4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        cat1 = self.dropout(cat1)\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        cat2 = self.dropout(cat2)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        if self.out_activation == 'sigmoid':\n",
        "            conv_1x1 = torch.sigmoid(conv_1x1)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Patch creation**"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "training_images = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "training_masks = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "training_masks = training_masks.strip()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4qR6zmj4U-pC",
        "outputId": "7e0fe7be-b510-41ce-9ce0-1eefe6b1394f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./gdrive/MyDrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE_NEW_PATCHES = False\n",
        "SAVE_PATCHES_TO_DISK = False\n",
        "CREATE_NEW_PATCHES = True\n",
        "SAVE_PATCHES_TO_DISK = True\n",
        "# Example usage:\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "mask_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_masks\"\n",
        "\n",
        "if CREATE_NEW_PATCHES:\n",
        "    patch_size = 512  # Define your patch size here\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\"\n",
        "    else:\n",
        "        output_folder = os.getcwd()\n",
        "    image_patches_path, mask_patches_path = create_image_patches(image_folder, mask_folder, output_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\"\n",
        "    image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, 'mask_patches')\n"
      ],
      "metadata": {
        "id": "UzznzOTP4s53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "WAJp45Xo8p_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_optimizer(model, config, beta1=None, beta2=None):\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=config.learning_rate,\n",
        "                              weight_decay=config.weight_decay,\n",
        "                              momentum=config.momentum)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=config.learning_rate,\n",
        "                               betas=(config.beta1, config.beta2),\n",
        "                               weight_decay=config.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "# TRAIN_LOADER = train_loader\n",
        "# VAL_LOADER = val_loader\n",
        "def build_dataloaders(config): # TODO: check if there is a better way to do this\n",
        "    image_patches_path = os.path.join(config.image_patches_path, 'patches_'+ config.image_denoising_methods)\n",
        "    mask_patches_path = os.path.join(config.mask_patches_path, 'patches_'+ config.image_denoising_methods)\n",
        "    image_patches_path = os.path.join(image_patches_path, 'image_patches')\n",
        "    mask_patches_path = os.path.join(mask_patches_path, 'mask_patches')\n",
        "    train_loader, val_loader, _ = get_loaders(\n",
        "        image_patches_path,\n",
        "        mask_patches_path,\n",
        "        config.data_split,\n",
        "        config.batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    return train_loader, val_loader # this is the simplest way to do it, wandb train cannot take any arguments\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    # model.train()\n",
        "    running_loss = 0\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            # TODO: change this\n",
        "            # loss = F.nll_loss(torch.sigmoid(predictions), targets)\n",
        "            loss = get_loss(predictions, targets, loss_fn)\n",
        "            # loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "        if WANDB_CONNECTED or WANDB_LOG:\n",
        "            wandb.log({\"batch loss\": loss.item()})\n",
        "\n",
        "    number_of_batches = batch_idx+1\n",
        "    return running_loss/number_of_batches\n",
        "\n",
        "def build_model(model_name, dropout, loss_func):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    if '+' in model_name:\n",
        "        name_parts = model_name.split('+')\n",
        "        encoder = name_parts[-2]\n",
        "        weights = name_parts[-1]\n",
        "    if loss_func != 'bcelog' and loss_func != 'weighted_bce':\n",
        "        out_activation = None\n",
        "    else:\n",
        "        out_activation = 'sigmoid'\n",
        "    if model_name == 'plain_unet':\n",
        "        model = UNET(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                device=DEVICE,\n",
        "                dropout_probability=dropout,\n",
        "                activations='ReLU',\n",
        "                out_activation=out_activation).to(DEVICE)\n",
        "    elif 'Unet++' in model_name:\n",
        "        model = smp.UnetPlusPlus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'Linknet' in model_name:\n",
        "        model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'FPN' in model_name:\n",
        "        model = smp.FPN(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'DeepLabV3' in model_name:\n",
        "        model = smp.DeepLabV3(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    else:\n",
        "        model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def get_loss(pred, target, func_name):\n",
        "    loss_func = None\n",
        "    if func_name == 'dice':\n",
        "        loss_func = smp.losses.DiceLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'bcelog':\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'jaccard':\n",
        "        loss_func = smp.losses.JaccardLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'weighted_bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'focal':\n",
        "        loss_func = smp.losses.FocalLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'dice+bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = loss1 + loss2\n",
        "\n",
        "    # elif func_name == 'tversky':\n",
        "\n",
        "    # elif func_name == 'hausdorff':\n",
        "\n",
        "    return loss\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, val_loader = build_dataloaders(config)\n",
        "        model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "        optimizer = build_optimizer(model, config)\n",
        "        loss_fn = build_loss_func(config.loss_function) #nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            avg_loss = train_epoch(model, train_loader, optimizer, config.loss_function)#, loss_fn)\n",
        "            # print(avg_loss)\n",
        "            metrics = {\"train/loss\": avg_loss, \"train/epoch\": epoch}\n",
        "            val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "            val_metrics = {\"val/val_loss\": val_loss,\n",
        "                           \"val/dice_score\": dice_score}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "class DictObject:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "def train(config, model_out_path):\n",
        "    if WANDB_LOG:\n",
        "        wandb.init(\n",
        "            project=\"LSEC_segmentation\",\n",
        "            config=config)\n",
        "        config = wandb.config\n",
        "    else:\n",
        "        config = DictObject(**config)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "    model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    # loss_fn = build_loss_func(config.loss_function) # nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "\n",
        "    best_dice_score = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.loss_function)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "\n",
        "        if dice_score > best_dice_score: # using dice score right now\n",
        "            save_state_dict(model, model_out_path)\n",
        "        best_dice_score = max(dice_score, best_dice_score)\n",
        "\n",
        "        dice_scores.append(dice_score)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Dice score: {dice_score}')\n",
        "        view_prediction(val_loader, model, device = DEVICE)\n",
        "        if WANDB_LOG:\n",
        "            wandb.log({\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": epoch,\n",
        "                       \"val/val_loss\": val_loss,\n",
        "                       \"val/dice_score\":dice_score,\n",
        "                       })\n",
        "    if WANDB_LOG:\n",
        "        wandb.finish()\n",
        "\n",
        "    return train_losses, val_losses, dice_scores"
      ],
      "metadata": {
        "id": "c6sxUMdmjwo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected\"\n",
        "data_split = 0.1\n",
        "\n",
        "# wandb sweep config\n",
        "sweep_config = {\n",
        "    'method': 'grid'#'bayes'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'val/dice_score',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        # 'values': ['adam', 'sgd']\n",
        "        'value': 'sgd'\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'value': 0.0186,\n",
        "        # # a flat distribution between min and max\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max': 0.02\n",
        "      },\n",
        "    'weight_decay': {\n",
        "        'value': 0.0189,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max' : 0.02,\n",
        "    },\n",
        "    # sgd parameters\n",
        "    'momentum':{\n",
        "        'value': 0.0722,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.06,\n",
        "        # 'max' : 0.08,\n",
        "    },\n",
        "\n",
        "    'dropout': {\n",
        "        'value': 0.0,\n",
        "        #   'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "        },\n",
        "    'epochs': {\n",
        "        'value': 12\n",
        "        },\n",
        "\n",
        "    # Dataloader params\n",
        "    'image_patches_path': {\n",
        "        'value': output_folder\n",
        "        },\n",
        "    'mask_patches_path': {\n",
        "        'value': output_folder\n",
        "        },\n",
        "    'data_split': {\n",
        "        'value': data_split\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'value': 6,\n",
        "        # # integers between min and max\n",
        "        # # with evenly-distributed logarithms\n",
        "        # 'distribution': 'q_log_uniform_values',\n",
        "        # 'q': 2, # the discrete step of the distribution\n",
        "        # 'min': 4,\n",
        "        # 'max': 8,\n",
        "      },\n",
        "    # Adam parameters\n",
        "    # 'beta1': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "    # 'beta2': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "        # 'fc_layer_size': {\n",
        "    #     'values': [128, 256, 512]\n",
        "    #     },\n",
        "    'image_denoising_methods': {\n",
        "        # 'value': 'median5',\n",
        "        'values': ['nlm_5_21', 'nlm_5_21+clahe', 'wzero']\n",
        "        # 'values': ['clahe+median5', 'med7', 'median5', 'median5+clahe', 'wave1_5+med3', 'wave2_5', 'wave2_5+med5'],#['wavelet', 'wavelet+median', 'advanced median'] # k waveletu jeste pridat ruzne thresholdy\n",
        "    },\n",
        "    'loss_function':{\n",
        "        'value': 'dice',\n",
        "        # 'values': ['dice', 'jaccard'],#['dice', 'bcelog', 'jaccard', 'weighted_bce', 'focal'],#, 'tversky', 'hausdorff']\n",
        "    },\n",
        "    'model_type':{\n",
        "        # 'values': ['plain_unet', 'resnet34+imagenet', 'resnet50+imagenet', 'inceptionv4+imagenet', 'efficientnet-b7+imagenet', 'resnet18+swsl', 'resnet18+imagenet','vgg11+imagenet'], # not great\n",
        "        # 'values': ['vgg11+imagenet', 'vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl','resnet34+imagenet','resnet50+ssl', 'resnext50_32x4d+ssl'], # good\n",
        "        # 'values': ['vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl'], # the best so far\n",
        "        'value': 'vgg13+imagenet',\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LSEC_segmentation\")"
      ],
      "metadata": {
        "id": "Y851Q6gvcd8h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae11122b-a25c-44e1-8711-8adba06b0a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: pkp1ed6w\n",
            "Sweep URL: https://wandb.ai/dpd/LSEC_segmentation/sweeps/pkp1ed6w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_CONNECTED = True\n",
        "wandb.agent(sweep_id, wandb_train, count=10)"
      ],
      "metadata": {
        "id": "EXcfFX2wo9P7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c82818c6948d41da949e1309dbca0ddd",
            "88688c884e7047a0a02ea0c506171c72",
            "11df07e3d03b473f8ffdfaeb4642ab0f",
            "6700ee5f603145d0a12712b4f7167dff",
            "66a5c4306b2748c8834c315f70e73e44",
            "ffc35a7cc32e40dc86f1931caf461157",
            "933a75dea95d46399e6a678c351fcc53",
            "440dddaf54014ecb87678e84e9152712",
            "df246588153f457e9fa7fc95adf12684",
            "85d9e9bf67cd414b826a2f55cdce8d92",
            "7c3820ffdc1b4edbae05302dacbb56ff",
            "c0ea1a4bc4a1492e90b3fa64caf3f814",
            "0c7e3e66b24d482796ac380e4b05a405",
            "7cdf3ff4f1ef4a4dba23454a929f1499",
            "aaac1dcc24ec4845b77e0998d1a3e27a",
            "ef5bc9dd339644859eb98e0285d0fbf0",
            "c00b1c621053413f91dae664763df271",
            "3bead9a0cfed40cd8dd9b77939bb2064",
            "4c714e44505a434b9d919d85e4b3ce77",
            "f459210b633640b38400cab26fe4af5e",
            "d58e082b372d46bd9a19d69d5a72bc3a",
            "57bbe7484e1e4d4b86db493be03a2ab5",
            "5a75e290766c476e892b758858a8760e",
            "0097bf01622d4ed09b15f938eea5f5ff",
            "dd1c8a2ecb3f45249a78aa8a1c6a4121",
            "d55590fe29194b4e9d64ec69eab4cfca",
            "f08ffc98666647dcbad96131d211a10e",
            "2caab7cd31a54b3cace40815a225a517",
            "1325fc38fd8340f195a5a901c194c381",
            "e54f3c75d7cd462aa9169b6989af3405",
            "8357a1666a904fd3a03da9e6e92598c5",
            "b4c76be2bfa94901911f2a018d54e70c"
          ]
        },
        "outputId": "2243e594-fad5-4062-a0a0-00d52fe5b8a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9pfz7835 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: nlm_5_21\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: dice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg13+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.0722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0189\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113399777777507, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c82818c6948d41da949e1309dbca0ddd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240309_202734-9pfz7835</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/9pfz7835' target=\"_blank\">zany-sweep-1</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/9pfz7835' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/9pfz7835</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:43<00:00, 12.2MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.10871555814534255, max=1.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df246588153f457e9fa7fc95adf12684"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>███▇▇▇▇▇▇▆▇▇▇▇▆▇▅▄▄▃▅▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train/loss</td><td>██▇▇▆▅▃▂▂▁▁▁</td></tr><tr><td>val/dice_score</td><td>▁▂▂▄▅▇██████</td></tr><tr><td>val/val_loss</td><td>██▇▇▆▄▃▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.19366</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/loss</td><td>0.19392</td></tr><tr><td>val/dice_score</td><td>0.86019</td></tr><tr><td>val/val_loss</td><td>0.17125</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zany-sweep-1</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/9pfz7835' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/9pfz7835</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240309_202734-9pfz7835/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iv89ttkd with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: nlm_5_21+clahe\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: dice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg13+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.0722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0189\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240309_203232-iv89ttkd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/iv89ttkd' target=\"_blank\">colorful-sweep-2</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/iv89ttkd' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/iv89ttkd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.011 MB uploaded\\r'), FloatProgress(value=0.1086612862966969, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c00b1c621053413f91dae664763df271"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█████▇▇█▇▇▇█▇▇▇▇█▆▇▇▆▇▇▇▇▅▆▅▆▅▄▄▆▂▃▂▁▂▂▄</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train/loss</td><td>████▇▇▇▆▅▄▂▁</td></tr><tr><td>val/dice_score</td><td>▁▁▂▂▂▃▄▅▇███</td></tr><tr><td>val/val_loss</td><td>████▇▇▇▆▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.54687</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/loss</td><td>0.28106</td></tr><tr><td>val/dice_score</td><td>0.84337</td></tr><tr><td>val/val_loss</td><td>0.22173</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">colorful-sweep-2</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/iv89ttkd' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/iv89ttkd</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240309_203232-iv89ttkd/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tlarqozh with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: wzero\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0186\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: dice\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/ROIs_manually_corrected\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg13+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.0722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0189\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240309_203644-tlarqozh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/tlarqozh' target=\"_blank\">zany-sweep-3</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/ktinya51</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/tlarqozh' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/tlarqozh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.1217577906325807, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd1c8a2ecb3f45249a78aa8a1c6a4121"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█████▇█▇█▇▇▇█▇█▇▆▆▆▇▄▆▅▄▅▃▅▃▂▂▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>train/loss</td><td>███▇▇▆▅▃▂▁▁▁</td></tr><tr><td>val/dice_score</td><td>▁▂▂▃▅▆▇█████</td></tr><tr><td>val/val_loss</td><td>███▇▇▆▄▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>0.17279</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/loss</td><td>0.20278</td></tr><tr><td>val/dice_score</td><td>0.84841</td></tr><tr><td>val/val_loss</td><td>0.17635</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zany-sweep-3</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/tlarqozh' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/tlarqozh</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240309_203644-tlarqozh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "KPwZ2wIG8htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wewnAeW8FnOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = False\n",
        "WANDB_CONNECTED = True\n",
        "WANDB_LOG = True\n",
        "image_patches_path = \"./gdrive/MyDrive/ROIs_manually_corrected\" # + image_denoising_methods\n",
        "mask_patches_path = \"./gdrive/MyDrive/ROIs_manually_corrected\"\n",
        "\n",
        "config = {\n",
        "    'batch_size' : 6,\n",
        "    'dropout' : 0.0,\n",
        "    'optimizer' : 'sgd',\n",
        "    'num_epochs' : 20,\n",
        "    'learning_rate' : 0.0186,\n",
        "    'weight_decay' : 0.0189,\n",
        "    'momentum' : 0.0722,\n",
        "    'data_split' : 0.1,\n",
        "    'image_patches_path': image_patches_path,\n",
        "    'mask_patches_path': mask_patches_path,\n",
        "    'image_denoising_methods': 'nlm_5_21',\n",
        "    'loss_function': 'dice+bce',\n",
        "    'model_type': 'vgg13+imagenet',\n",
        "}\n"
      ],
      "metadata": {
        "id": "-0Al6T1fdX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "    model = build_model(config['model_type'], config['dropout'], config['loss_function'])\n",
        "    model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activations = 'ReLU', out_activation=None).to(DEVICE)\n",
        "    load_state_dict(model, model_path)\n",
        "else:\n",
        "    # WANDB_LOG = False\n",
        "    train_losses, val_losses, dice_scores = train(config, model_path)"
      ],
      "metadata": {
        "id": "IaxEOPPRbMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0",
        "outputId": "614817e0-7942-413b-f045-75fd45046fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En",
        "outputId": "6496db18-5eec-4aaa-e492-ff7e020863c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0c88f9-a6f2-4042-9de2-32a81584d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:27<00:00, 19.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05.tif\"\n",
        "test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "# model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activation=None).to(DEVICE)\n",
        "model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "out_mask_path = inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "merge_original_mask(test_image_path, test_mask_path, output_folder)\n",
        "merge_masks(out_mask_path, test_mask_path, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Inference loop**"
      ],
      "metadata": {
        "id": "ZiGwUlrqBx1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "input_images_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "output_mask_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "output_folder = output_folder.strip()\n",
        "\n",
        "if not os.path.exists(output_mask_folder):\n",
        "    os.makedirs(output_mask_folder)\n",
        "if not os.path.exists(input_images_folder):\n",
        "    print(f'{input_images_folder} does not exist)')"
      ],
      "metadata": {
        "id": "9JALgAucLM_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference on full images\n",
        "# images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/sem_images\"\n",
        "# # test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "# output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/new_masks\"\n",
        "\n",
        "\n",
        "images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/sem_images\"\n",
        "masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/fen_masks\"\n",
        "output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/new_masks\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "image_names = [f for f in sorted(os.listdir(images_path)) if os.path.isfile(os.path.join(images_path, f))]\n",
        "mask_names = [f for f in sorted(os.listdir(masks_path)) if os.path.isfile(os.path.join(masks_path, f))]\n",
        "\n",
        "model = build_model('vgg19+imagenet', 0.0, 'dice')\n",
        "if torch.cuda.is_available():\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "# image_names = image_names[28:]\n",
        "# mask_names = mask_names[28:]"
      ],
      "metadata": {
        "id": "5YvVBtOmB2aG",
        "outputId": "374d8e05-6d7b-468a-f2ec-9384e9eae0d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:06<00:00, 92.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for image_name, mask_name in zip(image_names, mask_names):\n",
        "    print(image_name, mask_name)\n",
        "    image_path = os.path.join(images_path, image_name)\n",
        "    mask_path = os.path.join(masks_path, mask_name)\n",
        "    out_mask_path = inference_on_image_with_overlap(model, image_path, output_folder)\n",
        "    # print(out_mask_path)\n",
        "    # print(image_path, mask_path)\n",
        "    merge_original_mask(image_path, mask_path, output_folder)\n",
        "    merge_masks(out_mask_path, mask_path, output_folder)\n",
        "    # break"
      ],
      "metadata": {
        "id": "OyROv0wvZRUe",
        "outputId": "0a96d526-a0bf-4c2c-a917-2955c976a9f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "old12_NAC05_NE_04-2.tif old12_NAC05_NE_04-2.tif\n",
            "old12_NAC05_NE_04-3.tif old12_NAC05_NE_04-3.tif\n",
            "old12_NAC05_NE_05-1.tif old12_NAC05_NE_05-1.tif\n",
            "old12_NAC05_NE_05-2.tif old12_NAC05_NE_05-2.tif\n",
            "old12_NAC05_NE_05-3.tif old12_NAC05_NE_05-3.tif\n",
            "old12_kontrol_SE_01-1.tif old12_kontrol_SE_01-1.tif\n",
            "old12_kontrol_SE_01-2.tif old12_kontrol_SE_01-2.tif\n",
            "old12_kontrol_SE_01-3.tif old12_kontrol_SE_01-3.tif\n",
            "old12_kontrol_SE_01-4.tif old12_kontrol_SE_01-4.tif\n",
            "old12_kontrol_SE_02-1.tif old12_kontrol_SE_02-1.tif\n",
            "old12_kontrol_SE_02-2.tif old12_kontrol_SE_02-2.tif\n",
            "old12_kontrol_SE_02-3.tif old12_kontrol_SE_02-3.tif\n",
            "old12_kontrol_SE_03-1.tif old12_kontrol_SE_03-1.tif\n",
            "old12_kontrol_SE_03-2.tif old12_kontrol_SE_03-2.tif\n",
            "old12_kontrol_SE_03-3.tif old12_kontrol_SE_03-3.tif\n",
            "old13_kontrol_SE_04-1.tif old13_kontrol_SE_04-1.tif\n",
            "old13_kontrol_SE_04-2.tif old13_kontrol_SE_04-2.tif\n",
            "old13_kontrol_SE_04-3.tif old13_kontrol_SE_04-3.tif\n",
            "old13_kontrol_SE_04-4.tif old13_kontrol_SE_04-4.tif\n",
            "old13_kontrol_SE_05-1.tif old13_kontrol_SE_05-1.tif\n",
            "old13_kontrol_SE_05-2.tif old13_kontrol_SE_05-2.tif\n",
            "old13_kontrol_SE_05-3.tif old13_kontrol_SE_05-3.tif\n",
            "old13_kontrol_SW_01-1.tif old13_kontrol_SW_01-1.tif\n",
            "old13_kontrol_SW_01-2.tif old13_kontrol_SW_01-2.tif\n",
            "old13_kontrol_SW_01-3.tif old13_kontrol_SW_01-3.tif\n",
            "old13_kontrol_SW_01-4.tif old13_kontrol_SW_01-4.tif\n",
            "old13_kontrol_SW_01-5.tif old13_kontrol_SW_01-5.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Exclusion of fenestrations based on diameter and roundness**"
      ],
      "metadata": {
        "id": "StzWoL-FyWjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert the pixel size, and min and max fenestration diameters in nanometers:**\n",
        "\n",
        "#@markdown All fenestration with a smaller or larger diameter than the chosen range will be removed from the crated masks.\n",
        "#@markdown (Use dot '.' as the decimal separator, not comma ',').\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "pixel_size_nm = 10.62 #@param {type:\"number\"}\n",
        "min_diameter_nm = 105 #@param {type:\"number\"}\n",
        "max_diameter_nm = 500 #@param {type:\"number\"}\n",
        "min_roundness = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "mask_path = './gdrive/MyDrive/lsecs/mask_edit_test' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "mask_path = mask_path.strip()\n",
        "mask_names = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "\n",
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def remove_fenestrations(mask_path, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    contours = find_fenestration_contours(mask_path)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None:\n",
        "            center, axes, angle = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            # print(axes)\n",
        "            roundness = minor_axis_length/major_axis_length\n",
        "            if roundness >= min_roundness:\n",
        "                roundness_of_ellipses.append(roundness)\n",
        "            # rotation_angle = angle\n",
        "            diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "            # print(contour)\n",
        "            # print(diameter)\n",
        "            if diameter < min_d or diameter > max_d or roundness < min_roundness or np.isnan(diameter):\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "            else:\n",
        "                equivalent_diameters.append(diameter)\n",
        "                fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # print(equivalent_diameters)\n",
        "    if len(equivalent_diameters) > 0:\n",
        "        mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "        std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "        print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    return mask\n",
        "\n",
        "\n",
        "#TODO: ukazat statistiky pro celou slozku obrazku\n",
        "\n",
        "if not rewrite_images:\n",
        "    new_mask_path = os.path.join(mask_path, 'edited_masks')\n",
        "    os.makedirs(new_mask_path, exist_ok=True)\n",
        "else:\n",
        "    new_mask_path = mask_path\n",
        "# print(new_mask_path)\n",
        "for mask_name in mask_names:\n",
        "    # print(mask_name)\n",
        "    mask_path_full = os.path.join(mask_path, mask_name)\n",
        "    # print(mask_path)\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    edited_mask = remove_fenestrations(mask_path_full, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    # print(os.path.join(new_mask_path, mask_name))\n",
        "    cv.imwrite(os.path.join(new_mask_path, mask_name), edited_mask)\n",
        "    # cv.imwrite(os.path.join(new_mask_path, mask_name), mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Display the number of circles and their fitted ellipses\n",
        "# print(\"Number of fenestrations:\", len(contours))\n",
        "# print(\"Number of fitted ellipses:\", len(ellipses))\n"
      ],
      "metadata": {
        "id": "tRq0VTv6yrgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e1749b-72a0-4a92-c1bd-bd1b2427b25a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean equavalent diameter: 171 nm, std: 45 nm \n",
            "Mean equavalent diameter: 193 nm, std: 47 nm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Inference evaluation (dice score)**"
      ],
      "metadata": {
        "id": "RxiOKWQb6aFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert folders with ground truth masks and created masks for comparison:\n",
        "ground_truth_mask_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "created_mask_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "\n",
        "ground_truth_mask_folder = ground_truth_mask_folder.strip()\n",
        "created_mask_folder = created_mask_folder.strip()\n",
        "\n",
        "if not os.path.exists(created_mask_folder):\n",
        "    print(\"Ground truth folder does not exist\")\n",
        "    exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    print(\"Folder with created masks does not exist\")\n",
        "    exit()\n",
        "\n",
        "ground_truth_images = sorted([f for f in os.listdir(ground_truth_mask_folder) if os.path.isfile(os.path.join(ground_truth_mask_folder, f))])\n",
        "new_images = sorted([f for f in os.listdir(created_mask_folder) if os.path.isfile(os.path.join(created_mask_folder, f))])\n",
        "\n",
        "if len(ground_truth_images) != len(new_images):\n",
        "    print('The number of ground truths and created masks differs.')\n",
        "    exit()\n",
        "\n",
        "def compute_dice_score(image1, image2):\n",
        "    image1[image1 == 255] = 1\n",
        "    image2[image2 == 255] = 1\n",
        "    intersection_sum = np.logical_and(image1, image2).sum()\n",
        "    dice_score = 2*intersection_sum/(image1.sum() + image2.sum())\n",
        "    return dice_score\n",
        "\n",
        "dice_scores = []\n",
        "for ground_truth_mask_name, new_mask_name in zip(ground_truth_images, new_images):\n",
        "    print(f'Compare: {ground_truth_mask_name} - {new_mask_name}')\n",
        "    ground_truth_mask_path = os.path.join(ground_truth_images, ground_truth_mask_name)\n",
        "    new_mask_path = os.path.join(new_images, new_mask_name)\n",
        "    ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    new_mask = cv.imread(new_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    current_dice_score = compute_dice_score(ground_truth_mask, new_mask)\n",
        "    print(f'Dice score: {current_dice_score}')\n",
        "    dice_scores.append(current_dice_score)\n",
        "mean_dice = sum(dice_scores)/len(dice_scores)\n",
        "\n",
        "print(f'Mean dice score is {mean_dice}')\n",
        "\n"
      ],
      "metadata": {
        "id": "U1JDFWEQ6hbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c82818c6948d41da949e1309dbca0ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88688c884e7047a0a02ea0c506171c72",
              "IPY_MODEL_11df07e3d03b473f8ffdfaeb4642ab0f"
            ],
            "layout": "IPY_MODEL_6700ee5f603145d0a12712b4f7167dff"
          }
        },
        "88688c884e7047a0a02ea0c506171c72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a5c4306b2748c8834c315f70e73e44",
            "placeholder": "​",
            "style": "IPY_MODEL_ffc35a7cc32e40dc86f1931caf461157",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "11df07e3d03b473f8ffdfaeb4642ab0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_933a75dea95d46399e6a678c351fcc53",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_440dddaf54014ecb87678e84e9152712",
            "value": 1
          }
        },
        "6700ee5f603145d0a12712b4f7167dff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a5c4306b2748c8834c315f70e73e44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffc35a7cc32e40dc86f1931caf461157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "933a75dea95d46399e6a678c351fcc53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440dddaf54014ecb87678e84e9152712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df246588153f457e9fa7fc95adf12684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85d9e9bf67cd414b826a2f55cdce8d92",
              "IPY_MODEL_7c3820ffdc1b4edbae05302dacbb56ff"
            ],
            "layout": "IPY_MODEL_c0ea1a4bc4a1492e90b3fa64caf3f814"
          }
        },
        "85d9e9bf67cd414b826a2f55cdce8d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c7e3e66b24d482796ac380e4b05a405",
            "placeholder": "​",
            "style": "IPY_MODEL_7cdf3ff4f1ef4a4dba23454a929f1499",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "7c3820ffdc1b4edbae05302dacbb56ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aaac1dcc24ec4845b77e0998d1a3e27a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef5bc9dd339644859eb98e0285d0fbf0",
            "value": 1
          }
        },
        "c0ea1a4bc4a1492e90b3fa64caf3f814": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7e3e66b24d482796ac380e4b05a405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdf3ff4f1ef4a4dba23454a929f1499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaac1dcc24ec4845b77e0998d1a3e27a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef5bc9dd339644859eb98e0285d0fbf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c00b1c621053413f91dae664763df271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3bead9a0cfed40cd8dd9b77939bb2064",
              "IPY_MODEL_4c714e44505a434b9d919d85e4b3ce77"
            ],
            "layout": "IPY_MODEL_f459210b633640b38400cab26fe4af5e"
          }
        },
        "3bead9a0cfed40cd8dd9b77939bb2064": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58e082b372d46bd9a19d69d5a72bc3a",
            "placeholder": "​",
            "style": "IPY_MODEL_57bbe7484e1e4d4b86db493be03a2ab5",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "4c714e44505a434b9d919d85e4b3ce77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a75e290766c476e892b758858a8760e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0097bf01622d4ed09b15f938eea5f5ff",
            "value": 1
          }
        },
        "f459210b633640b38400cab26fe4af5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d58e082b372d46bd9a19d69d5a72bc3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57bbe7484e1e4d4b86db493be03a2ab5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a75e290766c476e892b758858a8760e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0097bf01622d4ed09b15f938eea5f5ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd1c8a2ecb3f45249a78aa8a1c6a4121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d55590fe29194b4e9d64ec69eab4cfca",
              "IPY_MODEL_f08ffc98666647dcbad96131d211a10e"
            ],
            "layout": "IPY_MODEL_2caab7cd31a54b3cace40815a225a517"
          }
        },
        "d55590fe29194b4e9d64ec69eab4cfca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1325fc38fd8340f195a5a901c194c381",
            "placeholder": "​",
            "style": "IPY_MODEL_e54f3c75d7cd462aa9169b6989af3405",
            "value": "0.011 MB of 0.011 MB uploaded\r"
          }
        },
        "f08ffc98666647dcbad96131d211a10e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8357a1666a904fd3a03da9e6e92598c5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4c76be2bfa94901911f2a018d54e70c",
            "value": 1
          }
        },
        "2caab7cd31a54b3cace40815a225a517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1325fc38fd8340f195a5a901c194c381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e54f3c75d7cd462aa9169b6989af3405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8357a1666a904fd3a03da9e6e92598c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c76be2bfa94901911f2a018d54e70c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}