{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for training a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECS)."
      ],
      "metadata": {
        "id": "-aHjwiD8IkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "J-Z80u6TN3Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a network, first connect to a GPU (**Runtime -> Change runtime time -> Hardware accelerator -> GPU**).\n",
        "\n",
        "If you are using a pretrained network for inference and not training, being connected only to a **CPU** is slower, but possible."
      ],
      "metadata": {
        "id": "NUZeORlUN_LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "-gq1-hflPdMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "LHteKyDySYvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f252b4-a814-47a4-b992-b1eb41068d7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N5QvbqMfiA4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cf3c4a-e9d0-4a8f-c52e-4e909c854d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.0.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.17.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=d10737c976a0d4804afe058807f3add0b040d44b7bfb015c3418f3acddbcb436\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=eab5116aea026a6512c794083e62f6a17893db4ef4a508253ac5a24bddd4bbe7\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from numba import njit\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "\n",
        "# gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = False\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transofrm=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transofrm\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask[mask == 255.0] = 1\n",
        "        mask /= 255\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_dir, mask_dir, split, batch_size, num_workers=4, pin_memory=True): # TODO: check these parameters\n",
        "    data = MyDataset(\n",
        "        image_dir=img_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        transofrm=None\n",
        "    )\n",
        "\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        range(len(data)),\n",
        "        test_size=split,\n",
        "        random_state=1\n",
        "    )\n",
        "    train_data = TransformDataset(Subset(data, train_indices), train_transform)\n",
        "    val_data = TransformDataset(Subset(data, test_indices), val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_indices\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "        [\n",
        "            A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # A.Affine(shear=(0.5,1)),\n",
        "            # A.Affine(scale=(-10, 10)),\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0, # normalization to [0, 1]\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# test_transform = A.Compose(\n",
        "#     [\n",
        "#     A.Normalize(\n",
        "#       mean = 0.0,\n",
        "#       std = 1.0,\n",
        "#       max_pixel_value=255.0,\n",
        "#     ),\n",
        "#         ToTensorV2()\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations if needed\n",
        "])\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    print('merging masks')\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - border_size:height, :])\n",
        "    bottom_zeros = np.zeros((bottom_edge-border_size, width), dtype = image.dtype)\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border, bottom_zeros))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - border_size:width])\n",
        "    right_zeros = np.zeros((top_bottom_mirrored.shape[0], right_edge-border_size), dtype = image.dtype)\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border, right_zeros))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, filter_type):\n",
        "    window_size = 512\n",
        "    oh, ow = 50, 50\n",
        "    # out_crop =\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    # bottom_edge = (image_height + oh) % (window_size - oh)\n",
        "    # right_edge = (image_height + ow) % (window_size - ow)\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    # print(mirrored_image.shape)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    # tryout = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "            # print(x, y)\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    # if original_height != window_size:\n",
        "                    weighting_window = top_left\n",
        "                    # print('top left')\n",
        "                elif y == image_width - window_size:\n",
        "                    # print('top right')\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "                    # print('top ')\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                    # print('bottom left')\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                    # print('bottom right')\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "                    # print('bottom')\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "                # print('left')\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "                # print('right')\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "                # print('middle')\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "            if filter_type == 'nlm':\n",
        "                square_section = nlm_filt(square_section)\n",
        "            elif filter_type == 'med5':\n",
        "                square_section = cv.medianBlur(square_section, 5) # TODO: prehodit tohle, at se to dela jednou pro celej obrazek, ne pro patche?\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            # output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # cv2_imshow(output_pil)\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "    # Crop\n",
        "    # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    # weights = weights[:original_height, :original_width]*255\n",
        "    # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # Apply weights\n",
        "    # output_probs /= weights\n",
        "\n",
        "    # Create image from mask\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    return output_mask\n",
        "\n",
        "    # filename_ext = os.path.basename(image_path)\n",
        "    # filename, ext = os.path.splitext(filename_ext)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_mirrored\"+ext), mirrored_image)\n",
        "\n",
        "    # # Merge image with created mask\n",
        "    # out_mask_path = os.path.join(output_folder, filename+\"_new_mask\"+ext)\n",
        "    # merge = merge_images(input_image, output_mask)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "    # cv.imwrite(out_mask_path, output_mask)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "    # return out_mask_path\n",
        "\n",
        "# def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "#     window_size = 512\n",
        "#     oh, ow = 124, 124\n",
        "#     input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#     image_height, image_width = input_image.shape\n",
        "#     original_height, original_width = image_height, image_width\n",
        "#     bottom_edge = image_height % (window_size - oh)\n",
        "#     right_edge = image_width % (window_size - ow)\n",
        "#     mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "#     mirrored_image[:image_height, :image_width] = input_image\n",
        "#     mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "#     mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "#     image_height += bottom_edge\n",
        "#     image_width += right_edge\n",
        "#     weights = np.zeros((image_height, image_width))\n",
        "#     # tryout = np.zeros((image_height, image_width))\n",
        "#     output_probs = np.zeros((image_height, image_width))\n",
        "#     output_mask = np.zeros((image_height, image_width))\n",
        "#     middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "#     for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "#         for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "#             # Choose weighting window\n",
        "#             if x == 0:\n",
        "#                 if y == 0:\n",
        "#                     if original_height != window_size:\n",
        "#                         weighting_window = top_left\n",
        "#                     else:\n",
        "#                         weighting_window = np.ones((window_size, window_size))\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = top_right\n",
        "#                 else:\n",
        "#                     weighting_window = top\n",
        "#             elif x == window_size - oh - 1:\n",
        "#                 if y == 0:\n",
        "#                     weighting_window = bottom_left\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = bottom_right\n",
        "#                 else:\n",
        "#                     weighting_window = bottom\n",
        "#             elif y == 0:\n",
        "#                 weighting_window = left\n",
        "#             elif y == window_size - ow - 1:\n",
        "#                 weighting_window = right\n",
        "#             else:\n",
        "#                 weighting_window = middle\n",
        "#             square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "#             weights[x:x + window_size, y:y + window_size] = weighting_window\n",
        "#             # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "#             square_section = preprocess_image(square_section)\n",
        "#             square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "#             # Forward pass through the model\n",
        "#             with torch.no_grad():\n",
        "#                 output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "#             # Scale the probablity to 0-255\n",
        "#             output = output*255\n",
        "#             output = output.to(torch.uint8)\n",
        "#             output_pil = output.squeeze(0).cpu().numpy()\n",
        "#             output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()*weighting_window\n",
        "#     # Crop\n",
        "#     output_probs = output_probs[:original_height, :original_width]\n",
        "#     # weights = weights[:original_height, :original_width]*255\n",
        "#     # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "#     # Apply weights\n",
        "#     # output_probs /= weights\n",
        "\n",
        "#     # Create image from mask\n",
        "#     output_mask = np.where(output_probs > 127, 255, 0)\n",
        "#     output_mask = output_mask.astype(np.uint8)\n",
        "#     filename_ext = os.path.basename(image_path)\n",
        "#     filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "#     # Merge image with created mask\n",
        "#     out_mask_path = os.path.join(output_folder, filename+\"_mask\"+ext)\n",
        "#     merge = merge_images(input_image, output_mask)\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_merge\"+ext), merge)\n",
        "\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "#     cv.imwrite(out_mask_path, output_mask)\n",
        "#     # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "#     return out_mask_path\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # image = nlm_filt(image)\n",
        "    # image = wavelet_denoise(image, threshold=1.5)\n",
        "    # image = apply_clahe(image)\n",
        "    # image = cv.medianBlur(image, 5)\n",
        "    return image\n",
        "\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "    image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    fenestration_area_thresh = 0.0 #0.01\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        # if image_name.endswith(\".tif\"): # TODO: tohle mozna odstranit\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "        # strides = (patch_size * width , patch_size)\n",
        "\n",
        "        # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "        #              strides=img.strides + img.strides, writeable=False)\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                img_patch = img_strided[i, j]\n",
        "                mask_patch = mask_strided[i, j]\n",
        "                # Compute the percentage of white pixels\n",
        "                fenestration_area = np.sum(mask_patch == 255)\n",
        "                # print(fenestration_area)\n",
        "                # fenestration_percentage = fenestration_area/patch_area\n",
        "                if fenestration_area >= fenestration_area_thresh:\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "                else:\n",
        "                    print(\"not writing patch\")\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "# This is the official implementation of BoundaryDOULoss https://arxiv.org/pdf/2308.00220.pdf\n",
        "# Taken from: https://github.com/sunfan-bvb/BoundaryDoULoss/tree/main\n",
        "class BoundaryDoULoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BoundaryDoULoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i\n",
        "            tensor_list.append(temp_prob.unsqueeze(1))\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _adaptive_size(self, score, target):\n",
        "        kernel = torch.Tensor([[0,1,0], [1,1,1], [0,1,0]])\n",
        "        padding_out = torch.zeros((target.shape[0], target.shape[-2]+2, target.shape[-1]+2))\n",
        "        padding_out[:, 1:-1, 1:-1] = target\n",
        "        h, w = 3, 3\n",
        "\n",
        "        Y = torch.zeros((padding_out.shape[0], padding_out.shape[1] - h + 1, padding_out.shape[2] - w + 1)).cuda()\n",
        "        for i in range(Y.shape[0]):\n",
        "            Y[i, :, :] = torch.conv2d(target[i].unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0).cuda(), padding=1)\n",
        "        Y = Y * target\n",
        "        Y[Y == 5] = 0\n",
        "        C = torch.count_nonzero(Y)\n",
        "        S = torch.count_nonzero(target)\n",
        "        smooth = 1e-5\n",
        "        alpha = 1 - (C + smooth) / (S + smooth)\n",
        "        alpha = 2 * alpha - 1\n",
        "\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        alpha = min(alpha, 0.8)  ## We recommend using a truncated alpha of 0.8, as using truncation gives better results on some datasets and has rarely effect on others.\n",
        "        loss = (z_sum + y_sum - 2 * intersect + smooth) / (z_sum + y_sum - (1 + alpha) * intersect + smooth)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "        target = target.squeeze(1)\n",
        "\n",
        "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            loss += self._adaptive_size(inputs[:, 0], target[:, 0])#(inputs[:, i], target[:, i])\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_dice_score = 0.0\n",
        "    total_samples = 0\n",
        "    eps = 1e-8\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            out = model(x)\n",
        "            loss = get_loss(out, y, loss_fn)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            if WANDB_CONNECTED or WANDB_LOG:\n",
        "                wandb.log({\"val/batch loss\": loss.item()})\n",
        "\n",
        "            predicted_probs = torch.sigmoid(out)\n",
        "            predicted = (predicted_probs > 0.5).float()\n",
        "            intersection = torch.sum(predicted * y)\n",
        "            dice_score = (2.0 * intersection + eps) / (torch.sum(predicted) + torch.sum(y) + eps)\n",
        "            total_dice_score += dice_score.item() * x.size(0)\n",
        "\n",
        "            total_samples += x.size(0)\n",
        "    model.train()\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    average_dice_score = total_dice_score / total_samples\n",
        "\n",
        "    return average_loss, average_dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def validate_model(model, loader, loss_fn):\n",
        "#     num_correct = 0\n",
        "#     num_pixels = 0\n",
        "#     dice_score = 0\n",
        "#     model.eval()\n",
        "#     running_loss = 0\n",
        "#     losses = []\n",
        "#     dice_scores = []\n",
        "#     with torch.no_grad():\n",
        "#         for idx, (x, y) in enumerate(loader):\n",
        "#             x = x.to(DEVICE)\n",
        "#             y = y.to(DEVICE).unsqueeze(1)\n",
        "#             # Forward\n",
        "#             preds = model(x)\n",
        "#             loss = get_loss(preds, y, loss_fn)\n",
        "#             # running_loss += loss.cpu()\n",
        "#             losses.append(loss.cpu())\n",
        "\n",
        "#             preds = torch.sigmoid(preds)\n",
        "#             preds = (preds > 0.5).float()\n",
        "\n",
        "#             # num_correct += (preds == y).sum()\n",
        "#             # num_pixels += torch.numel(preds)\n",
        "#             dice_score += (2*(preds*y).sum()) / ((preds+y).sum() + 1e-8) # this is a better predictor\n",
        "#             dice_scores.append(dice_score.cpu())\n",
        "#     # print(\n",
        "#     #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "#     # )\n",
        "#     # dice_score = dice_score/(idx+1)\n",
        "#     # val_loss = running_loss/(idx+1)\n",
        "#     val_loss = np.mean(np.array(losses))\n",
        "#     dice_score = np.mean(np.array(dice_scores))\n",
        "\n",
        "#     # dice_score = dice_score/len(loader)\n",
        "#     # val_loss = running_loss/len(loader) #TODO: not sure this is correct(dividing by batch size?)\n",
        "#     # print(f\"Dice score is {dice_score}\")\n",
        "#     # val_losses.append(running_loss/len(loader))\n",
        "#     # dice_scores.append(dice_score.cpu())\n",
        "#     model.train()\n",
        "#     return val_loss, dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            # output = torch.softmax(model(x), dim=1)\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# def getClassWeights(mask_path, train_indices):\n",
        "#     mask_dir_list = sorted(os.listdir(mask_path))\n",
        "#     class_count = np.zeros(2, dtype=int)\n",
        "#     for i in train_indices:\n",
        "#         mask = cv.imread(os.path.join(mask_path, mask_dir_list[i]), cv.IMREAD_GRAYSCALE) #np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "#         mask[mask == 255.0] = 1\n",
        "#         class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "#         class_count[1] += mask.sum()\n",
        "\n",
        "#     n_samples = class_count.sum()\n",
        "#     n_classes = 2\n",
        "\n",
        "#     class_weights = n_samples / (n_classes * class_count)\n",
        "#     return torch.from_numpy(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "            else:\n",
        "                ellipses.append(None)\n",
        "        else:\n",
        "            ellipses.append(None)\n",
        "    return ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "    # image = cv.cvtColor(seg_mask, cv.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    # min_contour_area = 10\n",
        "    # filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
        "    # return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    # print(contour_centers)\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01_original_mask.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "fenestration_areas_from_ellipses = []\n",
        "\n",
        "for ellipse in ellipses:\n",
        "    center, axes, angle = ellipse\n",
        "    # center_x, center_y = center\n",
        "    major_axis_length, minor_axis_length = axes\n",
        "    roundness = minor_axis_length/major_axis_length\n",
        "    roundness_of_ellipses.append(roundness)\n",
        "    # rotation_angle = angle\n",
        "    diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "    equivalent_diameters.append(diameter)\n",
        "    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "\n",
        "# show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ],
      "metadata": {
        "id": "BtPrBpQBcsmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "655b8a34-3491-4a67-e939-1aeab3834918"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fenestrations: 0\n",
            "Number of fitted ellipses: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Wavelet filtering debug\n",
        "\n",
        "# image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "# images = os.listdir(image_folder)\n",
        "# image_name = images[0]\n",
        "# image = cv.imread(os.path.join(image_folder, image_name), cv.IMREAD_GRAYSCALE)\n",
        "# # cv2_imshow(image)\n",
        "\n",
        "# denoised_image = wavelet_denoise(image)\n",
        "# # cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch, activation):\n",
        "    if activation == 'ReLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    elif activation == 'GeLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none'),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none')\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device, dropout_probability, activations, out_activation):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(p=dropout_probability)\n",
        "        self.activations = activations\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64, activation=activations)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128, activation=activations)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256, activation=activations)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512, activation=activations)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024, activation=activations)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512, activation=activations)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256, activation=activations)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128, activation=activations)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64, activation=activations)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "        self.out_activation = out_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        p2 = self.dropout(p2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        p3 = self.dropout(p3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        p4 = self.dropout(p4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        cat1 = self.dropout(cat1)\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        cat2 = self.dropout(cat2)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        if self.out_activation == 'sigmoid':\n",
        "            conv_1x1 = torch.sigmoid(conv_1x1)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Create training patches**"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "#@markdown If you want to create new 512x512 patches, check the following box. If you already have image patches, insert the folders below.\n",
        "create_patches = True # @param {type:\"boolean\"}\n",
        "\n",
        "training_images = './gdrive/MyDrive/lsecs/cropped_selections/images' #@param {type:\"string\"}\n",
        "training_masks = './gdrive/MyDrive/lsecs/cropped_selections/masks' #@param {type:\"string\"}\n",
        "patches_folder = './gdrive/MyDrive/lsecs/cropped_selections/patches' #@param {type:\"string\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "training_masks = training_masks.strip()\n",
        "patches_folder = patches_folder.strip()\n",
        "\n",
        "if not os.path.exists(training_images):\n",
        "    print(f'{training_images} does not exist.')\n",
        "if not os.path.exists(training_masks):\n",
        "    print(f'{training_masks} does not exist.')\n"
      ],
      "metadata": {
        "id": "4qR6zmj4U-pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATCHES_TO_DISK = True\n",
        "\n",
        "# training_images = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "# training_masks = \"./gdrive/MyDrive/ROIs_manually_corrected/train_masks\"\n",
        "\n",
        "if create_patches:\n",
        "    patch_size = 512  # Define your patch size here\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "        print(f'Saving patches to {output_folder}')\n",
        "    else:\n",
        "        output_folder = os.getcwd()\n",
        "    image_patches_path, mask_patches_path = create_image_patches(training_images, training_masks, output_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "    image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, 'mask_patches')\n",
        "\n",
        "print(f'Training image patches are located in {image_patches_path}')\n",
        "print(f'Training mask patches are located in {mask_patches_path}')"
      ],
      "metadata": {
        "id": "UzznzOTP4s53",
        "outputId": "188efa10-1178-44b4-d72b-773250102a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving patches to ./gdrive/MyDrive/lsecs/cropped_selections/patches\n",
            "Training image patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches/image_patches\n",
            "Training mask patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches/mask_patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "WAJp45Xo8p_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "def build_optimizer(model, config, beta1=None, beta2=None):\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=config.learning_rate,\n",
        "                              weight_decay=config.weight_decay,\n",
        "                              momentum=config.momentum)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=config.learning_rate,\n",
        "                               betas=(config.beta1, config.beta2),\n",
        "                               weight_decay=config.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "# TRAIN_LOADER = train_loader\n",
        "# VAL_LOADER = val_loader\n",
        "def build_dataloaders(config): # TODO: check if there is a better way to do this\n",
        "    image_patches_path = os.path.join(config.image_patches_path, 'patches_'+ config.image_denoising_methods)\n",
        "    mask_patches_path = os.path.join(config.mask_patches_path, 'patches_'+ config.image_denoising_methods)\n",
        "    image_patches_path = os.path.join(image_patches_path, 'image_patches')\n",
        "    mask_patches_path = os.path.join(mask_patches_path, 'mask_patches')\n",
        "    train_loader, val_loader, _ = get_loaders(\n",
        "        image_patches_path,\n",
        "        mask_patches_path,\n",
        "        config.data_split,\n",
        "        config.batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=False\n",
        "    )\n",
        "    return train_loader, val_loader # this is the simplest way to do it, wandb train cannot take any arguments\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    # model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "    losses = []\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            # TODO: change this\n",
        "            # loss = F.nll_loss(torch.sigmoid(predictions), targets)\n",
        "            loss = get_loss(predictions, targets, loss_fn)\n",
        "            # loss = loss_fn(predictions, targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # running_loss += loss.item()\n",
        "        # losses.append(loss.item())\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "        if WANDB_CONNECTED or WANDB_LOG:\n",
        "            wandb.log({\"train/batch loss\": loss.item()})\n",
        "\n",
        "    # number_of_batches = batch_idx+1\n",
        "    # mean_loss = np.mean(np.array(losses))\n",
        "    mean_loss = total_loss / total_samples\n",
        "    return mean_loss\n",
        "    # return running_loss/number_of_batches\n",
        "\n",
        "def build_model(model_name, dropout, loss_func):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    if '+' in model_name:\n",
        "        name_parts = model_name.split('+')\n",
        "        encoder = name_parts[-2]\n",
        "        if name_parts[-1] == 'imagenet' or name_parts[-1] == 'ssl':\n",
        "            weights = name_parts[-1]\n",
        "        else:\n",
        "            weights = None\n",
        "    # if loss_func == 'bcelog' or loss_func == 'weighted_bce':\n",
        "    #     out_activation = None\n",
        "    # else:\n",
        "    #     out_activation = 'sigmoid'\n",
        "    out_activation = None\n",
        "    # print(weights)\n",
        "    # print(out_activation)\n",
        "    if model_name == 'plain_unet':\n",
        "        model = UNET(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                device=DEVICE,\n",
        "                dropout_probability=dropout,\n",
        "                activations='ReLU',\n",
        "                out_activation=out_activation).to(DEVICE)\n",
        "    elif 'Unet++' in model_name:\n",
        "        model = smp.UnetPlusPlus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'Linknet' in model_name:\n",
        "        model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'FPN' in model_name:\n",
        "        model = smp.FPN(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'DeepLabV3' in model_name:\n",
        "        model = smp.DeepLabV3(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    else:\n",
        "        model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def get_loss(pred, target, func_name):\n",
        "    loss_func = None\n",
        "    if func_name == 'dice':\n",
        "        loss_func = smp.losses.DiceLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'bcelog':\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'jaccard':\n",
        "        loss_func = smp.losses.JaccardLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'weighted_bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'focal':\n",
        "        loss_func = smp.losses.FocalLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'dice+bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == '40dice+60bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.4*loss1 + 0.6*loss2\n",
        "    elif func_name == '60dice+40bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.6*loss1 + 0.4*loss2\n",
        "    elif func_name == 'dice+focal':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = smp.losses.FocalLoss(mode='binary')\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == 'tversky':\n",
        "        loss_func = smp.losses.TverskyLoss(mode='binary', alpha=0.7, beta=0.3)\n",
        "        loss = loss_func(pred, target)\n",
        "    # elif func_name == 'hausdorff':\n",
        "\n",
        "    return loss\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, val_loader = build_dataloaders(config)\n",
        "        model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "        optimizer = build_optimizer(model, config)\n",
        "        # loss_fn = get_loss(config.loss_function)\n",
        "\n",
        "        best_dice_score = 0\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "            avg_loss = train_epoch(model, train_loader, optimizer, config.loss_function)#, loss_fn)\n",
        "            # print(avg_loss)\n",
        "            metrics = {\"train/loss\": avg_loss, \"train/epoch\": epoch}\n",
        "            val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "            if dice_score > best_dice_score: # using dice score right now\n",
        "                torch.save(model, os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "                # save_state_dict(model, model_out_path)\n",
        "                best_dice_score = max(dice_score, best_dice_score)\n",
        "\n",
        "            val_metrics = {\"val/val_loss\": val_loss,\n",
        "                           \"val/dice_score\": dice_score}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "class DictObject:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "def train(config, model_out_path):\n",
        "    if WANDB_LOG:\n",
        "        wandb.init(\n",
        "            project=\"LSEC_segmentation\",\n",
        "            config=config)\n",
        "        config = wandb.config\n",
        "    else:\n",
        "        config = DictObject(**config)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "    model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    # loss_fn = build_loss_func(config.loss_function) # nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "\n",
        "    best_dice_score = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "\n",
        "    for epoch in range(config.num_epochs):\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.loss_function)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "\n",
        "        if dice_score > best_dice_score: # using dice score right now\n",
        "            # save_state_dict(model, model_out_path)\n",
        "            torch.save(model, os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "        best_dice_score = max(dice_score, best_dice_score)\n",
        "\n",
        "        dice_scores.append(dice_score)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Dice score: {dice_score}')\n",
        "        # view_prediction(val_loader, model, device = DEVICE)\n",
        "        print(train_loss, val_loss)\n",
        "        if WANDB_LOG:\n",
        "            wandb.log({\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": epoch,\n",
        "                       \"val/val_loss\": val_loss,\n",
        "                       \"val/dice_score\":dice_score,\n",
        "                       })\n",
        "    if WANDB_LOG:\n",
        "        wandb.finish()\n",
        "\n",
        "    return train_losses, val_losses, dice_scores"
      ],
      "metadata": {
        "id": "c6sxUMdmjwo6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "data_split = 0.2\n",
        "\n",
        "# wandb sweep config\n",
        "sweep_config = {\n",
        "    'method': 'grid'#'bayes'\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'val/dice_score',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        # 'values': ['adam', 'sgd']\n",
        "        'value': 'sgd'\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'value': 0.02,\n",
        "        # # a flat distribution between min and max\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max': 0.02\n",
        "      },\n",
        "    'weight_decay': {\n",
        "        # 'value': 0.0189,\n",
        "        'value': 0.01\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max' : 0.02,\n",
        "    },\n",
        "    # sgd parameters\n",
        "    'momentum':{\n",
        "        'value': 0.07,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.06,\n",
        "        # 'max' : 0.08,\n",
        "    },\n",
        "\n",
        "    'dropout': {\n",
        "        'value': 0.0,\n",
        "        #   'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "        },\n",
        "    'epochs': {\n",
        "        'value': 20,\n",
        "        },\n",
        "\n",
        "    # Dataloader params\n",
        "    'image_patches_path': {\n",
        "        'value': output_folder\n",
        "        },\n",
        "    'mask_patches_path': {\n",
        "        'value': output_folder\n",
        "        },\n",
        "    'data_split': {\n",
        "        'value': data_split\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'value': 8,\n",
        "        # # integers between min and max\n",
        "        # # with evenly-distributed logarithms\n",
        "        # 'distribution': 'q_log_uniform_values',\n",
        "        # 'q': 2, # the discrete step of the distribution\n",
        "        # 'min': 4,\n",
        "        # 'max': 8,\n",
        "      },\n",
        "    # Adam parameters\n",
        "    # 'beta1': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "    # 'beta2': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "        # 'fc_layer_size': {\n",
        "    #     'values': [128, 256, 512]\n",
        "    #     },\n",
        "    'image_denoising_methods': {\n",
        "        'value': 'med5',\n",
        "        # 'values': ['no_denoise', 'med5']\n",
        "        # 'values': ['nlm', 'med5']\n",
        "        # 'values': ['clahe+median5', 'med7', 'median5', 'median5+clahe', 'wave1_5+med3', 'wave2_5', 'wave2_5+med5'],#['wavelet', 'wavelet+median', 'advanced median'] # k waveletu jeste pridat ruzne thresholdy\n",
        "    },\n",
        "    'loss_function':{\n",
        "        # 'value': 'dice+bce',\n",
        "        # 'values': ['dice', 'dice+bce', 'dice+focal', 'tversky'],#['dice', 'bcelog', 'jaccard', 'weighted_bce', 'focal'],#, 'tversky', 'hausdorff']\n",
        "        # 'values': ['dice', 'dice+bce', 'focal','bcelog', 'dice+focal'],\n",
        "        'values': ['focal','bcelog', 'dice+focal'],\n",
        "\n",
        "\n",
        "    },\n",
        "    'model_type':{\n",
        "        # 'values': ['plain_unet', 'resnet34+imagenet', 'resnet50+imagenet', 'inceptionv4+imagenet', 'efficientnet-b7+imagenet', 'resnet18+swsl', 'resnet18+imagenet','vgg11+imagenet'], # not great\n",
        "        # 'values': ['vgg11+imagenet', 'vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl','resnet34+imagenet','resnet50+ssl', 'resnext50_32x4d+ssl'], # good\n",
        "        'values': ['vgg11+imagenet','vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl',  'resnet34+imagenet','resnet50+ssl', 'efficientnet-b7+imagenet'], # the best so far\n",
        "        # 'value': 'vgg19+imagenet',\n",
        "    },\n",
        "    'model_path':{\n",
        "        'value': './gdrive/MyDrive/lsecs',\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LSEC_segmentation\")"
      ],
      "metadata": {
        "id": "Y851Q6gvcd8h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "53f777d4-0bdb-439f-e525-6116c25a50a6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: mpmb1e63\n",
            "Sweep URL: https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_CONNECTED = True\n",
        "wandb.agent(sweep_id, wandb_train, count=510)"
      ],
      "metadata": {
        "id": "EXcfFX2wo9P7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "246b35c19f4b4ba8b7c48a4310451269",
            "26eaf515f9444324a8ad0536dbbe74ec",
            "e9fac2f00a784411abb35dde97f2dd19",
            "a7c916c702e247d487568f7f6a3126e5",
            "1c039b5b24644486b789cd34474c2858",
            "cd2a044f2d7d47e6aaf450d46b1b345c",
            "3de76e29818843f6ab994b98871a9579",
            "2e64dac7cd204e7f83bd642537e278e4",
            "f3cc78f395474bf5ab4edf4c75fe9d1f",
            "f9dbd579c5cd40b8a018d1230aed35cb",
            "06aac424c6f4445ba136aa7387c65fc4",
            "2ba7970bcf5543609be6b28332fd28a8",
            "6c4f9e3c26c54d2cb7ca8b536b7adf0e",
            "a160caeaff9645de81cbcb24d6b8d528",
            "02900c7fdf1d4bc2baa6f4227c6eae27",
            "5be96962a9be4a15b4991af4ff306d5d",
            "608148c4eadf47e8a61674f980f45c23",
            "1ae04d790b814c939c9b0dafb224ff97",
            "fd70c1be9a4b4214bf3aec5d967107f8",
            "78ee3bc72b4447cdb347c73a94286153",
            "545f2940ffba442e835e2f2ed048b03d",
            "a88c10b718044d8187ebe28aff2cc81d",
            "02bf1dade9e1424d956a7d717e0a6d21",
            "43879971e8d54a65b0cf54739f4679c6"
          ]
        },
        "outputId": "17db3493-5612-4a3f-8879-990cc8ae5cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ftj6sflx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: focal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg11+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarketakvasova1\u001b[0m (\u001b[33mdpd\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240502_162101-ftj6sflx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/ftj6sflx' target=\"_blank\">swept-sweep-1</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/ftj6sflx' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/ftj6sflx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg11-bbd30ac9.pth\" to /root/.cache/torch/hub/checkpoints/vgg11-bbd30ac9.pth\n",
            "100%|██████████| 507M/507M [00:02<00:00, 215MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "246b35c19f4b4ba8b7c48a4310451269",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.012 MB uploaded\\r'), FloatProgress(value=0.10239924729496629, max=1.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>█▄▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/loss</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/batch loss</td><td>█▆▄▄▄▃▂▂▃▂▂▂▃▂▁▁▂▂▂▂▂▂▂▁▂▂▂▁▂▂▂▁▂▂▁▁▁▁▂▁</td></tr><tr><td>val/dice_score</td><td>▁▄▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>val/val_loss</td><td>█▅▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>0.01598</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/loss</td><td>0.01204</td></tr><tr><td>val/batch loss</td><td>0.00703</td></tr><tr><td>val/dice_score</td><td>0.89352</td></tr><tr><td>val/val_loss</td><td>0.00881</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-sweep-1</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/ftj6sflx' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/ftj6sflx</a><br/> View project at: <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240502_162101-ftj6sflx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6eb8s9qo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: focal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg13+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240502_163729-6eb8s9qo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/6eb8s9qo' target=\"_blank\">robust-sweep-2</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/6eb8s9qo' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/6eb8s9qo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:11<00:00, 47.6MB/s]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3cc78f395474bf5ab4edf4c75fe9d1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>█▆▅▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/batch loss</td><td>█▆▄▄▄▃▂▂▃▂▂▂▃▂▁▁▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▁▂▂▁▁▁▁▂▁</td></tr><tr><td>val/dice_score</td><td>▁▄▅▆▇▇▇▇▇███████████</td></tr><tr><td>val/val_loss</td><td>█▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>0.00817</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/loss</td><td>0.01219</td></tr><tr><td>val/batch loss</td><td>0.00793</td></tr><tr><td>val/dice_score</td><td>0.88786</td></tr><tr><td>val/val_loss</td><td>0.00981</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-sweep-2</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/6eb8s9qo' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/6eb8s9qo</a><br/> View project at: <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240502_163729-6eb8s9qo/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zm8bdc0o with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: focal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg16+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240502_164734-zm8bdc0o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/zm8bdc0o' target=\"_blank\">olive-sweep-3</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/zm8bdc0o' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/zm8bdc0o</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 237MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "608148c4eadf47e8a61674f980f45c23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>█▅▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/batch loss</td><td>█▆▄▄▄▃▂▂▃▂▂▂▂▂▁▁▂▁▂▂▂▂▂▁▂▂▁▁▂▁▂▁▂▂▁▁▁▁▂▁</td></tr><tr><td>val/dice_score</td><td>▁▄▆▇▇▇▇▇████████████</td></tr><tr><td>val/val_loss</td><td>█▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>0.00835</td></tr><tr><td>train/epoch</td><td>19</td></tr><tr><td>train/loss</td><td>0.01138</td></tr><tr><td>val/batch loss</td><td>0.00745</td></tr><tr><td>val/dice_score</td><td>0.89044</td></tr><tr><td>val/val_loss</td><td>0.00911</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-sweep-3</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/zm8bdc0o' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/zm8bdc0o</a><br/> View project at: <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240502_164734-zm8bdc0o/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f96tn7wl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_split: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.02\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: focal\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: vgg19+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240502_165839-f96tn7wl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/f96tn7wl' target=\"_blank\">toasty-sweep-4</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/mpmb1e63</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/f96tn7wl' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/f96tn7wl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:04<00:00, 140MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "KPwZ2wIG8htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = False\n",
        "WANDB_CONNECTED = True\n",
        "WANDB_LOG = True\n",
        "image_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\" # + patches_image_denoising_methods\n",
        "mask_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_med5_checkpoint.pth\")\n",
        "config = {\n",
        "    'batch_size' : 6,\n",
        "    'dropout' : 0.0,\n",
        "    'optimizer' : 'sgd',\n",
        "    'num_epochs' : 15,\n",
        "    'learning_rate' : 0.0186,\n",
        "    'weight_decay' : 0.0189,\n",
        "    'momentum' : 0.0722,\n",
        "    'data_split' : 0.2,\n",
        "    'image_patches_path': image_patches_path,\n",
        "    'mask_patches_path': mask_patches_path,\n",
        "    'image_denoising_methods': 'med5',#'nlm'\n",
        "    'loss_function': 'dice+bce',\n",
        "    'model_type': 'vgg13+imagenet',\n",
        "    'model_path':'./gdrive/MyDrive/lsecs',\n",
        "}\n"
      ],
      "metadata": {
        "id": "-0Al6T1fdX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !wandb login --relogin\n",
        "if LOAD_MODEL:\n",
        "    model = build_model(config['model_type'], config['dropout'], config['loss_function'])\n",
        "    model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activations = 'ReLU', out_activation=None).to(DEVICE)\n",
        "    load_state_dict(model, model_path)\n",
        "else:\n",
        "    # WANDB_LOG = False\n",
        "    train_losses, val_losses, dice_scores = train(config, model_path)"
      ],
      "metadata": {
        "id": "IaxEOPPRbMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0",
        "outputId": "1c3fc928-2336-4ef1-f678-5a8531ff25a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En",
        "outputId": "037d2928-c874-4e8b-b024-c87fbf048621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgG0lEQVR4nO3deVhUZf8G8HtmgJlhVfZFFETFHXEj9ywSN9RWU1OkX5pvWCq9FS5oakq2mGambWqLpllupdJLuGTmFuSWgLsgyubCsMjAzJzfH8DoBCjgMAeY+3NdcyHPnHPmewZlbp/zPM+RCIIggIiIiMiMSMUugIiIiMjUGICIiIjI7DAAERERkdlhACIiIiKzwwBEREREZocBiIiIiMwOAxARERGZHQYgIiIiMjsMQERERGR2GICIGoG3334bEolE7DKonuLfD6KKGICI6pl169ZBIpHoHwqFAp6enggJCcHHH3+MvLw8sUvUu3z5MsLDw+Hn5weFQgF3d3f0798f8+bNE7u0Wjt48CCefPJJuLm5QS6Xw8fHBy+//DJSU1PFLs2Aj4+Pwd+Tqh7r1q0Tu1SieknCe4ER1S/r1q1DeHg4FixYAF9fX5SUlCAjIwP79u1DXFwcmjdvjh07dqBz5876fTQaDTQaDRQKhcnqPH/+PHr06AGlUokXX3wRPj4+uH79OhITE7F7924UFRWZrBZjWbFiBaZNm4aWLVti4sSJ8PDwQFJSEr788ksAwK5du9C7d2+Rqyy1bds25Ofn67/ftWsXvv/+e3z00UdwdnbWt/fu3RvNmzc3+d8PonpPIKJ6Ze3atQIA4dixYxWei4+PF5RKpdCiRQuhsLBQhOrueuWVVwQLCwvh8uXLFZ7LzMw0aS35+fkPfYw//vhDkEqlQr9+/YSCggKD586fPy+4ubkJHh4ews2bNx/6tWqiuuf2/vvvCwCES5cu1W1BRI0EL4ERNSCPPfYYoqOjceXKFXz33Xf69qrGeHz33Xfo2bMnrK2t0bRpU/Tv3x//+9//DLbZvXs3+vXrBxsbG9jZ2WHYsGH4559/HljLhQsX0KxZM7Ro0aLCc66urhXadu/ejQEDBsDOzg729vbo0aMHNmzYYLDN5s2b0a1bNyiVSjg7O+OFF15Aenq6wTYTJ06Era0tLly4gKFDh8LOzg7jxo0DAOh0OixbtgwdOnSAQqGAm5sbXn75Zdy6deuB57Nw4UJIJBJ8/fXXsLa2NnjOz88P7733Hq5fv47PPvsMAPDBBx9AIpHgypUrFY41c+ZMWFlZGbzukSNHMHjwYDg4OMDa2hoDBgzAwYMHDfYr/zmeOXMGY8eORdOmTdG3b98H1v4glf39kEgkmDp1KjZv3oz27dtDqVSiV69eOHXqFADgs88+Q6tWraBQKPDoo4/i8uXLFY5bnXMiqq8YgIgamPHjxwNAhSDzb/Pnz8f48eNhaWmJBQsWYP78+fD29saePXv023z77bcYNmwYbG1tsWTJEkRHR+PMmTPo27dvpR9492rRogXS0tIMjleVdevWYdiwYbh58yZmzpyJd999F126dEFsbKzBNs899xxkMhliYmIwadIkbNmyBX379sXt27cNjqfRaBASEgJXV1d88MEHePrppwEAL7/8Mt544w306dMHy5cvR3h4ONavX4+QkBCUlJRUWV9hYSHi4+PRr18/+Pr6VrrN6NGjIZfL8csvvwAAnnvuOUgkEvzwww8Vtv3hhx8waNAgNG3aFACwZ88e9O/fHyqVCvPmzcPixYtx+/ZtPPbYYzh69GiF/Z999lkUFhZi8eLFmDRp0v3f3Idw4MABvP766wgLC8Pbb7+NpKQkDB8+HCtXrsTHH3+MV155BW+88QYOHTqEF1980WDfmp4TUb0jdhcUERm63yWwcg4ODkJgYKD++3nz5gn3/nM+d+6cIJVKhSeffFLQarUG++p0OkEQBCEvL09o0qSJMGnSJIPnMzIyBAcHhwrt/3b69GlBqVQKAIQuXboI06ZNE7Zt21bh8tHt27cFOzs7ISgoSLhz506ltRQXFwuurq5Cx44dDbb55ZdfBADC3Llz9W1hYWECACEqKsrgWAcOHBAACOvXrzdoj42NrbT9XsePHxcACNOmTbvvOXfu3FlwdHTUf9+rVy+hW7duBtscPXpUACB88803+nNs3bq1EBISoj9fQRCEwsJCwdfXV3jiiSf0beU/xzFjxty3jsrc7xLYv/9+CIIgABDkcrnB9p999pkAQHB3dxdUKpW+febMmQbHrsk5EdVX7AEiaoBsbW3vOxts27Zt0Ol0mDt3LqRSw3/m5ZdC4uLicPv2bYwZMwY5OTn6h0wmQ1BQEPbu3XvfGjp06IDjx4/jhRdewOXLl7F8+XKMGjUKbm5u+OKLL/TbxcXFIS8vD1FRURUG4ZbX8tdffyErKwuvvPKKwTbDhg1D27ZtsXPnzgqv/5///Mfg+82bN8PBwQFPPPGEwfl069YNtra29z2f8vfSzs7uvudsZ2cHlUql/3706NFISEjAhQsX9G2bNm2CXC7HyJEjAQDHjx/HuXPnMHbsWNy4cUNfV0FBAR5//HH8/vvv0Ol0Bq8zZcqU+9ZhLI8//jh8fHz03wcFBQEAnn76aYP3orz94sWLAGp3TkT1jYXYBRBRzeXn51c6zqbchQsXIJVK0b59+yq3OXfuHIDScUWVsbe3f2Adbdq0wbfffgutVoszZ87gl19+wXvvvYfJkyfD19cXwcHB+nDQsWPHKo9TPo7G39+/wnNt27bFH3/8YdBmYWGBZs2aVTif3NzcKt+XrKysKl+//MP+QUsM5OXlGQSDZ599FpGRkdi0aRNmzZoFQRCwefNmDBkyRP/+lb/PYWFhVR43NzdXf7kMQJWX4YytefPmBt87ODgAALy9vSttLx/TVJtzIqpvGICIGpirV68iNzcXrVq1eqjjlP8P/dtvv4W7u3uF5y0sqv/rQSaToVOnTujUqRN69eqFgQMHYv369QgODn6oGqsil8sr9GzpdDq4urpi/fr1le7j4uJS5fFatWoFCwsLnDx5sspt1Go1UlJS0L17d32bp6cn+vXrhx9++AGzZs3C4cOHkZqaiiVLlhjUBQDvv/8+unTpUumxbW1tDb5XKpVV1mFMMpmsRu1C2aoptTknovqGAYiogfn2228BACEhIVVu4+fnB51OhzNnzlT5AeXn5wegdMaWMYNKeUC4fv26weucPn26ytBWPpMsJSWlQo9USkpKpTPN/s3Pzw+//fYb+vTpU+MAYWNjg4EDB2LPnj24cuVKpa/3ww8/QK1WY/jw4Qbto0ePxiuvvIKUlBRs2rQJ1tbWCA0NNagLKO1Rq6tAaGqN8ZzI/HAMEFEDsmfPHixcuBC+vr76qd+VGTVqFKRSKRYsWFBhLEb5/+JDQkJgb2+PxYsXVzpDKjs7+761HDhwoNL9du3aBeDu5axBgwbBzs4OMTExFRZHLK+le/fucHV1xerVq6FWq/XP7969G0lJSRg2bNh9awFKZ2VptVosXLiwwnMajabCTLJ/mzNnDgRBwMSJE3Hnzh2D5y5duoQ333wTHh4eePnllw2ee/rppyGTyfD9999j8+bNGD58OGxsbPTPd+vWDX5+fvjggw8MFi4s96D3uT5qjOdE5oc9QET11O7du5GcnAyNRoPMzEzs2bMHcXFxaNGiBXbs2HHfVX1btWqF2bNnY+HChejXrx+eeuopyOVyHDt2DJ6enoiJiYG9vT1WrVqF8ePHo2vXrnj++efh4uKC1NRU7Ny5E3369MEnn3xS5WssWbIECQkJeOqpp/SrUicmJuKbb76Bo6Mjpk+fDqC0l+Cjjz7CSy+9hB49eujXtzlx4gQKCwvx9ddfw9LSEkuWLEF4eDgGDBiAMWPGIDMzE8uXL4ePjw9mzJjxwPdrwIABePnllxETE4Pjx49j0KBBsLS0xLlz57B582YsX74czzzzTJX79+/fHx988AEiIyPRuXNn/UrQycnJ+OKLL6DT6bBr164K41pcXV0xcOBALF26FHl5eRg9erTB81KpFF9++SWGDBmCDh06IDw8HF5eXkhPT8fevXthb2+Pn3/++YHnV580xnMiMyTqHDQiqqB8Gnz5w8rKSnB3dxeeeOIJYfny5QbTk8tVNs1ZEARhzZo1QmBgoCCXy4WmTZsKAwYMEOLi4gy22bt3rxASEiI4ODgICoVC8PPzEyZOnCj89ddf963z4MGDQkREhNCxY0fBwcFBsLS0FJo3by5MnDhRuHDhQoXtd+zYIfTu3VtQKpWCvb290LNnT+H777832GbTpk36eh0dHYVx48YJV69eNdgmLCxMsLGxqbKuzz//XOjWrZugVCoFOzs7oVOnTsKbb74pXLt27b7nU+73338XRo4cKTg7O+vPadKkSZWueF3uiy++EAAIdnZ2Fab6l/v777+Fp556SnBychLkcrnQokUL4bnnnhPi4+P125T/HLOzs6tV671qMw0+IiLCoO3SpUsCAOH99983aN+7d68AQNi8eXONz4movuK9wIiIiMjscAwQERERmR0GICIiIjI7DEBERERkdhiAiIiIyOwwABEREZHZYQAiIiIis8OFECuh0+lw7do12NnZ6e9WTURERPWbIAjIy8uDp6dnhfsF/hsDUCWuXbtW4W7IRERE1DCkpaWhWbNm992GAagSdnZ2AErfQHt7e5GrISIioupQqVTw9vbWf47fDwNQJcove9nb2zMAERERNTDVGb7CQdBERERkdhiAiIiIyOwwABEREZHZ4Righ6DValFSUiJ2GVQHrKysHjiFkoiIGi4GoFoQBAEZGRm4ffu22KVQHZFKpfD19YWVlZXYpRARUR1gAKqF8vDj6uoKa2trLpbYyJQvhHn9+nU0b96cP18iokaIAaiGtFqtPvw4OTmJXQ7VERcXF1y7dg0ajQaWlpZil0NEREbGQQ41VD7mx9raWuRKqC6VX/rSarUiV0JERHWBAaiWeFmkcePPl4iocWMAIiIiIrPDAESVkkgk2LZtm9hlEBER1QkGIDMyceJESCQSSCQSWFpaws3NDU888QTWrFkDnU5nsO3169cxZMgQkSolIiKqWwxAZmbw4MG4fv06Ll++jN27d2PgwIGYNm0ahg8fDo1Go9/O3d0dcrlcxEqNR6vVVgh4RERiuVOsxc2CYgiCIHYpZo0ByMzI5XK4u7vDy8sLXbt2xaxZs7B9+3bs3r0b69at02/370tgV69exZgxY+Do6AgbGxt0794dR44c0T+/fft2dO3aFQqFAi1btsT8+fMNAtW/7du3Dz179oSNjQ2aNGmCPn364MqVK/rnf/75Z/To0QMKhQLOzs548skn9c/dunULEyZMQNOmTWFtbY0hQ4bg3Llz+ufXrVuHJk2aYMeOHWjfvj3kcjlSU1OhVqvx3//+F15eXrCxsUFQUBD27dv3cG8oEVElSrQ6XMopwJ7kTHx54CLmbDuFcV8eRu+YeLSbG4uuC+PQef7/MHLlQUT+cBwr957H7lPXkZKRh6ISzj41Ba4DZASCIOCOCH9hlZYyo8xWeuyxxxAQEIAtW7bgpZdeqvB8fn4+BgwYAC8vL+zYsQPu7u5ITEzU96ocOHAAEyZMwMcff4x+/frhwoULmDx5MgBg3rx5FY6n0WgwatQoTJo0Cd9//z2Ki4tx9OhR/bns3LkTTz75JGbPno1vvvkGxcXF2LVrl37/iRMn4ty5c9ixYwfs7e3x1ltvYejQoThz5ox+zZ7CwkIsWbIEX375JZycnODq6oqpU6fizJkz2LhxIzw9PbF161YMHjwYp06dQuvWrR/6fSQi8yIIAjJValzMycelnAJcyi4o/ZpTgNSbhdDo7t/Dk1ekwYm02ziRdtugXSoBvJoq4edii5bOtmjpYoOWLjbwc7GFq52cs1SNhAHICO6UaNF+7q8mf90zC0JgbWWcH2Hbtm1x8uTJSp/bsGEDsrOzcezYMTg6OgIAWrVqpX9+/vz5iIqKQlhYGACgZcuWWLhwId58881KA5BKpUJubi6GDx8OPz8/AEC7du30zy9atAjPP/885s+fr28LCAgAAH3wOXjwIHr37g0AWL9+Pby9vbFt2zY8++yzAErXa/r000/1+6WmpmLt2rVITU2Fp6cnAOC///0vYmNjsXbtWixevLgW7xoRmYPbhcW4+K+AczGnAJdzCu77n1+FpRQ+TqXhxdfZBr7OtmVfbWBtJcPlGwW4mF2AC1n5uJhTgIvZ+biYXYA8tQZpN+8g7eYd7EvJNjimrdyiNBA526ClS1k4Kjuu0kpW129Fo8IARABK/ydT1f8qjh8/jsDAQH34+bcTJ07g4MGDWLRokb5Nq9WiqKgIhYWFFRaNdHR0xMSJExESEoInnngCwcHBeO655+Dh4aF/vUmTJlX6WklJSbCwsEBQUJC+zcnJCf7+/khKStK3WVlZoXPnzvrvT506Ba1WizZt2hgcT61Wc0VvIkJhsQaXcwpx+UZZwMkuwKWynp1bhVXf9FomlaC5o7U+2Nz7cLdXQCqturemrbs92rrbG7QJgoDsfHVpMCoLRBezSwNS2s1C5Ks1OHk1Fyev5lY4nlcTpb6nqDwYtXSxgYeDgr1GlWAAMgKlpQxnFoSI8rrGkpSUBF9f38pfR6m87775+fmYP38+nnrqqQrPKRSKSvdZu3YtXnvtNcTGxmLTpk2YM2cO4uLi8Mgjjzzw9apDqVQa/IPPz8+HTCZDQkICZDLD983W1vahX4+I6rcSrQ45+WpkqtTIVBUh7WahvjfnUk4BrucW3Xd/DwcFfJ1t4ONc2vtSHnK8Ha1hKTPecFqJRAJXOwVc7RR4pKXhf87UGi1SbxTiwr3hKKf0a+6dEqTfvoP023dw4FyOwX7WVjL4lvcYOZf2SPk42cBGLoPcQga5hbT0q6UUVjLpfUNbY8IAZAQSicRol6LEsGfPHpw6dQozZsyo9PnOnTvjyy+/xM2bNyvtBeratStSUlIMLotVR2BgIAIDAzFz5kz06tULGzZswCOPPILOnTsjPj4e4eHhFfZp164dNBoNjhw5or8EduPGDaSkpKB9+/b3fS2tVousrCz069evRnUSUf2l0eqQk1+MTFURMlVFyMpTI0tVhEyVGll5d7/eKCjGgyZdNbW2hE9ZsGl5zyUrH2frevE7Xm4hQ2s3O7R2szNoFwQBNwtKL9P9+3LalZuFKCzW4p9rKvxzTVWt17GSSUtDkeXdYHQ3KEkht7znzxYyKAy2u2fbf+2nKN+v7GtTayu4O1T+n2RTEP8nSialVquRkZEBrVaLzMxMxMbGIiYmBsOHD8eECRMq3WfMmDFYvHgxRo0ahZiYGHh4eODvv/+Gp6cnevXqhblz52L48OFo3rw5nnnmGUilUpw4cQKnT5/GO++8U+F4ly5dwueff44RI0bA09MTKSkpOHfunP71582bh8cffxx+fn54/vnnodFosGvXLrz11lto3bo1Ro4ciUmTJuGzzz6DnZ0doqKi4OXlhZEjR1Z53m3atMG4ceMwYcIEfPjhhwgMDER2djbi4+PRuXNnDBs2zDhvMBEZRXmwKQ8xhuGm9M+ZKjVuFKgfGGzKWUglcLWTw9VeAc8mCv3YGV8XG/g62aCpjVXdnlQdkUgkcLKVw8lWjh4+hv9JLdHqkHqzsEIwunrrDoo0WqhLdCjSaA3ew2KtDsVaHfLUdVv3sM4eWDm2a92+yH0wAJmZ2NhYeHh4wMLCAk2bNkVAQAA+/vhjhIWFQSqtvBvXysoK//vf//D6669j6NCh0Gg0aN++PVauXAkACAkJwS+//IIFCxZgyZIlsLS0RNu2bSudUQaU3kg2OTkZX3/9NW7cuAEPDw9ERETg5ZdfBgA8+uij2Lx5MxYuXIh3330X9vb26N+/v37/tWvX6tcuKi4uRv/+/bFr164H3rV97dq1eOedd/D6668jPT0dzs7OeOSRRzB8+PDavJVEDYpWJ+BUei72p2Tjrys3oRMEWEilsJRJYCGVwkImgaVMCgupBBayu+2WMgksDP5cuo2lrGyfsn0tZFJYlu17b3ulx5dKkXunpLTXJq8IWf/qrclUqZGTX/1gI7sn2LjayeFmL4ebnQKu9qVtbnYKuNnL0dTaymwu75SzlEnh52ILP5eqL/ULggCNToBao4O6RFv6VaODWqNFUUnFNnXJPX/W6Mq+L9tWc8+2+v0Mt1NrdCgq0aGJ8v6/s+uaROBKTBWoVCo4ODggNzcX9vaGA9SKiopw6dIl+Pr6Vjm+hRo+/pypMcjKK8KBsznYfzYbB85l33cwb30kk0rgYlsaaO6Gm9Iw41oWcNzsFXA0w2BDlbvf5/e/sQeIiKiRKNHqkHjlFvafzcb+s9kVxnzYyS3Qt7Uz+rRyhp3CAiVaARqtDiW60q8arYASXenXe9tLtAI0Ze2Gf9ZBoyv7WtZe2fOVbWuvtICbveJukCnrpXEtCzdu9go42lhBxmBDdYQBiIioAbt6qxD7z2bj97PZOHj+BvLVhiuwd/JywIA2Lhjg74Iu3k2MOmOJqCFjACIiakCKSrQ4cukm9qdkY//ZLFzILjB43tHGCv1bO2OAvwv6tXaBs23juKcfkbExABER1WOCIOBiTkFZ4MnG4Ys3oNbcvbmvTCpB1+ZNMKCNC/q3cUFHTweOhyGqBgagWuLY8caNP18SU75agz/P5+jH8ly9dcfgeQ8HRellrTYu6N3KGQ4iz6YhaogYgGro3pttGmPFYqqfiouLAaDCqtFEdUEQBCRdzysLPFn46/ItgxtpWsmk6OnrqB/L09rVlrc2IHpIDEA1JJPJ0KRJE2RlZQEoXdOGv4gaF51Oh+zsbFhbW8PCgv9EqG7cLizGgXM5+gHMWf9adc7HyVofeB5p6VQvViImakz4L6oW3N3dAUAfgqjxkUqlaN68OcMtVYsgCCgs1iKvSANVUQnyikqgulP6Z1WRBnlFJaXP3Sn9mnqzECev3sY9nTxQWsrQ288JA/xd0L+1C3ycbcQ7ISIzwABUCxKJBB4eHnB1dUVJScNaWIyqx8rKqsqVsanxKdboSkNLWVhR3bkntJS1l4eX0u1K7gk7GuQVaaDV1XzcmL+bHQb4l47l6e7TFHILXnIlMpV6EYBWrlyJ999/HxkZGQgICMCKFSvQs2fPSrctKSlBTEwMvv76a6Snp8Pf3x9LlizB4MGD9dvExMRgy5YtSE5OhlKpRO/evbFkyRL4+/sbtW6ZTMYxIkQNwJ1iLf65losTV3Nx8uptXLlReE94KUFRie7BB6kGC6kE9kpL2CksYKewgL3C8p6vlrBXWsBOYQknGysEtXSEhwPHERKJRfQAtGnTJkRGRmL16tUICgrCsmXLEBISgpSUFLi6ulbYfs6cOfjuu+/wxRdfoG3btvj111/x5JNP4s8//0RgYCAAYP/+/YiIiECPHj2g0Wgwa9YsDBo0CGfOnIGNDbuViRqzEq0OKRl5OHH1Nk6m5eLE1ds4l5VfrR4aW7kF7BWlIcVOYaEPM/b/+t5OYanfzkFZ/r0lFJZSXjYlaiBEvxdYUFAQevTogU8++QRA6QBUb29vvPrqq4iKiqqwvaenJ2bPno2IiAh929NPPw2lUonvvvuu0tfIzs6Gq6sr9u/fb3BTzarU5F4iRCQenU7AxZx8nEgr7dk5cTUXZ66rUKyp2KPjaidH52ZNENDMAa3d7NDE+m64sVdYwlZhwdsuEDVwDeZeYMXFxUhISMDMmTP1bVKpFMHBwTh06FCl+6jV6go3p1Qqlfjjjz+qfJ3c3FwAgKOjY5XHVKvvzsBQqVSVbkdE4hEEAem37+Dk1Vx9787p9Fzk/evWDwBgr7BAgHcTdG7mUBZ6msDdgTe1JaK7RA1AOTk50Gq1cHNzM2h3c3NDcnJypfuEhIRg6dKl6N+/P/z8/BAfH48tW7ZAq9VWur1Op8P06dPRp08fdOzYsdJtYmJiMH/+/Ic7GSIyqpx8dWmvTlnvzsmrubhRUFxhO4WlFB09y4KOd+lXHycuT0FE9yf6GKCaWr58OSZNmoS2bdtCIpHAz88P4eHhWLNmTaXbR0RE4PTp0/ftIZo5cyYiIyP136tUKnh7exu9diKqXF5RCU6l5+Jk2SDlE2m5SL99p8J2FlIJ/N3t0LlZE3QpCzutXW1hwRt8ElENiRqAnJ2dIZPJkJmZadCemZmpX2vn31xcXLBt2zYUFRXhxo0b8PT0RFRUFFq2bFlh26lTp+KXX37B77//jmbNmlVZh1wuh1zOGwYSmYJao8U/11Q4mXZbfznrYk4BKhuN6Odig4BmZZeyvJugvYc9FJaceUlED0/UAGRlZYVu3bohPj4eo0aNAlB6ySo+Ph5Tp069774KhQJeXl4oKSnBTz/9hOeee07/nCAIePXVV7F161bs27cPvr6+dXkaRFQNRSVafHvoCj7ddx63Ciuun+XVRHnPmB0HdGzmAHsF73FFRHVD9EtgkZGRCAsLQ/fu3dGzZ08sW7YMBQUFCA8PBwBMmDABXl5eiImJAQAcOXIE6enp6NKlC9LT0/H2229Dp9PhzTff1B8zIiICGzZswPbt22FnZ4eMjAwAgIODA+/fRWRiWp2AnxKvYlncWVzLLQIANLW2RBfvJgbjdpxt2QtLRKYjegAaPXo0srOzMXfuXGRkZKBLly6IjY3VD4xOTU01WJG3qKgIc+bMwcWLF2Fra4uhQ4fi22+/RZMmTfTbrFq1CgDw6KOPGrzW2rVrMXHixLo+JSJCaU9s3JlMvP9rCs5l5QMA3O0VmPFEazzdtRnH7RCRqERfB6g+4jpARA/n6KWbWBKbjIQrtwAADkpLvPKoH8J6+3AMDxHVmQazDhARNS7JGSq8F5uCPcmlNwpWWEoR3scXUwb4wUHJ8TxEVH8wABHRQ0u7WYiP4s5i6/F0CAIgk0rwXHdvTA9uDTd7LkBIRPUPAxAR1dqNfDU+2Xse6w+nolhbevuJYZ08EDmoDfxcbEWujoioagxARFRjBWoNvjxwCV8cuIj8sltR9GnlhDdD2iLAu4m4xRERVQMDEBFVW7FGh++PpmLFnnPIyS+9LUVHL3u8Nbgt+rV2Ebk6IqLqYwAiogfS6QTsOHENH8alIO1m6S0qfJys8fogfwzr5AEp76JORA0MAxARVUkQBOw7m433YlOQdF0FAHC2lWNacGs838MbllzLh4gaKAYgIqpUYuotLNmdjCOXbgIA7OQWeHlAS7zY1xfWVvzVQUQNG3+LEZGB81n5eP/XZPz6T+lNiq0spAjr1QKvPNoKTW2sRK6OiMg4GICICABwPfcOlsWdw+aENOgEQCoBnu7aDNOfaAOvJryHHhE1LgxARGbudmExVu27gHV/XoZaU7qWzxPt3fBGiD/auNmJXB0RUd1gACIyU3eKtVj75yWs3ncBqqLStXx6+jjirSH+6NbCUeTqiIjqFgMQkZnRaHX44a+rWB5/FpkqNQCgrbsd3hzsj4H+rpBIOKWdiBo/BiAiM5F2sxA7TlzD5r/ScPlGIQDAq4kSrw9qg5FdvCDjWj5EZEYYgIgasZx8NXaevI7tx9ORmHpb3+5oY4WpA1th3CPNIbeQiVcgEZFIGICIGpl8tQa/ns7A9hPXcPB8DrQ6AQAgkQC9/ZwwMsALQzt7wFbOf/5EZL74G5CoEVBrtNifko3tJ67htzOZ+tlcABDQzAEjunhheGcPuNkrRKySiKj+YAAiaqC0OgFHLt3AjuPXsOvUdf1MLgBo6WyDkV28MKKLJ3ydbUSskoiofmIAImpABEHA6XQVth9Px88nr+lncQGAm70coZ09MbKLFzp62XM2FxHRfTAAETUAl3IKsP14OnYcv4aLOQX6dnuFBYZ28sCILp4I8nXiTC4iompiACKqp7JURfj55HXsOJ6OE1dz9e1yCymC27thZIAnBvi7cBYXEVEtMAAR1SO5d0rKZnCl49CFGyibwAWZVIK+rZwxsosnBnVw5wwuIqKHxN+iRCIrKtFiT3IWth9Px97kbBRr787g6taiKUZ28cTQTh5wtpWLWCURUePCAEQkAo1Wh0MXb2D78Wv49XQG8tR3Z3C1drXFqEAvjAjwhLejtYhVEhE1XgxARCaUlVeEVfsu4OcT15GTf3cGl1cTJUIDPDGyiyfauttxBhcRUR1jACIykUMXbuDV7//WB5+m1pYY1tkDI7t4oVvzppByBhcRkckwABHVMZ1OwOrfL+CDX1OgEwB/Nzu8NcQf/Vq7wFImFbs8IiKzxABEVIduFxbj9R9OID45CwDwVFcvLBrVCUorTl0nIhITAxBRHTl59TZeWZ+Iq7fuwMpCigUjOmB0D2+O7yEiqgcYgIiMTBAEfHckFQt/PoNirQ7NHa3x6biu6OjlIHZpRERUhgGIyIgK1BrM3noK245fAwAMau+G958NgIPSUuTKiIjoXvViBObKlSvh4+MDhUKBoKAgHD16tMptS0pKsGDBAvj5+UGhUCAgIACxsbEPdUwiYziflYeRKw9i2/FrkEklmD20HT4b343hh4ioHhI9AG3atAmRkZGYN28eEhMTERAQgJCQEGRlZVW6/Zw5c/DZZ59hxYoVOHPmDKZMmYInn3wSf//9d62PSfSwth9Px4hPDuJ8Vj5c7eT4ftIjmNS/Jcf7EBHVUxJBEAQxCwgKCkKPHj3wySefAAB0Oh28vb3x6quvIioqqsL2np6emD17NiIiIvRtTz/9NJRKJb777rtaHfPfVCoVHBwckJubC3t7e2OcJjVSao0W7/yShG8PXwEA9PZzwvLnA+Fix9tWEBGZWk0+v0XtASouLkZCQgKCg4P1bVKpFMHBwTh06FCl+6jVaigUCoM2pVKJP/7446GOqVKpDB5ED5J2sxDPrj6kDz+vPtYK3/5fEMMPEVEDIGoAysnJgVarhZubm0G7m5sbMjIyKt0nJCQES5cuxblz56DT6RAXF4ctW7bg+vXrtT5mTEwMHBwc9A9vb28jnB01ZvFJmRi+4g+cvJqLJtaWWBveA68P8oeMqzkTETUIoo8Bqqnly5ejdevWaNu2LaysrDB16lSEh4dDKq39qcycORO5ubn6R1pamhErpsZEo9Xhvdhk/N/XfyH3TgkCvJtg52v9MNDfVezSiIioBkSdBu/s7AyZTIbMzEyD9szMTLi7u1e6j4uLC7Zt24aioiLcuHEDnp6eiIqKQsuWLWt9TLlcDrmcly3o/rLyivDa93/j8MWbAICJvX0wa2g7WFk0uP9HEBGZPVF/c1tZWaFbt26Ij4/Xt+l0OsTHx6NXr1733VehUMDLywsajQY//fQTRo4c+dDHJKrKkYs3MOzjP3D44k3YWMmwYkwg3h7RgeGHiKiBEn0hxMjISISFhaF79+7o2bMnli1bhoKCAoSHhwMAJkyYAC8vL8TExAAAjhw5gvT0dHTp0gXp6el4++23odPp8Oabb1b7mETVpdMJ+PzARbz/awq0OgFt3Gzx6bhuaOVqK3ZpRET0EEQPQKNHj0Z2djbmzp2LjIwMdOnSBbGxsfpBzKmpqQbje4qKijBnzhxcvHgRtra2GDp0KL799ls0adKk2sckqo7cwhK8vvk4fksqu5FpoBfeebIjrK1E/2dDREQPSfR1gOojrgNEp67m4j/rE0pvZCqT4u0RHTCmJ29kSkRUn9Xk85v/lSW6hyAI2HA0FfN3lN7I1NtRiVXjuvFGpkREjQwDEFGZwmINZm89ja1/pwMAgtu54cNnA+BgzXt5ERE1NgxARADOZ+XjlfUJOJuZD5lUgjdD/DGZ9/IiImq0GIDI7O04cQ1RP51EYbEWrnZyrBgTiKCWTmKXRUREdYgBiMyWWqPFop1J+OZQ6b28erV0wvIxXeBqp3jAnkRE1NAxAJFZunqrEBHrE3Hiai4AIGKgH2YEt4GFjAsbEhGZAwYgMjt7k7MwfdNx5N4pgYPSEh+NDsBjbblGFBGROWEAIrNRotVhadxZrNp3AQAQ0MwBK8d1RbOm1iJXRkREpsYARGbhck4Bpm38W3/Ja0KvFpg9rB3kFjKRKyMiIjEwAFGjJggCfkpMx7ztp1FQrIWD0hLvPtUJQzp5iF0aERGJiAGIGq3cOyWYs+00fj5xDQAQ5OuIj0Z3gWcTpciVERGR2BiAqFH66/JNTNt4HOm370AmlSDyiTaYMsAPMikXNiQiIgYgamQ0Wh1W7DmPFXvOQScAzR2tsfz5Lghs3lTs0oiIqB5hAKJGI+1mIaZvOo6EK7cAAE919cL8ER1gp+C9vIiIyBADEDUKO05cw+wtp5Cn1sBOboF3nuyIkV28xC6LiIjqKQYgatDy1RrM2/4Pfkq8CgDo2rwJlj8fCG9Hru1DRERVYwCiBut42m1M2/g3rtwohFQCTH2sNV57rBVvZ0FERA/EAEQNjlYnYPX+C/go7iw0OgFeTZT4aHQX9PR1FLs0IiJqIBiAqEG5nnsHMzYdx+GLNwEAwzp7YPGTneCg5EBnIiKqPgYgajBiT1/HWz+dQu6dElhbyfD2iA54tlszSCRc24eIiGqGAYjqvcJiDRb+koTvj6YCADo3c8Dy5wPh62wjcmVERNRQMQBRvXY6PRevbfwbF7MLIJEAL/f3Q+QTbWBlwYHORERUewxAVC/pdALWHLyEJbHJKNEKcLOXY+lzXdCnlbPYpRERUSPAAET1TlZeEV7/4QQOnMsBAAxq74YlT3dGUxsrkSsjIqLGggGI6pU9yZl4Y/NJ3CgohsJSiujh7TG2Z3MOdCYiIqNiAKJ6oahEi5hdSfj60BUAQDsPe3z8fBe0drMTuTIiImqMGIBIdCkZeXjt+7+RkpkHAHixjy/eHOwPhaVM5MqIiKixYgAi0QiCgG8PX8E7O5NQrNHB2dYKHzwbgEf9XcUujYiIGjkGIBLFjXw13vzxJOKTswAAj/q74P1nAuBiJxe5MiIiMgcMQGRyB85lI/KHE8jOU8NKJsXMoW0xsbcPBzoTEZHJMACRSX0cfw5L484CAFq72uLjMYFo52EvclVERGRuRF9Od+XKlfDx8YFCoUBQUBCOHj163+2XLVsGf39/KJVKeHt7Y8aMGSgqKtI/r9VqER0dDV9fXyiVSvj5+WHhwoUQBKGuT4Ue4K/LN/Xh54VHmmPH1L4MP0REJApRe4A2bdqEyMhIrF69GkFBQVi2bBlCQkKQkpICV9eKA2E3bNiAqKgorFmzBr1798bZs2cxceJESCQSLF26FACwZMkSrFq1Cl9//TU6dOiAv/76C+Hh4XBwcMBrr71m6lOkMhqtDtHb/wEAPNe9Gd4Z1UnkioiIyJyJ2gO0dOlSTJo0CeHh4Wjfvj1Wr14Na2trrFmzptLt//zzT/Tp0wdjx46Fj48PBg0ahDFjxhj0Gv35558YOXIkhg0bBh8fHzzzzDMYNGjQA3uWqG6tP5KKpOsq2Css8NbgtmKXQ0REZk60AFRcXIyEhAQEBwffLUYqRXBwMA4dOlTpPr1790ZCQoI+zFy8eBG7du3C0KFDDbaJj4/H2bOll1pOnDiBP/74A0OGDKnDs6H7yclX44P/pQAA3gjxh5MtZ3oREZG4RLsElpOTA61WCzc3N4N2Nzc3JCcnV7rP2LFjkZOTg759+0IQBGg0GkyZMgWzZs3SbxMVFQWVSoW2bdtCJpNBq9Vi0aJFGDduXJW1qNVqqNVq/fcqleohz47u9e7uZOQVadDB0x5jg1qIXQ4REZH4g6BrYt++fVi8eDE+/fRTJCYmYsuWLdi5cycWLlyo3+aHH37A+vXrsWHDBiQmJuLrr7/GBx98gK+//rrK48bExMDBwUH/8Pb2NsXpmIWEK7fwY8JVAMCCkR0hk3KqOxERiU8iiDQ9qri4GNbW1vjxxx8xatQofXtYWBhu376N7du3V9inX79+eOSRR/D+++/r27777jtMnjwZ+fn5kEql8Pb2RlRUFCIiIvTbvPPOO/juu++q7FmqrAfI29sbubm5sLfnLKXa0uoEjPjkD/xzTYVnuzXD+88GiF0SERE1YiqVCg4ODtX6/BatB8jKygrdunVDfHy8vk2n0yE+Ph69evWqdJ/CwkJIpYYly2Sl94sqz3FVbaPT6aqsRS6Xw97e3uBBD2/DkSv451rZwOchHPhMRET1h6jT4CMjIxEWFobu3bujZ8+eWLZsGQoKChAeHg4AmDBhAry8vBATEwMACA0NxdKlSxEYGIigoCCcP38e0dHRCA0N1Qeh0NBQLFq0CM2bN0eHDh3w999/Y+nSpXjxxRdFO09zdCNfjfd/LR34/N8Qfzhz4DMREdUjogag0aNHIzs7G3PnzkVGRga6dOmC2NhY/cDo1NRUg96cOXPmQCKRYM6cOUhPT4eLi4s+8JRbsWIFoqOj8corryArKwuenp54+eWXMXfuXJOfnzlbEpsMVZEG7T3sMY4Dn4mIqJ4RbQxQfVaTa4hUUWLqLTz16Z8AgJ/+0wvdWjiKXBEREZmDBjEGiBonrU7A3O2nAQDPdGvG8ENERPUSAxAZ1YajqTidroKdwgJRHPhMRET1FAMQGc3NgmJ8UDbw+fUn2nDgMxER1VsMQGQ078UmI/dOCdq62+GFRzjwmYiI6i8GIDKKv1NvYeOxNADAwlEdYSHjXy0iIqq/+ClFD6104PM/AICnunqhhw8HPhMRUf3GAEQPbeOxVJxKz4Wd3AIzh7QTuxwiIqIHYgCih3KzoFi/4nPkoDZwsePAZyIiqv8YgOihvP9rMm4Xlg58Hs+Bz0RE1EAwAFGtnUi7rR/4vGAkBz4TEVHDwU8sqhVd2YrPggA8GeiFnr4c+ExERA0HAxDVyqa/0nDiatnA56Fc8ZmIiBoWBiCqsVsFxVgSmwwAmP5EG7jaKUSuiIiIqGYYgKjG3v9fCm4XlsDfzQ5hvTjwmYiIGh4GIKqRk1dv4/ujqQCABSM7cOAzERE1SPz0omrT6QREb/8HggCM6uKJoJZOYpdERERUKwxAVG0//JWGE2m3YSu3wKyhXPGZiIgaLgYgqpbbhfcMfA5uDVd7DnwmIqKGiwGIquWD/6XgVmEJ2rjZIqy3j9jlEBERPRQGIHqgU1dzsf5I+cDnjrDkwGciImrg+ElG91U68Ll0xecRAZ54hAOfiYioEahVALpw4QLmzJmDMWPGICsrCwCwe/du/PPPP0YtjsT3Y8JVHE+7DRsrGWYP48BnIiJqHGocgPbv349OnTrhyJEj2LJlC/Lz8wEAJ06cwLx584xeIIknt7AE7+oHPreBGwc+ExFRI1HjABQVFYV33nkHcXFxsLKy0rc/9thjOHz4sFGLI3F98L8U3CwoRmtXW0zs4yN2OUREREZT4wB06tQpPPnkkxXaXV1dkZOTY5SiSHyn03Ox/sgVAMD8kR048JmIiBqVGn+qNWnSBNevX6/Q/vfff8PLy8soRZG4dDoBc7efhk4AQgM80dvPWeySiIiIjKrGAej555/HW2+9hYyMDEgkEuh0Ohw8eBD//e9/MWHChLqokUzsx8SrSEwtG/jMFZ+JiKgRqnEAWrx4Mdq2bQtvb2/k5+ejffv26N+/P3r37o05c+bURY1kQrmFJViyu3Tg82uPt4a7Awc+ExFR42NRk40FQUBGRgY+/vhjzJ07F6dOnUJ+fj4CAwPRunXruqqRTGhpXApuFBTDz8UG4X18xS6HiIioTtQ4ALVq1Qr//PMPWrduDW9v77qqi0Twz7VcfHu4dODzgpEdYWXBgc9ERNQ41egTTiqVonXr1rhx40Zd1UMiKR34/A90AjCsswf6tOLAZyIiarxq/F/8d999F2+88QZOnz5dF/WQSLb8nY6EK7dgbSXDHK74TEREjVyNA9CECRNw9OhRBAQEQKlUwtHR0eBRUytXroSPjw8UCgWCgoJw9OjR+26/bNky+Pv7Q6lUwtvbGzNmzEBRUZHBNunp6XjhhRfg5OQEpVKJTp064a+//qpxbeYi904J3t2dBAB49bHW8HBQilwRERFR3arRGCCgNIAYy6ZNmxAZGYnVq1cjKCgIy5YtQ0hICFJSUuDq6lph+w0bNiAqKgpr1qxB7969cfbsWUycOBESiQRLly4FANy6dQt9+vTBwIEDsXv3bri4uODcuXNo2rSp0epubD6KO4uc/GK0dLHB//XlwGciImr8JIIgCGK9eFBQEHr06IFPPvkEAKDT6eDt7Y1XX30VUVFRFbafOnUqkpKSEB8fr297/fXXceTIEfzxxx8ASm/VcfDgQRw4cKDWdalUKjg4OCA3Nxf29va1Pk5DcOaaCsNXHIBOAL77vyD0bc2xP0RE1DDV5PO7VtN8tFotfvrpJ7zzzjt45513sHXrVmi12hodo7i4GAkJCQgODr5bjFSK4OBgHDp0qNJ9evfujYSEBP1lsosXL2LXrl0YOnSofpsdO3age/fuePbZZ+Hq6orAwEB88cUX961FrVZDpVIZPMyBIAiYt6N0xeehndwZfoiIyGzU+BLY+fPnMXToUKSnp8Pf3x8AEBMTA29vb+zcuRN+fn7VOk5OTg60Wi3c3NwM2t3c3JCcnFzpPmPHjkVOTg769u0LQRCg0WgwZcoUzJo1S7/NxYsXsWrVKkRGRmLWrFk4duwYXnvtNVhZWSEsLKzS48bExGD+/PnVqrsx2fp3Oo5dvgWlpQxzhrUXuxwiIiKTqXEP0GuvvQY/Pz+kpaUhMTERiYmJSE1Nha+vL1577bW6qFFv3759WLx4MT799FMkJiZiy5Yt2LlzJxYuXKjfRqfToWvXrli8eDECAwMxefJkTJo0CatXr67yuDNnzkRubq7+kZaWVqfnUR+oikqweFdp0Hz18VbwbMKBz0REZD5q3AO0f/9+HD582GDGl5OTE95991306dOn2sdxdnaGTCZDZmamQXtmZibc3d0r3Sc6Ohrjx4/HSy+9BADo1KkTCgoKMHnyZMyePRtSqRQeHh5o396wN6Ndu3b46aefqqxFLpdDLpdXu/bGoHTgsxotnW3wUt+WYpdDRERkUjXuAZLL5cjLy6vQnp+fDysrq2ofx8rKCt26dTMY0KzT6RAfH49evXpVuk9hYSGkUsOSZTIZgNLxLADQp08fpKSkGGxz9uxZtGjRotq1NXbJGSp8c6h0xee3R3Tgis9ERGR2avzJN3z4cEyePBlHjhyBIAgQBAGHDx/GlClTMGLEiBodKzIyEl988QW+/vprJCUl4T//+Q8KCgoQHh4OoHTNoZkzZ+q3Dw0NxapVq7Bx40ZcunQJcXFxiI6ORmhoqD4IzZgxA4cPH8bixYtx/vx5bNiwAZ9//jkiIiJqeqqNkiAImLvtH2h1AoZ0dEf/Ni5il0RERGRyNb4E9vHHHyMsLAy9evWCpaUlAECj0WDEiBFYvnx5jY41evRoZGdnY+7cucjIyECXLl0QGxurHxidmppq0OMzZ84cSCQSzJkzB+np6XBxcUFoaCgWLVqk36ZHjx7YunUrZs6ciQULFsDX1xfLli3DuHHjanqqjdK24+k4evlm6cDn4Rz4TERE5qnW6wCdP38eSUmlqwe3a9cOrVq1MmphYmqs6wAVlWjR7729yM5T440Qf0QMbDw/MyIiopp8fte4B6hcq1atGlXoMQf/XMtFdp4aTjZWeKkfV3wmIiLzVeMxQE8//TSWLFlSof29997Ds88+a5SiqG4kXS8dvN7RywFyC5nI1RAREYmnxgHo999/N1h5udyQIUPw+++/G6UoqhvJGaUrXLf1sBO5EiIiInHVOABVNd3d0tLSbG4h0VClZJT2ALVzbzzjmoiIiGqjxgGoU6dO2LRpU4X2jRs3VliAkOoPQRCQXBaA/N3ZA0REROatxoOgo6Oj8dRTT+HChQt47LHHAADx8fH4/vvvsXnzZqMXSMZxLbcIeUUaWEgl8HOxFbscIiIiUdU4AIWGhmLbtm1YvHgxfvzxRyiVSnTu3Bm//fYbBgwYUBc1khEkXy+9POnnYsuVn4mIyOzVahr8sGHDMGzYMGPXQnWo/PIXB0ATERHVYgxQWloarl69qv/+6NGjmD59Oj7//HOjFkbGpQ9AHABNRERU8wA0duxY7N27FwCQkZGB4OBgHD16FLNnz8aCBQuMXiAZR/klsLYcAE1ERFTzAHT69Gn07NkTAPDDDz+gU6dO+PPPP7F+/XqsW7fO2PWREag1WlzMKQDAS2BERERALQJQSUkJ5HI5AOC3337T3wG+bdu2uH79unGrI6M4n5UPrU6AvcIC7vYKscshIiISXY0DUIcOHbB69WocOHAAcXFxGDx4MADg2rVrcHJyMnqB9PCSr5cPgLaHRCIRuRoiIiLx1TgALVmyBJ999hkeffRRjBkzBgEBAQCAHTt26C+NUf2Sklm+AjQvfxEREQG1mAb/6KOPIicnByqVCk2bNtW3T548GdbW1kYtjowjqWwAtD9ngBEREQGo5TpAMpnMIPwAgI+PjzHqoTqQwjWAiIiIDHBJ4EbuRr4aWXlqAEAbNwYgIiIigAGo0Svv/WnuaA1bea06/IiIiBodBqBG7u4K0Oz9ISIiKvdQAaioqMhYdVAdSc7gCtBERET/VuMApNPpsHDhQnh5ecHW1hYXL14EAERHR+Orr74yeoH0cO4OgOYMMCIionI1DkDvvPMO1q1bh/feew9WVlb69o4dO+LLL780anH0cLQ6Qb8GkD97gIiIiPRqHIC++eYbfP755xg3bhxkMpm+PSAgAMnJyUYtjh7OlRsFKCrRQW4hhY+TjdjlEBER1Rs1DkDp6elo1apVhXadToeSkhKjFEXGUX75y9/dDjIpb4FBRERUrsYBqH379jhw4ECF9h9//BGBgYFGKYqMI6k8AHH9HyIiIgM1Xhhm7ty5CAsLQ3p6OnQ6HbZs2YKUlBR88803+OWXX+qiRqqllPIZYBwATUREZKDGPUAjR47Ezz//jN9++w02NjaYO3cukpKS8PPPP+OJJ56oixqplrgGEBERUeVqtTRwv379EBcXZ+xayIgK1BpcuVEIgAGIiIjo32rcA3Ts2DEcOXKkQvuRI0fw119/GaUoenhny6a/u9jJ4WQrF7kaIiKi+qXGASgiIgJpaWkV2tPT0xEREWGUoujh8fIXERFR1WocgM6cOYOuXbtWaA8MDMSZM2eMUhQ9vBQGICIioirVOADJ5XJkZmZWaL9+/TosLGp3t/GVK1fCx8cHCoUCQUFBOHr06H23X7ZsGfz9/aFUKuHt7Y0ZM2ZUeV+yd999FxKJBNOnT69VbQ1V0vXSGWD+7pwBRkRE9G81DkCDBg3CzJkzkZubq2+7ffs2Zs2aVatZYJs2bUJkZCTmzZuHxMREBAQEICQkBFlZWZVuv2HDBkRFRWHevHlISkrCV199hU2bNmHWrFkVtj127Bg+++wzdO7cucZ1NWSCIPASGBER0X3UOAB98MEHSEtLQ4sWLTBw4EAMHDgQvr6+yMjIwIcffljjApYuXYpJkyYhPDwc7du3x+rVq2FtbY01a9ZUuv2ff/6JPn36YOzYsfDx8cGgQYMwZsyYCr1G+fn5GDduHL744gs0bdq0xnU1ZJkqNXLvlEAmlaCVq63Y5RAREdU7NQ5AXl5eOHnyJN577z20b98e3bp1w/Lly3Hq1Cl4e3vX6FjFxcVISEhAcHDw3YKkUgQHB+PQoUOV7tO7d28kJCToA8/Fixexa9cuDB061GC7iIgIDBs2zODYVVGr1VCpVAaPhiypbAFEX2cbKCxlD9iaiIjI/NRq0I6NjQ0mT5780C+ek5MDrVYLNzc3g3Y3N7cqb6w6duxY5OTkoG/fvhAEARqNBlOmTDG4BLZx40YkJibi2LFj1aojJiYG8+fPr/2J1DMcAE1ERHR/1QpAO3bswJAhQ2BpaYkdO3bcd9sRI0YYpbCq7Nu3D4sXL8ann36KoKAgnD9/HtOmTcPChQsRHR2NtLQ0TJs2DXFxcVAoFNU65syZMxEZGan/XqVS1bg3qz5JLhsAzQBERERUuWoFoFGjRiEjIwOurq4YNWpUldtJJBJotdpqv7izszNkMlmFWWWZmZlwd3evdJ/o6GiMHz8eL730EgCgU6dOKCgowOTJkzF79mwkJCQgKyvLYKq+VqvF77//jk8++QRqtRoymeFlIblcDrm88SwWeHcANGeAERERVaZaY4B0Oh1cXV31f67qUZPwAwBWVlbo1q0b4uPjDV4rPj4evXr1qnSfwsJCSKWGZZcHGkEQ8Pjjj+PUqVM4fvy4/tG9e3eMGzcOx48frxB+GptijQ4XsvMBAG092ANERERUmdot3GNEkZGRCAsLQ/fu3dGzZ08sW7YMBQUFCA8PBwBMmDABXl5eiImJAQCEhoZi6dKlCAwM1F8Ci46ORmhoKGQyGezs7NCxY0eD17CxsYGTk1OF9sboYk4+SrQC7OQW8GqiFLscIiKieqlGAUin02HdunXYsmULLl++DIlEAl9fXzzzzDMYP348JBJJjQsYPXo0srOzMXfuXGRkZKBLly6IjY3VD4xOTU016PGZM2cOJBIJ5syZg/T0dLi4uCA0NBSLFi2q8Ws3RuUDoP3d7Wr18yAiIjIHEkEQhOpsKAgCQkNDsWvXLgQEBKBt27YQBAFJSUk4deoURowYgW3bttVxuaahUqng4OCA3Nxc2Ns3rHE07+5Oxur9FzAuqDkWPdlJ7HKIiIhMpiaf39XuAVq3bh1+//13xMfHY+DAgQbP7dmzB6NGjcI333yDCRMm1K5qMorksjWA2no0rOBGRERkStVeCPH777/HrFmzKoQfAHjssccQFRWF9evXG7U4qjmuAURERPRg1Q5AJ0+exODBg6t8fsiQIThx4oRRiqLayS0swfXc0pvC+jMAERERVanaAejmzZsVVmy+l5ubG27dumWUoqh2yi9/eTVRwl5hKXI1RERE9Ve1A5BWq4WFRdVDhmQyGTQajVGKotrhHeCJiIiqp9qDoAVBwMSJE6tcMVmtVhutKKqduwOgGYCIiIjup9oBKCws7IHbcAaYuJL1awBxBhgREdH9VDsArV27ti7roIek0wn6GWDteAmMiIjovqo9Bojqt6u37qCwWAsrmRS+zjZil0NERFSvMQA1Ekll439audrCQsYfKxER0f3wk7KR0C+AyAHQRERED8QA1EjoZ4Bx/A8REdEDMQA1EsnXy9cA4gwwIiKiB2EAagTuFGtx+UYBAF4CIyIiqg4GoEbgXFYedALgaGMFF9vKF6okIiKiuxiAGoF7b4EhkUhEroaIiKj+YwBqBMrH//AO8ERERNXDANQIlM8Aa8cB0ERERNXCANTACYJw9xIYB0ATERFVCwNQA5edr8bNgmJIJEBrVwYgIiKi6mAAauDKV4D2dbKB0komcjVEREQNAwNQA8cB0ERERDXHANTAJelvgcEB0ERERNXFANTA8SaoRERENccA1IBptDqcy8oHwJugEhER1QQDUAN2+UYBijU6WFvJ4N3UWuxyiIiIGgwGoAYsqWwAdBs3O0ilvAUGERFRdTEANWD6FaA5/oeIiKhGGIAaMP0AaM4AIyIiqhEGoAYsiWsAERER1QoDUAOlKipB+u07ADgDjIiIqKbqRQBauXIlfHx8oFAoEBQUhKNHj953+2XLlsHf3x9KpRLe3t6YMWMGioqK9M/HxMSgR48esLOzg6urK0aNGoWUlJS6Pg2TOlt2+cvdXoEm1lYiV0NERNSwiB6ANm3ahMjISMybNw+JiYkICAhASEgIsrKyKt1+w4YNiIqKwrx585CUlISvvvoKmzZtwqxZs/Tb7N+/HxERETh8+DDi4uJQUlKCQYMGoaCgwFSnVeeSuAAiERFRrVmIXcDSpUsxadIkhIeHAwBWr16NnTt3Ys2aNYiKiqqw/Z9//ok+ffpg7NixAAAfHx+MGTMGR44c0W8TGxtrsM+6devg6uqKhIQE9O/fvw7PxnRSeAsMIiKiWhO1B6i4uBgJCQkIDg7Wt0mlUgQHB+PQoUOV7tO7d28kJCToL5NdvHgRu3btwtChQ6t8ndzcXACAo6Njpc+r1WqoVCqDR31XfhNUjv8hIiKqOVF7gHJycqDVauHm5mbQ7ubmhuTk5Er3GTt2LHJyctC3b18IggCNRoMpU6YYXAK7l06nw/Tp09GnTx907Nix0m1iYmIwf/78hzsZExIEgfcAIyIiegiijwGqqX379mHx4sX49NNPkZiYiC1btmDnzp1YuHBhpdtHRETg9OnT2LhxY5XHnDlzJnJzc/WPtLS0uirfKNJv30GeWgMLqQQtnW3FLoeIiKjBEbUHyNnZGTKZDJmZmQbtmZmZcHd3r3Sf6OhojB8/Hi+99BIAoFOnTigoKMDkyZMxe/ZsSKV3M93UqVPxyy+/4Pfff0ezZs2qrEMul0MulxvhjEyj/PJXK1dbWFk0uAxLREQkOlE/Pa2srNCtWzfEx8fr23Q6HeLj49GrV69K9yksLDQIOQAgk8kAlF4aKv86depUbN26FXv27IGvr28dnYE4UjI5/oeIiOhhiD4LLDIyEmFhYejevTt69uyJZcuWoaCgQD8rbMKECfDy8kJMTAwAIDQ0FEuXLkVgYCCCgoJw/vx5REdHIzQ0VB+EIiIisGHDBmzfvh12dnbIyMgAADg4OECpVIpzokaUdL10kLY/Z4ARERHViugBaPTo0cjOzsbcuXORkZGBLl26IDY2Vj8wOjU11aDHZ86cOZBIJJgzZw7S09Ph4uKC0NBQLFq0SL/NqlWrAACPPvqowWutXbsWEydOrPNzqmscAE1ERPRwJEL5dSPSU6lUcHBwQG5uLuzt61cvS1GJFh3m/QqtTsChmY/Bw6Hh92gREREZQ00+vzmCtoE5n5UPrU6Ag9IS7vYKscshIiJqkBiAGpjyy1/+7naQSCQiV0NERNQwMQA1MMllt8BoxxlgREREtcYA1MAk6wdA16+xSURERA0JA1ADk3zPJTAiIiKqHQagBuRGvhrZeWoAgL8bAxAREVFtMQA1IOUDoJs7WsNGLvoSTkRERA0WA1ADkpTBW2AQEREZAwNQA5JSNgOMA6CJiIgeDgNQA5LMHiAiIiKjYABqILQ64e49wBiAiIiIHgoDUANx5UYB1BodFJZStHCyEbscIiKiBo0BqIEov/zVxs0OMilvgUFERPQwGIAaCI7/ISIiMh4GoAYi+XrpDDB/d84AIyIielgMQA1EeQ8Qb4JKRET08BiAGoACtQapNwsB8B5gRERExsAA1ACkZJb2/rjYyeFkKxe5GiIiooaPAagB4Po/RERExsUA1ACUD4BmACIiIjIOBqAG4O5NUDkDjIiIyBgYgOo5Qbh7CwwOgCYiIjIOBqB6LkNVhNw7JZBJJWjlait2OURERI0CA1A9V77+T0tnGygsZSJXQ0RE1DgwANVzydd5+YuIiMjYGIDqueSM0hlg7Tw4AJqIiMhYGIDqOf0AaDf2ABERERkLA1A9VqzR4XxWPgCgrQcDEBERkbEwANVjF3PyodEJsJNbwKuJUuxyiIiIGg0GoHrs3gHQEolE5GqIiIgaDwageiypbAA0L38REREZV70IQCtXroSPjw8UCgWCgoJw9OjR+26/bNky+Pv7Q6lUwtvbGzNmzEBRUdFDHbM+ursCNGeAERERGZPoAWjTpk2IjIzEvHnzkJiYiICAAISEhCArK6vS7Tds2ICoqCjMmzcPSUlJ+Oqrr7Bp0ybMmjWr1sesr8ovgbXjGkBERERGJXoAWrp0KSZNmoTw8HC0b98eq1evhrW1NdasWVPp9n/++Sf69OmDsWPHwsfHB4MGDcKYMWMMenhqesz66HZhMTJUpb1abRiAiIiIjErUAFRcXIyEhAQEBwfr26RSKYKDg3Ho0KFK9+nduzcSEhL0gefixYvYtWsXhg4dWutj1kflt8DwaqKEvcJS5GqIiIgaFwsxXzwnJwdarRZubm4G7W5ubkhOTq50n7FjxyInJwd9+/aFIAjQaDSYMmWK/hJYbY6pVquhVqv136tUqoc5LaNIvl6+AjR7f4iIiIxN9EtgNbVv3z4sXrwYn376KRITE7Flyxbs3LkTCxcurPUxY2Ji4ODgoH94e3sbseLaScnkPcCIiIjqiqg9QM7OzpDJZMjMzDRoz8zMhLu7e6X7REdHY/z48XjppZcAAJ06dUJBQQEmT56M2bNn1+qYM2fORGRkpP57lUoleghKKhsA3ZYzwIiIiIxO1B4gKysrdOvWDfHx8fo2nU6H+Ph49OrVq9J9CgsLIZUali2TyQAAgiDU6phyuRz29vYGDzHpdALOlvUA8RIYERGR8YnaAwQAkZGRCAsLQ/fu3dGzZ08sW7YMBQUFCA8PBwBMmDABXl5eiImJAQCEhoZi6dKlCAwMRFBQEM6fP4/o6GiEhobqg9CDjlnfpd0qRGGxFlYWUvg42YhdDhERUaMjegAaPXo0srOzMXfuXGRkZKBLly6IjY3VD2JOTU016PGZM2cOJBIJ5syZg/T0dLi4uCA0NBSLFi2q9jHru/LLX61dbWEha3DDtIiIiOo9iSAIgthF1DcqlQoODg7Izc0V5XLY8t/O4aPfzuKprl5Y+lwXk78+ERFRQ1STz292L9RDyWX3AGvHAdBERER1ggGoHrp7DzAOgCYiIqoLDED1zJ1iLS7dKADAu8ATERHVFQageuZsZh4EAXCysYKLrVzscoiIiBolBqB65t7LXxKJRORqiIiIGicGoHomqWwANFeAJiIiqjsMQPVMeQ9QWw6AJiIiqjMMQPWIIAhIKrsLPAdAExER1R0GoHokO0+NW4UlkEqA1q4MQERERHWFAageSS67/OXjZAOllUzkaoiIiBovBqB6pHwFaF7+IiIiqlsMQPVIeQ+QvxtngBEREdUlBqB6JLnsLvDsASIiIqpbDED1hEarw/msfAC8CSoREVFdYwCqJy7lFKBYq4O1lQzNmirFLoeIiKhRYwCqJ5LuuQWGVMpbYBAREdUlBqB6IkV/CwyO/yEiIqprDED1hH4ANMf/EBER1TkGoHoimfcAIyIiMhkGoHpAVVSC9Nt3ALAHiIiIyBQYgOqB8jvAezgo4GBtKXI1REREjR8DUD2QfM8MMCIiIqp7DED1QPL18hlgvPxFRERkCgxA9UD5JbB2vAUGERGRSTAAiUwQBF4CIyIiMjEGIJFdvXUH+WoNLGUStHS2FbscIiIis8AAJLLyy19+LrawsuCPg4iIyBT4iSuyZN4Cg4iIyOQYgESmXwHagzPAiIiITIUBSGQcAE1ERGR6DEAiKirR4lJOAQCgHdcAIiIiMhkGIBGdz8qHVifAQWkJN3u52OUQERGZjXoRgFauXAkfHx8oFAoEBQXh6NGjVW776KOPQiKRVHgMGzZMv01+fj6mTp2KZs2aQalUon379li9erUpTqVG7r0DvEQiEbkaIiIi8yF6ANq0aRMiIyMxb948JCYmIiAgACEhIcjKyqp0+y1btuD69ev6x+nTpyGTyfDss8/qt4mMjERsbCy+++47JCUlYfr06Zg6dSp27NhhqtOqlpSyGWDtOACaiIjIpEQPQEuXLsWkSZMQHh6u76mxtrbGmjVrKt3e0dER7u7u+kdcXBysra0NAtCff/6JsLAwPProo/Dx8cHkyZMREBBw354lMXAANBERkThEDUDFxcVISEhAcHCwvk0qlSI4OBiHDh2q1jG++uorPP/887CxsdG39e7dGzt27EB6ejoEQcDevXtx9uxZDBo0qNJjqNVqqFQqg4cpJF2/ewmMiIiITEfUAJSTkwOtVgs3NzeDdjc3N2RkZDxw/6NHj+L06dN46aWXDNpXrFiB9u3bo1mzZrCyssLgwYOxcuVK9O/fv9LjxMTEwMHBQf/w9vau/UlVU06+Gjn5agBAGzcGICIiIlMS/RLYw/jqq6/QqVMn9OzZ06B9xYoVOHz4MHbs2IGEhAR8+OGHiIiIwG+//VbpcWbOnInc3Fz9Iy0trc5rL78FRgsna9jILer89YiIiOguUT95nZ2dIZPJkJmZadCemZkJd3f3++5bUFCAjRs3YsGCBQbtd+7cwaxZs7B161b9zLDOnTvj+PHj+OCDDwwut5WTy+WQy007Df3eGWBERERkWqL2AFlZWaFbt26Ij4/Xt+l0OsTHx6NXr1733Xfz5s1Qq9V44YUXDNpLSkpQUlICqdTw1GQyGXQ6nfGKf0jJ10vHGflzAUQiIiKTE/3aS2RkJMLCwtC9e3f07NkTy5YtQ0FBAcLDwwEAEyZMgJeXF2JiYgz2++qrrzBq1Cg4OTkZtNvb22PAgAF44403oFQq0aJFC+zfvx/ffPMNli5darLzepDyHqB27AEiIiIyOdED0OjRo5GdnY25c+ciIyMDXbp0QWxsrH5gdGpqaoXenJSUFPzxxx/43//+V+kxN27ciJkzZ2LcuHG4efMmWrRogUWLFmHKlCl1fj7VodUJOJvJKfBERERikQiCIIhdRH2jUqng4OCA3Nxc2Nsb/xLVhex8PP7hfigspfhn/mDIpFwFmoiI6GHV5PO7Qc8Ca6jKZ4C1cbNj+CEiIhIBA5AIygdAcwYYERGROBiARJCknwLPGWBERERiYAASQQrXACIiIhIVA5CJ5as1SL1ZCIAzwIiIiMTCAGRi5dPfXezkcLI17erTREREVIoByMSSeQd4IiIi0TEAmVhyRukMsHYeHABNREQkFgYgEyu/BYa/G3uAiIiIxMIAZEKCINxdA8iDAYiIiEgsDEAmlKEqgqpIA5lUglautmKXQ0REZLYYgEyofAB0S2cbyC1kIldDRERkvhiATOhmQTHs5BZoywHQREREorIQuwBz8nS3ZniqqxcKi7Vil0JERGTW2ANkYhKJBDZy5k4iIiIxMQARERGR2WEAIiIiIrPDAERERERmhwGIiIiIzA4DEBEREZkdBiAiIiIyOwxAREREZHYYgIiIiMjsMAARERGR2WEAIiIiIrPDAERERERmhwGIiIiIzA4DEBEREZkd3pa8EoIgAABUKpXIlRAREVF1lX9ul3+O3w8DUCXy8vIAAN7e3iJXQkRERDWVl5cHBweH+24jEaoTk8yMTqfDtWvXYGdnB4lEYtRjq1QqeHt7Iy0tDfb29kY9dkNg7ucP8D3g+Zv3+QN8D8z9/IG6ew8EQUBeXh48PT0hld5/lA97gCohlUrRrFmzOn0Ne3t7s/2LD/D8Ab4HPH/zPn+A74G5nz9QN+/Bg3p+ynEQNBEREZkdBiAiIiIyOwxAJiaXyzFv3jzI5XKxSxGFuZ8/wPeA52/e5w/wPTD38wfqx3vAQdBERERkdtgDRERERGaHAYiIiIjMDgMQERERmR0GICIiIjI7DEAmtHLlSvj4+EChUCAoKAhHjx4VuySTiYmJQY8ePWBnZwdXV1eMGjUKKSkpYpclmnfffRcSiQTTp08XuxSTSk9PxwsvvAAnJycolUp06tQJf/31l9hlmYRWq0V0dDR8fX2hVCrh5+eHhQsXVuueRQ3V77//jtDQUHh6ekIikWDbtm0GzwuCgLlz58LDwwNKpRLBwcE4d+6cOMXWgfudf0lJCd566y106tQJNjY28PT0xIQJE3Dt2jXxCjayB/387zVlyhRIJBIsW7bMZPUxAJnIpk2bEBkZiXnz5iExMREBAQEICQlBVlaW2KWZxP79+xEREYHDhw8jLi4OJSUlGDRoEAoKCsQuzeSOHTuGzz77DJ07dxa7FJO6desW+vTpA0tLS+zevRtnzpzBhx9+iKZNm4pdmkksWbIEq1atwieffIKkpCQsWbIE7733HlasWCF2aXWmoKAAAQEBWLlyZaXPv/fee/j444+xevVqHDlyBDY2NggJCUFRUZGJK60b9zv/wsJCJCYmIjo6GomJidiyZQtSUlIwYsQIESqtGw/6+ZfbunUrDh8+DE9PTxNVVkYgk+jZs6cQERGh/16r1Qqenp5CTEyMiFWJJysrSwAg7N+/X+xSTCovL09o3bq1EBcXJwwYMECYNm2a2CWZzFtvvSX07dtX7DJEM2zYMOHFF180aHvqqaeEcePGiVSRaQEQtm7dqv9ep9MJ7u7uwvvvv69vu337tiCXy4Xvv/9ehArr1r/PvzJHjx4VAAhXrlwxTVEmVNX5X716VfDy8hJOnz4ttGjRQvjoo49MVhN7gEyguLgYCQkJCA4O1rdJpVIEBwfj0KFDIlYmntzcXACAo6OjyJWYVkREBIYNG2bwd8Fc7NixA927d8ezzz4LV1dXBAYG4osvvhC7LJPp3bs34uPjcfbsWQDAiRMn8Mcff2DIkCEiVyaOS5cuISMjw+DfgoODA4KCgsz696JEIkGTJk3ELsUkdDodxo8fjzfeeAMdOnQw+evzZqgmkJOTA61WCzc3N4N2Nzc3JCcni1SVeHQ6HaZPn44+ffqgY8eOYpdjMhs3bkRiYiKOHTsmdimiuHjxIlatWoXIyEjMmjULx44dw2uvvQYrKyuEhYWJXV6di4qKgkqlQtu2bSGTyaDVarFo0SKMGzdO7NJEkZGRAQCV/l4sf86cFBUV4a233sKYMWPM5gapS5YsgYWFBV577TVRXp8BiEwuIiICp0+fxh9//CF2KSaTlpaGadOmIS4uDgqFQuxyRKHT6dC9e3csXrwYABAYGIjTp09j9erVZhGAfvjhB6xfvx4bNmxAhw4dcPz4cUyfPh2enp5mcf5UtZKSEjz33HMQBAGrVq0SuxyTSEhIwPLly5GYmAiJRCJKDbwEZgLOzs6QyWTIzMw0aM/MzIS7u7tIVYlj6tSp+OWXX7B37140a9ZM7HJMJiEhAVlZWejatSssLCxgYWGB/fv34+OPP4aFhQW0Wq3YJdY5Dw8PtG/f3qCtXbt2SE1NFaki03rjjTcQFRWF559/Hp06dcL48eMxY8YMxMTEiF2aKMp/95n778Xy8HPlyhXExcWZTe/PgQMHkJWVhebNm+t/J165cgWvv/46fHx8TFIDA5AJWFlZoVu3boiPj9e36XQ6xMfHo1evXiJWZjqCIGDq1KnYunUr9uzZA19fX7FLMqnHH38cp06dwvHjx/WP7t27Y9y4cTh+/DhkMpnYJda5Pn36VFj64OzZs2jRooVIFZlWYWEhpFLDX7kymQw6nU6kisTl6+sLd3d3g9+LKpUKR44cMZvfi+Xh59y5c/jtt9/g5OQkdkkmM378eJw8edLgd6KnpyfeeOMN/PrrryapgZfATCQyMhJhYWHo3r07evbsiWXLlqGgoADh4eFil2YSERER2LBhA7Zv3w47Ozv9NX4HBwcolUqRq6t7dnZ2FcY72djYwMnJyWzGQc2YMQO9e/fG4sWL8dxzz+Ho0aP4/PPP8fnnn4tdmkmEhoZi0aJFaN68OTp06IC///4bS5cuxYsvvih2aXUmPz8f58+f139/6dIlHD9+HI6OjmjevDmmT5+Od955B61bt4avry+io6Ph6emJUaNGiVe0Ed3v/D08PPDMM88gMTERv/zyC7Rarf73oqOjI6ysrMQq22ge9PP/d+CztLSEu7s7/P39TVOgyeabkbBixQqhefPmgpWVldCzZ0/h8OHDYpdkMgAqfaxdu1bs0kRjbtPgBUEQfv75Z6Fjx46CXC4X2rZtK3z++edil2QyKpVKmDZtmtC8eXNBoVAILVu2FGbPni2o1WqxS6sze/furfTffVhYmCAIpVPho6OjBTc3N0EulwuPP/64kJKSIm7RRnS/87906VKVvxf37t0rdulG8aCf/7+Zehq8RBAa8TKkRERERJXgGCAiIiIyOwxAREREZHYYgIiIiMjsMAARERGR2WEAIiIiIrPDAERERERmhwGIiIiIzA4DEBFRNUgkEmzbtk3sMojISBiAiKjemzhxIiQSSYXH4MGDxS6NiBoo3guMiBqEwYMHY+3atQZtcrlcpGqIqKFjDxARNQhyuRzu7u4Gj6ZNmwIovTy1atUqDBkyBEqlEi1btsSPP/5osP+pU6fw2GOPQalUwsnJCZMnT0Z+fr7BNmvWrEGHDh0gl8vh4eGBqVOnGjyfk5ODJ598EtbW1mjdujV27NhRtydNRHWGAYiIGoXo6Gg8/fTTOHHiBMaNG4fnn38eSUlJAICCggKEhISgadOmOHbsGDZv3ozffvvNIOCsWrUKERERmDx5Mk6dOoUdO3agVatWBq8xf/58PPfcczh58iSGDh2KcePG4ebNmyY9TyIyEpPddpWIqJbCwsIEmUwm2NjYGDwWLVokCIIgABCmTJlisE9QUJDwn//8RxAEQfj888+Fpk2bCvn5+frnd+7cKUilUiEjI0MQBEHw9PQUZs+eXWUNAIQ5c+bov8/PzxcACLt37zbaeRKR6XAMEBE1CAMHDsSqVasM2hwdHfV/7tWrl8FzvXr1wvHjxwEASUlJCAgIgI2Njf75Pn36QKfTISUlBRKJBNeuXcPjjz9+3xo6d+6s/7ONjQ3s7e2RlZVV21MiIhExABFRg2BjY1PhkpSxKJXKam1naWlp8L1EIoFOp6uLkoiojnEMEBE1CocPH67wfbt27QAA7dq1w4kTJ1BQUKB//uDBg5BKpfD394ednR18fHwQHx9v0pqJSDzsASKiBkGtViMjI8OgzcLCAs7OzgCAzZs3o3v37ujbty/Wr1+Po0eP4quvvgIAjBs3DvPmzUNYWBjefvttZGdn49VXX8X48ePh5uYGAHj77bcxZcoUuLq6YsiQIcjLy8PBgwfx6quvmvZEicgkGICIqEGIjY2Fh4eHQZu/vz+Sk5MBlM7Q2rhxI1555RV4eHjg+++/R/v27QEA1tbW+PXXXzFt2jT06NED1tbWePrpp7F06VL9scLCwlBUVISPPvoI//3vf+Hs7IxnnnnGdCdIRCYlEQRBELsIIqKHIZFIsHXrVowaNUrsUoiogeAYICIiIjI7DEBERERkdjgGiIgaPF7JJ6KaYg8QERERmR0GICIiIjI7DEBERERkdhiAiIiIyOwwABEREZHZYQAiIiIis8MARERERGaHAYiIiIjMDgMQERERmZ3/B4qKvLb5KKHKAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0c88f9-a6f2-4042-9de2-32a81584d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:27<00:00, 19.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05.tif\"\n",
        "test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "# model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activation=None).to(DEVICE)\n",
        "model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "out_mask_path = inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "merge_original_mask(test_image_path, test_mask_path, output_folder)\n",
        "merge_masks(out_mask_path, test_mask_path, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Inference loop**"
      ],
      "metadata": {
        "id": "ZiGwUlrqBx1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = preprocess_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "# class SigmoidWrapper(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super(SigmoidWrapper, self).__init__()\n",
        "#         self.model = model\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.model(x)\n",
        "#         x = self.sigmoid(x)\n",
        "#         return x\n",
        "\n",
        "# model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_checkpoint.pth\")\n",
        "# device = torch.device('cuda')\n",
        "# model = build_model('vgg13+none', 0.0, 'dice+bce')\n",
        "# if torch.cuda.is_available():\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "# model_with_sigmoid = SigmoidWrapper(model)\n",
        "# model_with_sigmoid.to(device=device)\n",
        "# wrapper = FunctionWrapper(model_with_sigmoid)\n",
        "# wrapper.to(device=device)\n",
        "# full_model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_sigmoid_checkpoint.pth\")\n",
        "# torch.save(wrapper, full_model_path)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XK2mzMWMK02R",
        "outputId": "8e478e45-6ff3-4ae6-8872-3c5f1bfc0628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "input_images_folder = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "output_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "model_path = './gdrive/MyDrive/lsecs/vgg13+imagenet_dice+bce_nlm.pth' #@param {type:\"string\"}\n",
        "\n",
        "input_images_folder = input_images_folder.strip()\n",
        "output_mask_folder = output_mask_folder.strip()\n",
        "model_path = model_path.strip()\n",
        "\n",
        "if not os.path.exists(output_mask_folder):\n",
        "    os.makedirs(output_mask_folder)\n",
        "if not os.path.exists(input_images_folder):\n",
        "    print(f'{input_images_folder} does not exist)')"
      ],
      "metadata": {
        "id": "9JALgAucLM_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13+imagenet_dice+bce_nlm.pth\")\n",
        "# # Inference on full images\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/sem_images\"\n",
        "# # # test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/new_masks\"\n",
        "\n",
        "\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/sem_images\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/zapotoczny_test/edited_masks\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/whole_cells/fen_mask\"\n",
        "\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/new_masks\"\n",
        "# # if not os.path.exists(output_folder):\n",
        "# #     os.makedirs(output_folder)\n",
        "\n",
        "# image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "# mask_names = [f for f in sorted(os.listdir(masks_path)) if os.path.isfile(os.path.join(masks_path, f))]\n",
        "\n",
        "# if len(image_names) != len(mask_names):\n",
        "#     print(f'There are {len(image_names)} images, but {len(mask_names)} masks.')\n",
        "# # print(image_names, mask_names)\n",
        "# # model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "# # if torch.cuda.is_available():\n",
        "# #     model.load_state_dict(torch.load(model_path))\n",
        "# # else:\n",
        "# #     model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "# model = torch.load(model_path)"
      ],
      "metadata": {
        "id": "5YvVBtOmB2aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image_name, mask_name in zip(image_names, mask_names):\n",
        "image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "model = torch.load(model_path)\n",
        "for image_name in image_names:\n",
        "    print(image_name)\n",
        "    # print(f'{image_name} - {mask_name}')\n",
        "    image_path = os.path.join(input_images_folder, image_name)\n",
        "    # mask_path = os.path.join(masks_path, mask_name)\n",
        "    filter = None\n",
        "    output_mask = inference_on_image_with_overlap(model, image_path, filter)\n",
        "\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    out_mask_path = os.path.join(output_mask_folder, filename+\"_new_mask\"+ext)\n",
        "    # Save created mask\n",
        "    cv.imwrite(out_mask_path, output_mask)\n",
        "    # # Merge image with created mask\n",
        "    # merge = merge_images(cv.imread(image_path, cv.IMREAD_GRAYSCALE), output_mask)\n",
        "    # cv.imwrite(os.path.join(output_mask_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "    # # print(out_mask_path)\n",
        "    # # print(image_path, mask_path)\n",
        "    # merge_original_mask(image_path, mask_path, output_mask_folder)\n",
        "    # merge_masks(out_mask_path, mask_path, output_mask_folder)\n",
        "    # # break"
      ],
      "metadata": {
        "id": "OyROv0wvZRUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8fdc544-170a-4d60-d7cf-c773547e4922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IV_Cal10_CB_16.tif\n",
            "IV_Cal10_CB_18.tif\n",
            "I_K_12.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Apply cell mask**"
      ],
      "metadata": {
        "id": "vDkdVH2o-xRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "\n",
        "images = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "if len(images) != len(cells):\n",
        "    print('The number of ground truths and created masks differs.')\n",
        "\n",
        "for image_name, cell_name in zip(images, cells):\n",
        "    print(f'Image: {image_name} - cell: {cell_name}')\n",
        "    im_path = os.path.join(mask_path, image_name)\n",
        "    cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "    image = cv.imread(im_path, cv.IMREAD_GRAYSCALE)\n",
        "    cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "    image[cell == 0] = 0\n",
        "    if rewrite_images:\n",
        "        new_name = image_name\n",
        "        cv.imwrite(os.path.join(mask_path, new_name), image)\n",
        "    else:\n",
        "        new_name = image_name\n",
        "        if not os.path.exists(os.path.join(mask_path, 'single_cell')):\n",
        "            os.makedirs(os.path.join(mask_path, 'single_cell'))\n",
        "        print(os.path.join(mask_path, 'single_cell', new_name))\n",
        "        cv.imwrite(os.path.join(mask_path, 'single_cell', new_name), image)\n",
        "\n"
      ],
      "metadata": {
        "id": "gSHpG-sO-urM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208c72ea-faf4-4a9d-a30a-3d40c669bcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: IV_Cal10_CB_16_new_mask.tif - cell: IV_Cal10_CB_16_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/IV_Cal10_CB_16_new_mask.tif\n",
            "Image: IV_Cal10_CB_18_new_mask.tif - cell: IV_Cal10_CB_18_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/IV_Cal10_CB_18_new_mask.tif\n",
            "Image: I_K_12_new_mask.tif - cell: I_K_12_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/I_K_12_new_mask.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Exclusion of fenestrations based on diameter and roundness**"
      ],
      "metadata": {
        "id": "StzWoL-FyWjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert the pixel size, and min and max fenestration diameters in nanometers:**\n",
        "\n",
        "#@markdown All fenestration with a smaller or larger diameter than the chosen range will be removed from the crated masks.\n",
        "#@markdown (Use dot '.' as the decimal separator, not comma ',').\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "pixel_size_nm = 10.62 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "min_diameter_nm = 105 #@param {type:\"number\"}\n",
        "max_diameter_nm = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_roundness = False # @param {type:\"boolean\"}\n",
        "min_roundness = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "# #@markdown ---\n",
        "# filter_by_fenestration_area = False # @param {type:\"boolean\"}\n",
        "# min_area_nm2 = 105 #@param {type:\"number\"}\n",
        "# max_area_nm2 = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "mask_path = './gdrive/MyDrive/lsecs/mask_edit_test' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "mask_names = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "\n",
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def remove_fenestrations(mask_path, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    contours = find_fenestration_contours(mask_path)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None:\n",
        "            center, axes, angle = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            # print(axes)\n",
        "            roundness = minor_axis_length/major_axis_length\n",
        "            if roundness >= min_roundness:\n",
        "                roundness_of_ellipses.append(roundness)\n",
        "            # rotation_angle = angle\n",
        "            diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "            # print(contour)\n",
        "            # print(diameter)\n",
        "            if filter_by_diameter and (diameter < min_d or diameter > max_d) or filter_by_roundness and (roundness < min_roundness) or np.isnan(diameter):\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "            else:\n",
        "                equivalent_diameters.append(diameter)\n",
        "                fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # print(equivalent_diameters)\n",
        "    if len(equivalent_diameters) > 0:\n",
        "        mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "        std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "        print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    return mask\n",
        "\n",
        "\n",
        "#TODO: ukazat statistiky pro celou slozku obrazku\n",
        "\n",
        "if not rewrite_images:\n",
        "    new_mask_path = os.path.join(mask_path, 'edited_masks')\n",
        "    os.makedirs(new_mask_path, exist_ok=True)\n",
        "else:\n",
        "    new_mask_path = mask_path\n",
        "# print(new_mask_path)\n",
        "for mask_name in mask_names:\n",
        "    # print(mask_name)\n",
        "    mask_path_full = os.path.join(mask_path, mask_name)\n",
        "    # print(mask_path)\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    edited_mask = remove_fenestrations(mask_path_full, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    # print(os.path.join(new_mask_path, mask_name))\n",
        "    cv.imwrite(os.path.join(new_mask_path, mask_name), edited_mask)\n",
        "    # cv.imwrite(os.path.join(new_mask_path, mask_name), mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Display the number of circles and their fitted ellipses\n",
        "# print(\"Number of fenestrations:\", len(contours))\n",
        "# print(\"Number of fitted ellipses:\", len(ellipses))\n"
      ],
      "metadata": {
        "id": "tRq0VTv6yrgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e1749b-72a0-4a92-c1bd-bd1b2427b25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean equavalent diameter: 171 nm, std: 45 nm \n",
            "Mean equavalent diameter: 193 nm, std: 47 nm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Inference evaluation (dice score)**"
      ],
      "metadata": {
        "id": "RxiOKWQb6aFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert folders with cell images and their ground truth masks for comparison:\n",
        "from sklearn.metrics import f1_score\n",
        "images_path = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "ground_truth_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/ground_truth_masks' #@param {type:\"string\"}\n",
        "models_path = './gdrive/MyDrive/lsecs' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "\n",
        "log_file_path = './gdrive/MyDrive/lsecs/dice_score_test/log.txt'\n",
        "\n",
        "ground_truth_mask_folder = ground_truth_mask_folder.strip()\n",
        "images_path = images_path.strip()\n",
        "models_path = models_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "\n",
        "\n",
        "model_names = sorted([f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f)) and 'pt' in f])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "print(model_names)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(images_path):\n",
        "    print(\"Images folder does not exist\")\n",
        "    # exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    print(\"Folder with ground truth masks does not exist\")\n",
        "    # exit()\n",
        "\n",
        "ground_truth_images = sorted([f for f in os.listdir(ground_truth_mask_folder) if os.path.isfile(os.path.join(ground_truth_mask_folder, f))])\n",
        "images = sorted([f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))])\n",
        "\n",
        "if len(ground_truth_images) != len(images):\n",
        "    print('The number of ground truths and images differs.')\n",
        "    # exit()\n",
        "\n",
        "def compute_dice_score(image1, image2):\n",
        "    eps = 1e-8\n",
        "    image1[image1 == 255] = 1\n",
        "    image2[image2 == 255] = 1\n",
        "    intersection_sum = np.logical_and(image1, image2).sum()\n",
        "    dice_score = (2*intersection_sum+eps)/(image1.sum() + image2.sum() + eps)\n",
        "    return dice_score\n",
        "\n",
        "def compute_f1(image1, image2):\n",
        "    image1 = image1.flatten()\n",
        "    image2 = image2.flatten()\n",
        "    return f1_score(image1, image2)\n",
        "\n",
        "with open(log_file_path, \"a+\") as file:\n",
        "    file.write(f'{len(images)} images\\n')\n",
        "    for model_name in model_names:\n",
        "        file.write(f'{model_name}\\n')\n",
        "        print(model_name)\n",
        "        model = torch.load(os.path.join(models_path, model_name)) # TODO:these models do not include sigmoid and preprocessing yet\n",
        "        dice_scores = []\n",
        "        f1_scores = []\n",
        "        if 'nlm' in model_name:\n",
        "            filter_type = 'nlm'\n",
        "        elif 'med5' in model_name:\n",
        "            filter_type = 'med5'\n",
        "        else:\n",
        "            filter_type = None\n",
        "        print(filter_type)\n",
        "        for ground_truth_mask_name, image_name, cell_name  in zip(ground_truth_images, images, cells):\n",
        "            print(f'Compare: {ground_truth_mask_name} - {image_name} - {cell_name}')\n",
        "            file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "            ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "            image_path = os.path.join(images_path, image_name)\n",
        "            cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "            cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "            ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "            # image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "            # output_folder = None\n",
        "            new_mask = inference_on_image_with_overlap(model, image_path, filter_type)\n",
        "            new_mask[cell == 0] = 0\n",
        "            current_dice_score = compute_dice_score(ground_truth_mask, new_mask)\n",
        "            print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "            file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "            dice_scores.append(current_dice_score)\n",
        "\n",
        "            current_f1_score = compute_f1(ground_truth_mask, new_mask)\n",
        "            print(f'Image F1 score: {round(current_f1_score*100, 1)}')\n",
        "            file.write(f'Image F1 score: {round(current_f1_score*100, 1)}\\n')\n",
        "            f1_scores.append(current_f1_score)\n",
        "\n",
        "        dice_scores = np.array(dice_scores)\n",
        "        mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "        std_dice = round(np.std(dice_scores)*100, 1)\n",
        "\n",
        "        f1_scores = np.array(f1_scores)\n",
        "        mean_f1 = round(np.mean(f1_scores)*100, 1)\n",
        "        std_f1 = round(np.std(f1_scores)*100, 1)\n",
        "\n",
        "\n",
        "        print(f'{model_name} Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "        file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "        print(f'{model_name} Mean F1: {mean_f1} +- {std_f1}\\n')\n",
        "        file.write(f'{model_name} Mean F1: {mean_f1} += {std_f1}\\n\\n')\n",
        "\n",
        "# print(f'Mean dice score is {mean_dice}')\n",
        "\n"
      ],
      "metadata": {
        "id": "U1JDFWEQ6hbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91e1b55-477b-44cd-b180-28919cf50c14"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['efficientnet-b7+imagenet_dice+bce_no_denoise.pth', 'efficientnet-b7+imagenet_dice_no_denoise.pth', 'resnet18+ssl_dice+bce_no_denoise.pth', 'resnet18+ssl_dice_no_denoise.pth', 'resnet34+imagenet_dice+bce_no_denoise.pth', 'resnet34+imagenet_dice_no_denoise.pth', 'resnet50+ssl_dice+bce_no_denoise.pth', 'resnet50+ssl_dice_no_denoise.pth', 'vgg11+imagenet_dice+bce_no_denoise.pth', 'vgg11+imagenet_dice_no_denoise.pth', 'vgg11+imagenet_focal_no_denoise.pth', 'vgg13+imagenet_dice+bce_no_denoise.pth', 'vgg13+imagenet_dice_no_denoise.pth', 'vgg13+imagenet_focal_no_denoise.pth', 'vgg16+imagenet_dice+bce_no_denoise.pth', 'vgg16+imagenet_dice_no_denoise.pth', 'vgg16+imagenet_focal_no_denoise.pth', 'vgg19+imagenet_dice+bce_no_denoise.pth', 'vgg19+imagenet_dice_no_denoise.pth', 'vgg19+imagenet_focal_no_denoise.pth']\n",
            "efficientnet-b7+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 87.3\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 89.9\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 87.3\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 80.8\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 81.1\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 84.6\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 86.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 86.9\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 92.4\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 90.2\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.3\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 86.6\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 82.8\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 78.0\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 76.2\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 74.4\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 72.1\n",
            "efficientnet-b7+imagenet_dice+bce_no_denoise.pth Mean dice: 84.7 +- 5.4\n",
            "\n",
            "efficientnet-b7+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 90.8\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 81.4\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 81.1\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 82.5\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 87.1\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 89.0\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 92.9\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.7\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 90.6\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 86.8\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.0\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 82.7\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 79.0\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 77.3\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 74.5\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 70.8\n",
            "efficientnet-b7+imagenet_dice_no_denoise.pth Mean dice: 85.0 +- 5.7\n",
            "\n",
            "resnet18+ssl_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 81.3\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 83.2\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 83.5\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 87.5\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 89.2\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 92.6\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 90.4\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 87.8\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 87.0\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 88.0\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 83.8\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 82.1\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 80.6\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 77.2\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 73.7\n",
            "resnet18+ssl_dice+bce_no_denoise.pth Mean dice: 85.7 +- 4.6\n",
            "\n",
            "resnet18+ssl_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 91.5\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 89.2\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 83.8\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 81.4\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 80.7\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.3\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 89.8\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 92.0\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 88.8\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.4\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 87.3\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.1\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 83.1\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 81.5\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 81.8\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 72.8\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 68.4\n",
            "resnet18+ssl_dice_no_denoise.pth Mean dice: 85.6 +- 6.1\n",
            "\n",
            "resnet34+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 83.8\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 86.5\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 83.9\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 75.0\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 80.2\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 77.0\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 84.9\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 85.6\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 86.4\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 92.0\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 86.9\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 91.3\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 87.0\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 87.1\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 85.1\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 86.8\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 80.1\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 80.1\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 78.7\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 73.0\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 68.3\n",
            "resnet34+imagenet_dice+bce_no_denoise.pth Mean dice: 82.9 +- 5.8\n",
            "\n",
            "resnet34+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 84.0\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 88.7\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 85.5\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 78.8\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 80.0\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 78.0\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 85.3\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 87.2\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 87.0\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.1\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.4\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 92.1\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 88.4\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 85.9\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 81.2\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 80.8\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 79.2\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 71.1\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 67.4\n",
            "resnet34+imagenet_dice_no_denoise.pth Mean dice: 83.7 +- 6.3\n",
            "\n",
            "resnet50+ssl_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 87.9\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 89.9\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 87.9\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 80.7\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 82.6\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 84.1\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.9\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.6\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 89.8\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 92.0\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 88.7\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 89.1\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 87.6\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 83.9\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 82.4\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 81.1\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 76.6\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 73.0\n",
            "resnet50+ssl_dice+bce_no_denoise.pth Mean dice: 86.0 +- 5.0\n",
            "\n",
            "resnet50+ssl_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 88.8\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 91.5\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 88.8\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 83.1\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 82.4\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 82.6\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 89.2\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 90.0\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.6\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 90.6\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 92.7\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.7\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 83.7\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 82.1\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 83.0\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 75.5\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 69.8\n",
            "resnet50+ssl_dice_no_denoise.pth Mean dice: 86.3 +- 5.7\n",
            "\n",
            "vgg11+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 86.4\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 91.6\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 84.7\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 78.5\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 67.2\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.8\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 86.1\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 86.3\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.0\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 93.0\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 89.1\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 90.2\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 75.8\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 75.7\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 78.4\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 61.8\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 56.1\n",
            "vgg11+imagenet_dice+bce_no_denoise.pth Mean dice: 82.7 +- 10.0\n",
            "\n",
            "vgg11+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 86.6\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 92.5\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 89.3\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 85.6\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 81.0\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 71.0\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 88.3\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 94.0\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 90.8\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 93.3\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 90.3\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 90.1\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 90.5\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 79.2\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 79.1\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 81.1\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 67.1\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 58.7\n",
            "vgg11+imagenet_dice_no_denoise.pth Mean dice: 84.5 +- 9.0\n",
            "\n",
            "vgg11+imagenet_focal_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 84.0\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 86.3\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 84.0\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 77.8\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 79.1\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 77.4\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 84.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 84.2\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 85.7\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 89.7\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 86.2\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 88.3\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 84.9\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 85.8\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 83.7\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 85.3\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 78.9\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 77.6\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 76.4\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 69.5\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 66.0\n",
            "vgg11+imagenet_focal_no_denoise.pth Mean dice: 81.7 +- 5.9\n",
            "\n",
            "vgg13+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 86.2\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 90.9\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 88.3\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 83.1\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 81.3\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 75.7\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 87.0\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.6\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 92.9\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 79.2\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 77.6\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 79.6\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 65.7\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 59.4\n",
            "vgg13+imagenet_dice+bce_no_denoise.pth Mean dice: 83.9 +- 8.5\n",
            "\n",
            "vgg13+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 86.5\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 92.5\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 86.1\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 79.0\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 68.4\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 86.1\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 86.9\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.3\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.8\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 93.9\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.3\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 90.4\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 73.7\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 72.9\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 76.5\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 57.2\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 49.1\n",
            "vgg13+imagenet_dice_no_denoise.pth Mean dice: 82.1 +- 11.7\n",
            "\n",
            "vgg13+imagenet_focal_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 83.8\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 86.1\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 83.9\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 79.4\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 78.0\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 74.9\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 84.3\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 83.6\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 84.4\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 85.2\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 85.0\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 85.7\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 83.7\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 85.3\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 76.9\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 74.5\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 73.3\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 66.5\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 61.1\n",
            "vgg13+imagenet_focal_no_denoise.pth Mean dice: 80.6 +- 7.1\n",
            "\n",
            "vgg16+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 91.2\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 88.8\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 83.4\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 78.6\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 65.4\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.2\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 85.2\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 86.6\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.3\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.4\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 93.5\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.1\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 74.9\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 74.5\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 77.7\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 61.2\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 54.4\n",
            "vgg16+imagenet_dice+bce_no_denoise.pth Mean dice: 82.3 +- 10.5\n",
            "\n",
            "vgg16+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 86.9\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 92.4\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 89.6\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 86.7\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 79.0\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 70.1\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 86.9\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 86.7\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.2\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 94.0\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.0\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 90.3\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 75.7\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 75.3\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 80.9\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 61.3\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 56.0\n",
            "vgg16+imagenet_dice_no_denoise.pth Mean dice: 83.3 +- 10.1\n",
            "\n",
            "vgg16+imagenet_focal_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 82.7\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 85.1\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 83.9\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 76.0\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 78.7\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 78.0\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 83.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 83.6\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 85.7\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 89.7\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 86.1\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 84.0\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 85.2\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 83.1\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 84.1\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 79.6\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 78.7\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 76.4\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 70.6\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 68.6\n",
            "vgg16+imagenet_focal_no_denoise.pth Mean dice: 81.5 +- 5.3\n",
            "\n",
            "vgg19+imagenet_dice+bce_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 87.5\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 91.7\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 89.1\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 84.4\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 80.1\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 68.1\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 85.9\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.2\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 93.2\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 88.6\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.2\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 77.4\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 75.9\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 78.7\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 63.1\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 57.0\n",
            "vgg19+imagenet_dice+bce_no_denoise.pth Mean dice: 83.1 +- 9.7\n",
            "\n",
            "vgg19+imagenet_dice_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 87.5\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 92.9\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 90.3\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 85.8\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 80.7\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 71.0\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 88.5\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 87.8\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 87.7\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 93.9\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 90.0\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 94.3\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 90.5\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 89.4\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 88.9\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 90.5\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 78.8\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 77.6\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 80.0\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 64.4\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 58.5\n",
            "vgg19+imagenet_dice_no_denoise.pth Mean dice: 84.2 +- 9.4\n",
            "\n",
            "vgg19+imagenet_focal_no_denoise.pth\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Image Dice score: 80.0\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Image Dice score: 79.3\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Image Dice score: 79.1\n",
            "Compare: IV_Cal10_CB_06.tif - IV_Cal10_CB_06.tif - mask_IV_Cal10_CB_06.tif\n",
            "Image Dice score: 71.6\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Image Dice score: 74.8\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Image Dice score: 72.1\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Image Dice score: 79.7\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n",
            "Image Dice score: 77.5\n",
            "Compare: IV_K_14_mask.tif - IV_K_14.tif - mask_IV_K_14.tif\n",
            "Image Dice score: 79.4\n",
            "Compare: IV_K_16_mask.tif - IV_K_16.tif - mask_IV_K_16.tif\n",
            "Image Dice score: 84.0\n",
            "Compare: IV_K_17_mask.tif - IV_K_17.tif - mask_IV_K_17.tif\n",
            "Image Dice score: 80.5\n",
            "Compare: I_K_04.tif - I_K_04.tif - mask_I_K_04.tif\n",
            "Image Dice score: 82.6\n",
            "Compare: I_K_06.tif - I_K_06.tif - mask_I_K_06.tif\n",
            "Image Dice score: 78.4\n",
            "Compare: I_K_10_mask.tif - I_K_10.tif - mask_I_K_10.tif\n",
            "Image Dice score: 79.3\n",
            "Compare: I_K_11_mask.tif - I_K_11.tif - mask_I_K_11.tif\n",
            "Image Dice score: 78.3\n",
            "Compare: I_K_12_mask.tif - I_K_12.tif - mask_I_K_12.tif\n",
            "Image Dice score: 78.9\n",
            "Compare: V_ML_04.tif - V_ML_04.tif - mask_V_ML_04.tif\n",
            "Image Dice score: 72.6\n",
            "Compare: V_ML_05.tif - V_ML_05.tif - mask_V_ML_05.tif\n",
            "Image Dice score: 70.5\n",
            "Compare: V_ML_12_mask.tif - V_ML_12.tif - mask_V_ML_12.tif\n",
            "Image Dice score: 67.6\n",
            "Compare: V_ML_14_mask.tif - V_ML_14.tif - mask_V_ML_14.tif\n",
            "Image Dice score: 65.0\n",
            "Compare: V_ML_16_mask.tif - V_ML_16.tif - mask_V_ML_16.tif\n",
            "Image Dice score: 60.7\n",
            "vgg19+imagenet_focal_no_denoise.pth Mean dice: 75.8 +- 5.9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "246b35c19f4b4ba8b7c48a4310451269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26eaf515f9444324a8ad0536dbbe74ec",
              "IPY_MODEL_e9fac2f00a784411abb35dde97f2dd19"
            ],
            "layout": "IPY_MODEL_a7c916c702e247d487568f7f6a3126e5"
          }
        },
        "26eaf515f9444324a8ad0536dbbe74ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c039b5b24644486b789cd34474c2858",
            "placeholder": "​",
            "style": "IPY_MODEL_cd2a044f2d7d47e6aaf450d46b1b345c",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "e9fac2f00a784411abb35dde97f2dd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3de76e29818843f6ab994b98871a9579",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e64dac7cd204e7f83bd642537e278e4",
            "value": 1
          }
        },
        "a7c916c702e247d487568f7f6a3126e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c039b5b24644486b789cd34474c2858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd2a044f2d7d47e6aaf450d46b1b345c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3de76e29818843f6ab994b98871a9579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e64dac7cd204e7f83bd642537e278e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3cc78f395474bf5ab4edf4c75fe9d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9dbd579c5cd40b8a018d1230aed35cb",
              "IPY_MODEL_06aac424c6f4445ba136aa7387c65fc4"
            ],
            "layout": "IPY_MODEL_2ba7970bcf5543609be6b28332fd28a8"
          }
        },
        "f9dbd579c5cd40b8a018d1230aed35cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c4f9e3c26c54d2cb7ca8b536b7adf0e",
            "placeholder": "​",
            "style": "IPY_MODEL_a160caeaff9645de81cbcb24d6b8d528",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "06aac424c6f4445ba136aa7387c65fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02900c7fdf1d4bc2baa6f4227c6eae27",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5be96962a9be4a15b4991af4ff306d5d",
            "value": 1
          }
        },
        "2ba7970bcf5543609be6b28332fd28a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c4f9e3c26c54d2cb7ca8b536b7adf0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a160caeaff9645de81cbcb24d6b8d528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02900c7fdf1d4bc2baa6f4227c6eae27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5be96962a9be4a15b4991af4ff306d5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "608148c4eadf47e8a61674f980f45c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ae04d790b814c939c9b0dafb224ff97",
              "IPY_MODEL_fd70c1be9a4b4214bf3aec5d967107f8"
            ],
            "layout": "IPY_MODEL_78ee3bc72b4447cdb347c73a94286153"
          }
        },
        "1ae04d790b814c939c9b0dafb224ff97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_545f2940ffba442e835e2f2ed048b03d",
            "placeholder": "​",
            "style": "IPY_MODEL_a88c10b718044d8187ebe28aff2cc81d",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "fd70c1be9a4b4214bf3aec5d967107f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02bf1dade9e1424d956a7d717e0a6d21",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43879971e8d54a65b0cf54739f4679c6",
            "value": 1
          }
        },
        "78ee3bc72b4447cdb347c73a94286153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "545f2940ffba442e835e2f2ed048b03d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a88c10b718044d8187ebe28aff2cc81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02bf1dade9e1424d956a7d717e0a6d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43879971e8d54a65b0cf54739f4679c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}