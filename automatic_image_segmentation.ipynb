{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDNFLvtqYw7o",
        "outputId": "7a8710ee-750b-4c9c-c1ad-d3ed5ce23268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5QvbqMfiA4o",
        "outputId": "c04e33b6-aff9-4d40-a836-aed37ef9f75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transofrm=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transofrm\n",
        "        self.images = sorted(os.listdir(self.image_dir)) # listdir returns arbitrary order\n",
        "        self.masks = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = np.array(Image.open(img_path).convert('L')) # TODO: only grayscale images\n",
        "        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32) #TODO, ten float asi neni potreba\n",
        "        mask[mask == 255.0] = 1\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_dir, mask_dir, split, batch_size, train_transform, val_transform, num_workers=4, pin_memory=True): # TODO: check these parameters\n",
        "    data = MyDataset(\n",
        "        image_dir=img_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        transofrm=None\n",
        "    )\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        range(len(data)),\n",
        "        test_size=split,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_data = TransformDataset(Subset(data, train_indices), train_transform)\n",
        "    val_data = TransformDataset(Subset(data, test_indices), val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_indices\n",
        "\n",
        "train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "    [\n",
        "        A.Rotate(limit=35, p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        # A.Affine(shear=(0.5,1)),\n",
        "        # A.Affine(scale=(-10, 10)),\n",
        "        A.Normalize(\n",
        "            mean = 0.0,\n",
        "            std = 1.0,\n",
        "            max_pixel_value=255.0, # normalization to [0, 1]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "            mean = 0.0,\n",
        "            std = 1.0,\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# test_transform = A.Compose(\n",
        "#     [\n",
        "#     A.Normalize(\n",
        "#       mean = 0.0,\n",
        "#       std = 1.0,\n",
        "#       max_pixel_value=255.0,\n",
        "#     ),\n",
        "#         ToTensorV2()\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations if needed\n",
        "])\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "    window_size = 512\n",
        "    oh, ow = 124, 124\n",
        "    input_image = cv.imread(image_path, 0)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "    bottom_edge = image_height % (window_size - oh)\n",
        "    right_edge = image_width % (window_size - ow)\n",
        "    mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "    mirrored_image[:image_height, :image_width] = input_image\n",
        "    mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "    mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "    image_height += bottom_edge\n",
        "    image_width += right_edge\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += 1\n",
        "            square_section = preprocess_image(square_section)\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy()\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()\n",
        "    output_probs = output_probs[:original_height, :original_width]\n",
        "    weights = weights[:original_height, :original_width]\n",
        "    output_probs /= weights\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+\".png\"), output_probs)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask\"+\".png\"), output_mask)\n",
        "\n",
        "def inference_on_image(model, image_path, output_folder):\n",
        "    # image = np.array(Image.open(image_path).convert('L'))\n",
        "    # input_size = next(model.parameters()).shape\n",
        "    window_size = 512\n",
        "    input_image = Image.open(image_path).convert('L')\n",
        "    image_width, image_height = input_image.size\n",
        "    output_probs = Image.new(\"L\", (image_width, image_height))\n",
        "    output_mask = Image.new(\"L\", (image_width, image_height))\n",
        "    for y in range(0, image_height, window_size):\n",
        "        for x in range(0, image_width, window_size):\n",
        "    # for y in range(0, 1):\n",
        "        # for x in range(0, 1):\n",
        "            square_section = input_image.crop((x, y, x + window_size, y + window_size))\n",
        "            square_section = preprocess_image(square_section)\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "                mask = (output > 0.5).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output = output.to(torch.uint8)\n",
        "            output_pil = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
        "            output_probs.paste(output_pil, (x, y))\n",
        "\n",
        "            mask_pil = transforms.ToPILImage()(mask.squeeze(0).cpu())\n",
        "            output_mask.paste(mask_pil, (x, y))\n",
        "\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "    output_mask.save(os.path.join(output_folder, filename+\"_mask\"+\".png\"))\n",
        "    output_probs.save(os.path.join(output_folder, filename+\"_probs\"+\".png\"))\n",
        "    # return output_image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "#     image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "#     mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "#     # rejected_path = os.path.join(output_folder,'rejected')\n",
        "#     # print(image_path)\n",
        "\n",
        "#     if not os.path.exists(output_folder):\n",
        "#         os.makedirs(output_folder)\n",
        "\n",
        "#     if os.path.exists(image_patches_path):\n",
        "#         shutil.rmtree(image_patches_path)\n",
        "#     os.mkdir(image_patches_path)\n",
        "#     if os.path.exists(mask_patches_path):\n",
        "#         shutil.rmtree(mask_patches_path)\n",
        "#     os.mkdir(mask_patches_path)\n",
        "#     # if os.path.exists(rejected_path):\n",
        "#     #     shutil.rmtree(rejected_path)\n",
        "#     # os.mkdir(rejected_path)\n",
        "\n",
        "\n",
        "#     image_filenames = sorted(os.listdir(image_folder))\n",
        "#     mask_filenames = sorted(os.listdir(mask_folder))\n",
        "#     # def cut_and_save_paches(image_name, mask_name)\n",
        "\n",
        "#     for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "#         if image_name.endswith(\".tif\"):\n",
        "#             input_path = os.path.join(image_folder, image_name)\n",
        "#             mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "#             img = Image.open(input_path).convert('L')\n",
        "#             mask = Image.open(mask_path).convert('L')\n",
        "#             width, height = img.size\n",
        "\n",
        "#             for y in range(0, height, patch_size):\n",
        "#                 for x in range(0, width, patch_size):\n",
        "#                     # Define the coordinates of the patch\n",
        "#                     left = x\n",
        "#                     upper = y\n",
        "#                     right = min(x + patch_size, width)\n",
        "#                     lower = min(y + patch_size, height)\n",
        "\n",
        "#                     # Crop the patch from the image\n",
        "#                     image_patch = img.crop((left, upper, right, lower))\n",
        "#                     mask_patch = mask.crop((left, upper, right, lower))\n",
        "\n",
        "#                     patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{y // patch_size}_{x // patch_size}.tif\"\n",
        "\n",
        "#                     image_patch.save(os.path.join(image_patches_path, patch_filename))\n",
        "#                     mask_patch.save(os.path.join(mask_patches_path, patch_filename))\n",
        "#     return image_patches_path, mask_patches_path\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = wavelet_denoise(image)\n",
        "    image = apply_clahe(image)\n",
        "    return image\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "    image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "\n",
        "    image_filenames = sorted(os.listdir(image_folder))\n",
        "    mask_filenames = sorted(os.listdir(mask_folder))\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        if image_name.endswith(\".tif\"):\n",
        "            input_path = os.path.join(image_folder, image_name)\n",
        "            mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "            img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "            mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "            height, width = img.shape\n",
        "            # print(width, height)\n",
        "\n",
        "            shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "            strides = (patch_size * width , patch_size , width, 1)\n",
        "            # strides = (patch_size * width , patch_size)\n",
        "\n",
        "            # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "            #              strides=img.strides + img.strides, writeable=False)\n",
        "            img_strided = as_strided(img, shape=shape,\n",
        "                          strides=strides, writeable=False)\n",
        "            mask_strided = as_strided(mask, shape=shape,\n",
        "                          strides=strides, writeable=False)\n",
        "            # print(img_strided.shape)\n",
        "\n",
        "            for i in range(img_strided.shape[0]):\n",
        "                for j in range(img_strided.shape[1]):\n",
        "                    img_patch = img_strided[i, j]\n",
        "                    mask_patch = img_strided[i, j]\n",
        "\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Denoising\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2 * np.sqrt(data + 3/8)\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    data = (data / 2)**2 - 3/8\n",
        "    return data.astype(np.uint8)\n",
        "\n",
        "def wavelet_denoising(data, wavelet='db1', level=1, threshold_type='soft', sigma=None):\n",
        "    coeffs = pywt.wavedec2(data, wavelet, level=level)\n",
        "    # threshold = sigma * np.sqrt(2 * np.log(len(data))) if sigma is not None else None\n",
        "    threshold = 0.5\n",
        "    #print(threshold)\n",
        "    for i in range(len(coeffs) - 1, 0, -1):\n",
        "        threshold_value = threshold * np.nanmean(np.abs(coeffs[i]))\n",
        "        coeffs[i] = tuple(pywt.threshold(c, threshold_value, threshold_type) for c in coeffs[i])\n",
        "\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "def wavelet_denoise(image):\n",
        "    image_anscombe = anscombe_transform(image)\n",
        "    denoised_image = wavelet_denoising(image_anscombe, wavelet = 'db1', level=3, threshold_type='soft', sigma=1)\n",
        "\n",
        "    denoised_image = inverse_anscombe_transform(denoised_image)\n",
        "    return denoised_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def check_accuracy(loader, model, val_losses, dice_scores, device, loss_fn):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1) # label is grayscale\n",
        "            # preds = torch.softmax(model(x), dim=1)\n",
        "            preds = torch.sigmoid(model(x)) # TODO: vystupy modelu bez sigmoidy jsou zaporny hodnoty\n",
        "            loss = loss_fn(preds, y)\n",
        "            running_loss += loss.cpu()\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2*(preds*y).sum()) / (preds+y).sum() + 1e-8 # this is a better predictor\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "    )\n",
        "    dice_score = dice_score/len(loader)\n",
        "    print(f\"Dice score is {dice_score}\")\n",
        "    val_losses.append(running_loss/len(loader))\n",
        "    dice_scores.append(dice_score.cpu())\n",
        "    model.train()\n",
        "    return dice_score\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            # output = torch.softmax(model(x), dim=1)\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def getClassWeights(mask_path, train_indices):\n",
        "    mask_dir_list = sorted(os.listdir(mask_path))\n",
        "    class_count = np.zeros(2, dtype=int)\n",
        "    for i in train_indices:\n",
        "        mask = np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1\n",
        "        class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "        class_count[1] += mask.sum()\n",
        "\n",
        "    n_samples = class_count.sum()\n",
        "    n_classes = 2\n",
        "\n",
        "    class_weights = n_samples / (n_classes * class_count)\n",
        "    return torch.from_numpy(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv2.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        cv2.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "        center, axes, angle = ellipse\n",
        "        center_x, center_y = center\n",
        "        major_axis_length, minor_axis_length = axes\n",
        "        rotation_angle = angle\n",
        "        # print(center_x, center_y)\n",
        "        cv2.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv2.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well\n",
        "            # ellipse = cv2.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv2.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "    return ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv2.findContours(seg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # image = cv2.cvtColor(seg_mask, cv2.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv2.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    min_contour_area = 10\n",
        "    filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n",
        "    return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv2.moments(cnt)\n",
        "        center_x = int(M['m10'] / M['m00'])\n",
        "        center_y = int(M['m01'] / M['m00'])\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(4 * major_axis_length * minor_axis_length)\n",
        "\n",
        "def show_statistics(fenestration_areas, roundness_of_ellipses, equivalent_diameters):\n",
        "    plt.figure(figsize=(17, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black')\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.hist(roundness_of_ellipses, bins=20, color='blue', edgecolor='black')\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black')\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/old11_CA150_NE_01.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv2.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "\n",
        "for ellipse in ellipses:\n",
        "    center, axes, angle = ellipse\n",
        "    # center_x, center_y = center\n",
        "    major_axis_length, minor_axis_length = axes\n",
        "    roundness = minor_axis_length/major_axis_length\n",
        "    roundness_of_ellipses.append(roundness)\n",
        "    # rotation_angle = angle\n",
        "    diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "    equivalent_diameters.append(diameter) # TODO: tohle taky do nm?\n",
        "\n",
        "show_statistics(fenestration_areas, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "BtPrBpQBcsmn",
        "outputId": "c5fc6e18-b4a1-4f6b-bc20-213ef9a6dada"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fenestrations: 0\n",
            "Number of fitted ellipses: 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1700x500 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABYEAAAHcCAYAAACAkGWPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABys0lEQVR4nO3deZiN9f/H8dfsYzb7GLIMY18i42uLZB2lomwJIaQi2UsUUlmKqCwpW6UsJW3IZCkhsrUg+1IYS7INxpj5/P5wzfk55j5j5sxm7p6P6+r6fuc+n/s+n/t9bvOa8z73fR8PY4wRAAAAAAAAAMCWPLN7AgAAAAAAAACAzEMTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYWSY8PFxdu3bN7mnY3htvvKFSpUrJy8tL1apVy+7p2M6aNWvk4eGhNWvWZPdUACBLkeNZ47+S4/fee6/uvffe7J4GgAxETmSNnJATWfGeqWvXrgoPD8+07WcFDw8PjRw5MsO2dztk6+0wB7hGExhumTNnjjw8PLR582bLx++9915Vrlw53c+zdOnSDP2laHcrVqzQkCFDdPfdd2v27Nl6/fXXXY7t2rWrPDw8LP9bvnx5Fs7aPTt37tTIkSN16NChTNn+1KlTNWfOnEzZdkaoWbOmPDw8NG3atOyeCoAciBy/PaUnx/38/FS2bFm9/PLLunLlShbOGoAdkRO3p4x6v+fv75+Fs8653H3PuX37dnXq1EnFihWTn5+f8uXLpyZNmmj27NlKSEjInMlmgpuPoaCgIJUqVUpt2rTR559/rsTExOyeolv+y793vLN7Avjv2L17tzw90/a5w9KlSzVlypT/7D/QtFq1apU8PT01c+ZM+fr63nK8n5+fPvjgg2TLq1atmhnTy1A7d+7UqFGjdO+992bKJ8BTp05VgQIFkp3NcM899+jy5cupqm9m2bt3r3755ReFh4dr3rx5evrpp7NtLgD+O8jxzJeeHD937py+/PJLjR49Wvv379e8efMye7oA4IScyHwZ9X7Py8srM6bncDu8Z8oI7rzn/OCDD/TUU0+pUKFC6ty5s8qUKaMLFy5o5cqV6t69u44fP64XX3xRknT58mV5e9/ebbkbj6HLly/r8OHD+vrrr9WmTRvde++9+vLLLxUSEuIYv2LFiuyaaqr9l3/v3N5HG2zFz88vu6eQZrGxsQoMDMzuaaTayZMnlStXrlSHrbe3tzp16pTJs8p+xhhduXJFuXLlSve2PD09s/2T848//lihoaGaMGGC2rRpo0OHDqXqj5KcdjwDuL2Q45kvvTn+zDPPqG7duvr00081ceJEFSpUKLOmCgDJkBOZL6e837sd3jNlh59//llPPfWU6tSpo6VLlyo4ONjxWL9+/bR582b98ccfjmWpqVF2H6NWx9Crr76qsWPHaujQoerZs6cWLFjgeCynN/7dlZE9h8zE7SCQZW6+R1R8fLxGjRqlMmXKyN/fX/nz51e9evUUHR0t6fqlB1OmTJEkp0sQksTGxmrgwIGOSyzKlSunN998U8YYp+e9fPmy+vbtqwIFCig4OFgPPfSQjh49muz+OyNHjpSHh4d27typxx57THnz5lW9evUkSb/99pu6du2qUqVKyd/fX2FhYXriiSf0zz//OD1X0jb27NmjTp06KXfu3CpYsKBeeuklGWP0119/qWXLlgoJCVFYWJgmTJiQqtpdu3ZNo0ePVkREhPz8/BQeHq4XX3xRcXFxjjEeHh6aPXu2YmNjHbVK7+0MEhMTNWnSJFWqVEn+/v4qVKiQevXqpX///ddpXHh4uB544AH99NNPqlmzpvz9/VWqVCl9+OGHybZ59uxZ9evXz/G6lS5dWuPGjUt2Kcn8+fMVGRmp4OBghYSEqEqVKpo8ebKk65entW3bVpLUsGFDx/4m3XMqaT7fffedatSooVy5cum9996TJM2ePVuNGjVSaGio/Pz8VLFixWS3VAgPD9eOHTv0ww8/OLaddF8jV/e3WrRokSIjI5UrVy4VKFBAnTp10tGjR53GdO3aVUFBQTp69KhatWqloKAgFSxYUIMGDUrTZUGffPKJ2rRpowceeEC5c+fWJ598kmxMSsezdL2RnDTffPny6dFHH9Vff/3ltI21a9eqbdu2Kl68uPz8/FSsWDH1799fly9fdhoXExOjbt26qWjRovLz81PhwoXVsmXLTLtVB4DsQY7f/jnu4eGhevXqyRijAwcOOD02depUVapUSX5+fipSpIh69+6ts2fPOo1xdT/Pm+/vl5SFCxcu1GuvvaaiRYvK399fjRs31r59+5KtP2PGDEVERChXrlyqWbOm1q5dm2xMWre5ceNGNW/eXLlz51ZAQIAaNGigdevWOY25cOGC+vXrp/DwcPn5+Sk0NFRNmzbV1q1bHWP27t2r1q1bKywsTP7+/ipatKgeffRRnTt3zqrEAFJATtz+OeHKjh071KhRI+XKlUtFixbVq6++qlmzZsnDw8Ppb3pX97C9+bW/+T1Tnz59FBQUpEuXLiVbt0OHDgoLC3O8H/ryyy/VokULFSlSRH5+foqIiNDo0aNT9X4pI9+/3uo9p5VRo0bJw8ND8+bNc2oAJ6lRo4ZTndJyjErX38PVrFlTAQEByps3r+65555bnnkbFxenESNGqHTp0o73dEOGDHE6ttzxwgsvqFmzZlq0aJH27NnjWH7z3wxXr17Vyy+/rMjISOXOnVuBgYGqX7++Vq9e7bS9Q4cOycPDQ2+++aamTJmiUqVKKSAgQM2aNdNff/0lY4xGjx6tokWLKleuXGrZsqXOnDmTbF7Lli1T/fr1FRgYqODgYLVo0UI7duxwPH6r3ztpPYaseg7R0dGqV6+e8uTJo6CgIJUrV85x9nd240xgpMu5c+d0+vTpZMvj4+Nvue7IkSM1ZswY9ejRQzVr1tT58+e1efNmbd26VU2bNlWvXr107NgxRUdH66OPPnJa1xijhx56SKtXr1b37t1VrVo1fffddxo8eLCOHj2qt956yzG2a9euWrhwoTp37qzatWvrhx9+UIsWLVzOq23btipTpoxef/11xx8Y0dHROnDggLp166awsDDt2LFDM2bM0I4dO/Tzzz87/dKQpPbt26tChQoaO3asvv32W7366qvKly+f3nvvPTVq1Ejjxo3TvHnzNGjQIP3vf//TPffck2KtevTooblz56pNmzYaOHCgNm7cqDFjxmjXrl364osvJEkfffSRZsyYoU2bNjku16hbt+4tX4ebXz8fHx/lzp1bktSrVy/NmTNH3bp1U9++fXXw4EG9++672rZtm9atWycfHx/Hevv27VObNm3UvXt3denSRbNmzVLXrl0VGRmpSpUqSZIuXbqkBg0a6OjRo+rVq5eKFy+u9evXa+jQoTp+/LgmTZrkqHeHDh3UuHFjjRs3TpK0a9curVu3Ts8995zuuece9e3bV2+//bZefPFFVahQQZIc/ytdvxytQ4cO6tWrl3r27Kly5cpJkqZNm6ZKlSrpoYcekre3t77++ms988wzSkxMVO/evSVJkyZN0rPPPqugoCANGzZMklI8myqpRv/73/80ZswYnThxQpMnT9a6deu0bds25cmTxzE2ISFBUVFRqlWrlt588019//33mjBhgiIiIlJ1W4eNGzdq3759mj17tnx9ffXII49o3rx5LkPF6nh+7bXX9NJLL6ldu3bq0aOHTp06pXfeeUf33HOP03wXLVqkS5cu6emnn1b+/Pm1adMmvfPOO/r777+1aNEix3O0bt1aO3bs0LPPPqvw8HCdPHlS0dHROnLkSI7/sgbA7sjxnJ/jN0t6s543b17HspEjR2rUqFFq0qSJnn76ae3evVvTpk3TL7/8kizP02Ls2LHy9PTUoEGDdO7cOY0fP14dO3bUxo0bHWNmzpypXr16qW7duurXr58OHDighx56SPny5VOxYsXc2uaqVat03333KTIyUiNGjJCnp6fjQ961a9eqZs2akqSnnnpKn332mfr06aOKFSvqn3/+0U8//aRdu3apevXqunr1qqKiohQXF6dnn31WYWFhOnr0qL755hudPXvW8fcQ8F9GTuT8nLB6/Xx9fR2X8MfExKhhw4a6du2aXnjhBQUGBmrGjBkZekZj+/btNWXKFH377beOxqp0/f3h119/ra5duzpuUTFnzhwFBQVpwIABCgoK0qpVq/Tyyy/r/PnzeuONN1J8nox8/5qa95w3unTpklauXKl77rlHxYsXT1e9rI7RUaNGaeTIkapbt65eeeUV+fr6auPGjVq1apWaNWtmuZ3ExEQ99NBD+umnn/Tkk0+qQoUK+v333/XWW29pz549WrJkSbrm2blzZ61YsULR0dEqW7as5Zjz58/rgw8+UIcOHdSzZ09duHBBM2fOVFRUlDZt2pTsyw3nzZunq1ev6tlnn9WZM2c0fvx4tWvXTo0aNdKaNWv0/PPPa9++fXrnnXc0aNAgzZo1y7HuRx99pC5duigqKkrjxo3TpUuXNG3aNNWrV0/btm1TeHh4ir93pLQdQ1Y9hx07duiBBx7QnXfeqVdeeUV+fn7at29fsg+qs40B3DB79mwjKcX/KlWq5LROiRIlTJcuXRw/V61a1bRo0SLF5+ndu7exOkyXLFliJJlXX33VaXmbNm2Mh4eH2bdvnzHGmC1bthhJpl+/fk7junbtaiSZESNGOJaNGDHCSDIdOnRI9nyXLl1KtuzTTz81ksyPP/6YbBtPPvmkY9m1a9dM0aJFjYeHhxk7dqxj+b///mty5crlVBMr27dvN5JMjx49nJYPGjTISDKrVq1yLOvSpYsJDAxMcXs3jrV63Ro0aGCMMWbt2rVGkpk3b57TesuXL0+2vESJEslqcfLkSePn52cGDhzoWDZ69GgTGBho9uzZ47TNF154wXh5eZkjR44YY4x57rnnTEhIiLl27ZrL+S9atMhIMqtXr072WNJ8li9fnuwxq9cyKirKlCpVymlZpUqVHLW40erVq52e9+rVqyY0NNRUrlzZXL582THum2++MZLMyy+/7FiWVPNXXnnFaZt33XWXiYyMdLmvN+rTp48pVqyYSUxMNMYYs2LFCiPJbNu2zWmcq+P50KFDxsvLy7z22mtOy3///Xfj7e3ttNyqVmPGjDEeHh7m8OHDxpjrx7Ek88Ybb6Rq/gBuD+S4PXI8MDDQnDp1ypw6dcrs27fPvPnmm8bDw8NUrlzZkRMnT540vr6+plmzZiYhIcGx/rvvvmskmVmzZjmW3fwaJ2nQoIFTJiZlYYUKFUxcXJxj+eTJk40k8/vvvxtj/j8jq1Wr5jRuxowZTn9zpGWbiYmJpkyZMiYqKsqxj8Zcf41LlixpmjZt6liWO3du07t3b5c13LZtm5FkFi1a5HIM8F9FTtgjJ1y9dlFRUY5x/fr1M5LMxo0bHctOnjxpcufObSSZgwcPOpbfXNMkN7/2N79nSkxMNHfccYdp3bq103oLFy5MVmOr16JXr14mICDAXLlyxWn/SpQo4fg5M96/pvSe82a//vqrkWSee+65W45NktpjdO/evcbT09M8/PDDTllujHHKwpvz+qOPPjKenp5m7dq1TutMnz7dSDLr1q1LcX63Ot6ScrR///4u53Dt2jWnXDfm+r+NQoUKmSeeeMKx7ODBg0aSKViwoDl79qxj+dChQ40kU7VqVRMfH+9Y3qFDB+Pr6+s4Ji5cuGDy5Mljevbs6fRcMTExJnfu3E7LXf3ececYurnn8NZbbxlJ5tSpU8kLdhvgdhBIlylTpig6OjrZf3feeect182TJ4927NihvXv3pvl5ly5dKi8vL/Xt29dp+cCBA2WM0bJlyyRJy5cvl3T9Hnk3evbZZ11u+6mnnkq27MZPQa9cuaLTp0+rdu3akuR0SWGSHj16OP6/l5eXatSoIWOMunfv7lieJ08elStXLtnlmjdbunSpJGnAgAFOywcOHChJ+vbbb1NcPyX+/v7JXrukS5YWLVqk3Llzq2nTpjp9+rTjv8jISAUFBSW7fKNixYqqX7++4+eCBQsm279Fixapfv36yps3r9M2mzRpooSEBP3444+SrtcmNjbWcamYO0qWLKmoqKhky298LZPObGjQoIEOHDjg1qWfmzdv1smTJ/XMM8843dOpRYsWKl++vOXrc/MxVr9+/VseB9L1y8QWLFig9u3bO85GSLq1hasvALr5uRYvXqzExES1a9fO6TUICwtTmTJlnF7XG2sVGxur06dPq27dujLGaNu2bY4xvr6+WrNmTbJLZADc/sjxnJ3jsbGxKliwoAoWLKjSpUtr0KBBuvvuu/Xll186cuL777/X1atX1a9fP6cvbOrZs6dCQkLS9fzdunVzuvdf0t8BSTVJysinnnrKaVzXrl1dnmV7q21u375de/fu1WOPPaZ//vnHkWOxsbFq3LixfvzxR8ctpvLkyaONGzfq2LFjls+VNIfvvvvO8hJlAOSElLNzwur9XnR0tMaOHev0/LVr13ZcRSFdfy/XsWNHt5/3Zh4eHmrbtq2WLl2qixcvOpYvWLBAd9xxh9MtD258LS5cuKDTp0+rfv36unTpkv7880+Xz5EZ71/T4vz585JkeRuItLr5GF2yZIkSExP18ssvJ/vyxZvPUr/RokWLVKFCBZUvX96pJo0aNZKkZDVJq6CgIEnXXydXvLy8HLmemJioM2fO6Nq1a6pRo4blv622bds6/Y1Qq1YtSVKnTp2cvkSvVq1aunr1quMWjNHR0Tp79qw6dOjgtK9eXl6qVatWqvY1rceQVc8h6araL7/8MtktL28H3A4C6VKzZk3VqFEj2fKkJl9KXnnlFbVs2VJly5ZV5cqV1bx5c3Xu3DlVf1AcPnxYRYoUSfYLNunSjMOHDzv+19PTUyVLlnQaV7p0aZfbvnmsJJ05c0ajRo3S/PnzdfLkSafHrBqHN1/+kTt3bvn7+6tAgQLJlt98n6mbJe3DzXMOCwtTnjx5HPvqDi8vLzVp0sTysb179+rcuXMKDQ21fPzmOlhd8pI3b16nxuDevXv122+/qWDBgilu85lnntHChQt133336Y477lCzZs3Url07NW/ePFX7JVm/jpK0bt06jRgxQhs2bEj2hu/cuXNpvvQzqf5Jt5u4Ufny5fXTTz85LfP390+2/zfXyZUVK1bo1KlTqlmzptP9ERs2bKhPP/1U48aNS/ZHwc112Lt3r4wxKlOmjOVz3Hh5y5EjR/Tyyy/rq6++Sja/pOPez89P48aN08CBA1WoUCHVrl1bDzzwgB5//HGFhYXdcp8AZC9yPGfnuL+/v77++mtJ0t9//63x48c7vjToxueXkueUr6+vSpUqla7nv7lOSbegSMqMpG3fnDk+Pj4qVaqUW9tMaiZ16dLF5bzOnTunvHnzavz48erSpYuKFSumyMhI3X///Xr88ccdz12yZEkNGDBAEydO1Lx581S/fn099NBDjvt8AiAnpJydEym937vx+ZMabTeyen+THu3bt9ekSZP01Vdf6bHHHtPFixe1dOlS9erVy6mRuWPHDg0fPlyrVq1yNFaTpHTSTma8f02LpNtrpNQQTa2bj9H9+/fL09NTFStWTNN29u7dq127dt3y/be7khr6t2p8z507VxMmTNCff/7pdCsZq3+LVv+2JCW7hVTS8pv/PkhqcN8s6fVJSVqPIav5t2/fXh988IF69OihF154QY0bN9YjjzyiNm3aJHuvnh1oAiPb3HPPPdq/f7++/PJLrVixQh988IHeeustTZ8+3emT1axmde+jdu3aaf369Ro8eLCqVaumoKAgJSYmqnnz5paf7iTdz+hWyyQl+2IDV1L6hC8zJCYmpniG6c1Bkpr9S0xMVNOmTTVkyBDLsUn3EQoNDdX27dv13XffadmyZVq2bJlmz56txx9/XHPnzk3V/K1ex/3796tx48YqX768Jk6cqGLFisnX11dLly7VW2+9lSWf1LmqU2okvRbt2rWzfPyHH35Qw4YNnZbdXIfExER5eHho2bJllnNJ+jQ3ISFBTZs21ZkzZ/T888+rfPnyCgwM1NGjR9W1a1enWvXr108PPviglixZou+++04vvfSSxowZo1WrVumuu+5ye38B3N7I8euyM8dvfnMfFRWl8uXLq1evXvrqq6/SvD1Xc0xISMiUmli51TaTXq833ngj2X0EkyRlWbt27VS/fn198cUXWrFihd544w2NGzdOixcv1n333SdJmjBhgrp27eo4jvv27asxY8bo559/VtGiRd3eDwDkRJLb9f1eRkrNl7bVrl1b4eHhWrhwoR577DF9/fXXunz5stq3b+8Yc/bsWTVo0EAhISF65ZVXFBERIX9/f23dulXPP/98iu/XMuP9a1qULl1a3t7e+v33391a/0YZdT/mxMREValSRRMnTrR83Ore/Gnxxx9/SEr5Q5ePP/5YXbt2VatWrTR48GCFhobKy8tLY8aM0f79+5ONd/W6pPbvg48++sjyZKQbzyJ2Ja3HkNXrlCtXLv34449avXq1vv32Wy1fvlwLFixQo0aNtGLFinT1AzICTWBkq3z58qlbt27q1q2bLl68qHvuuUcjR450/FHgKghLlCih77//XhcuXHD61Cnp8pASJUo4/jcxMVEHDx50OgvF6lumXfn333+1cuVKjRo1Si+//LJjuTuXNbkjaR/27t3rdBP6EydO6OzZs459zWgRERH6/vvvdffdd2dYCEVEROjixYu3/DRaun6G0oMPPqgHH3xQiYmJeuaZZ/Tee+/ppZdeUunSpd36I+nrr79WXFycvvrqK6dPGK0uDUnt9pPqv3v37mSfOu7evTvDXp/Y2Fh9+eWXat++vdq0aZPs8b59+2revHnJmsA3i4iIkDFGJUuWdHnzfkn6/ffftWfPHs2dO1ePP/64Y7mrW3RERERo4MCBGjhwoPbu3atq1appwoQJ+vjjj1O5hwByInL81rIyxwsXLqz+/ftr1KhR+vnnn1W7dm2nnLrx7NurV6/q4MGDTpmcN29enT17Ntl2Dx8+7PLM3ZQkPffevXudMjI+Pl4HDx5U1apV07zNiIgISdfP6EnN3xOFCxfWM888o2eeeUYnT55U9erV9dprrzmawJJUpUoVValSRcOHD9f69et19913a/r06Xr11VfTPD8AzsiJW8uu93s3Pr/Vvu7evTvZMqucuHr1qo4fP56q52rXrp0mT56s8+fPa8GCBQoPD3fcdkOS1qxZo3/++UeLFy92+jK9gwcP3nLbmfH+NS3vOQMCAtSoUSOtWrVKf/31V7obrDeKiIhQYmKidu7c6fIDUFfr/frrr2rcuHGmfMjw0UcfycPDQ02bNnU55rPPPlOpUqW0ePFipzmMGDEiQ+eS9PdBaGjoLf8+cFWLjDqGPD091bhxYzVu3FgTJ07U66+/rmHDhmn16tWp+tslM2X/ucj4z7r5spigoCCVLl1acXFxjmWBgYGSlCxo7r//fiUkJOjdd991Wv7WW2/Jw8PD8Yd90v1Zpk6d6jTunXfeSfU8kz6pufkTwUmTJqV6G+lx//33Wz5f0qd5KX3zbXq0a9dOCQkJGj16dLLHrl27ZvkmMTXb3LBhg7777rtkj509e1bXrl2TlPzY8PT0dFw2lnR8uDo2UmL1Wp47d06zZ89ONjYwMDBV265Ro4ZCQ0M1ffp0p2N32bJl2rVrV4a9Pl988YViY2PVu3dvtWnTJtl/DzzwgD7//HOnOVh55JFH5OXlpVGjRiU7po0xjtpb1coYo8mTJzutc+nSJV25csVpWUREhIKDg285FwA5GzmeOlmd488++6wCAgIc93ts0qSJfH199fbbbzvVYObMmTp37pzT80dEROjnn3/W1atXHcu++eYb/fXXX27NpUaNGipYsKCmT5/utM05c+a49XeEJEVGRioiIkJvvvmm030lk5w6dUrS9bPSbr5sODQ0VEWKFHEco+fPn3f87ZGkSpUq8vT0JMOADEBOpE52vd+78fl//vlnbdq0ybHs1KlTlmdDRkREOL7HJcmMGTNSdSawdP1S+bi4OM2dO1fLly9PdoWj1Wtx9erVZK+vlcx4/5rW95wjRoyQMUadO3e2zKgtW7ak+srWG7Vq1Uqenp565ZVXkp0NndKZy+3atdPRo0f1/vvvJ3vs8uXLio2NTfNckowdO1YrVqxQ+/btXd5qULJ+TTdu3KgNGza4/dxWoqKiFBISotdff93plhNJkv4+kFy/rhlxDJ05cybZsqTG/e3wtwVnAiPbVKxYUffee68iIyOVL18+bd68WZ999pn69OnjGBMZGSnp+lmOUVFR8vLy0qOPPqoHH3xQDRs21LBhw3To0CFVrVpVK1as0Jdffql+/fo5PgWKjIxU69atNWnSJP3zzz+qXbu2fvjhB+3Zs0dS6j7ZCwkJ0T333KPx48crPj5ed9xxh1asWJGqTyMzQtWqVdWlSxfNmDHDcXnMpk2bNHfuXLVq1eqWZ366q0GDBurVq5fGjBmj7du3q1mzZvLx8dHevXu1aNEiTZ482fKM1JQMHjxYX331lR544AF17dpVkZGRio2N1e+//67PPvtMhw4dUoECBdSjRw+dOXNGjRo1UtGiRXX48GG98847qlatmuPT8WrVqsnLy0vjxo3TuXPn5Ofn5/iSNFeaNWvmOMO4V69eunjxot5//32FhoYm+/Q6MjJS06ZN06uvvqrSpUsrNDTU8v5CPj4+GjdunLp166YGDRqoQ4cOOnHihCZPnqzw8HD1798/TTVyZd68ecqfP7/q1q1r+fhDDz2k999/X99++60eeeQRl9uJiIjQq6++qqFDh+rQoUNq1aqVgoODdfDgQX3xxRd68sknNWjQIJUvX14REREaNGiQjh49qpCQEH3++efJ7pG1Z88eNW7cWO3atVPFihXl7e2tL774QidOnNCjjz6aIfsO4PZEjqdOVud4/vz51a1bN02dOlW7du1ShQoVNHToUI0aNUrNmzfXQw89pN27d2vq1Kn63//+p06dOjnW7dGjhz777DM1b95c7dq10/79+/Xxxx87Xo+08vHx0auvvqpevXqpUaNGat++vQ4ePKjZs2e7dWaxdP2D4Q8++ED33XefKlWqpG7duumOO+7Q0aNHtXr1aoWEhOjrr7/WhQsXVLRoUbVp00ZVq1ZVUFCQvv/+e/3yyy+OL8FdtWqV+vTpo7Zt26ps2bK6du2aPvroI3l5eal169ZuzQ/A/yMnUiczc+LatWsur8x7+OGHFRgYqCFDhuijjz5S8+bN9dxzzykwMFAzZsxQiRIl9Ntvvzmt06NHDz311FNq3bq1mjZtql9//VXfffddsnshu1K9enWVLl1aw4YNU1xcnNOtICSpbt26yps3r7p06aK+ffvKw8NDH330Uapu0ZAZ71/T+p6zbt26mjJlip555hmVL19enTt3VpkyZXThwgWtWbNGX331lVtXmSTVbPTo0apfv74eeeQR+fn56ZdfflGRIkU0ZswYy/U6d+6shQsX6qmnntLq1at19913KyEhQX/++acWLlyo7777zvKe3ze68Ri6cuWKDh8+rK+++kq//fabGjZsqBkzZqS4/gMPPKDFixfr4YcfVosWLXTw4EFNnz5dFStWtGyUuyskJETTpk1T586dVb16dT366KMqWLCgjhw5om+//VZ3332340MlV793MuIYeuWVV/Tjjz+qRYsWKlGihE6ePKmpU6eqaNGiTl+AmG0M4IbZs2cbSeaXX36xfLxBgwamUqVKTstKlChhunTp4vj51VdfNTVr1jR58uQxuXLlMuXLlzevvfaauXr1qmPMtWvXzLPPPmsKFixoPDw8zI2H7IULF0z//v1NkSJFjI+PjylTpox54403TGJiotPzxsbGmt69e5t8+fKZoKAg06pVK7N7924jyYwdO9YxbsSIEUaSOXXqVLL9+fvvv83DDz9s8uTJY3Lnzm3atm1rjh07ZiSZESNG3HIbXbp0MYGBgamqk5X4+HgzatQoU7JkSePj42OKFStmhg4daq5cuZKq57GS2rEzZswwkZGRJleuXCY4ONhUqVLFDBkyxBw7dswxpkSJEqZFixbJ1m3QoIFp0KCB07ILFy6YoUOHmtKlSxtfX19ToEABU7duXfPmm286XvvPPvvMNGvWzISGhhpfX19TvHhx06tXL3P8+HGnbb3//vumVKlSxsvLy0gyq1evTnE+xhjz1VdfmTvvvNP4+/ub8PBwM27cODNr1iwjyRw8eNAxLiYmxrRo0cIEBwcbSY79WL16tdNzJVmwYIG56667jJ+fn8mXL5/p2LGj+fvvv53GuKp50nHjyokTJ4y3t7fp3LmzyzGXLl0yAQEB5uGHH3baptXxbIwxn3/+ualXr54JDAw0gYGBpnz58qZ3795m9+7djjE7d+40TZo0MUFBQaZAgQKmZ8+e5tdffzWSzOzZs40xxpw+fdr07t3blC9f3gQGBprcuXObWrVqmYULF7qcK4DsR47bO8f3799vvLy8nF6vd99915QvX974+PiYQoUKmaefftr8+++/ydadMGGCueOOO4yfn5+5++67zebNm5PleVIWLlq0yGndgwcPOmVEkqlTp5qSJUsaPz8/U6NGDfPjjz+me5vbtm0zjzzyiMmfP7/x8/MzJUqUMO3atTMrV640xhgTFxdnBg8ebKpWrWqCg4NNYGCgqVq1qpk6dapjGwcOHDBPPPGEiYiIMP7+/iZfvnymYcOG5vvvv7esK/BfQk7YIyckufzvxvc+v/32m2nQoIHx9/c3d9xxhxk9erSZOXNmsnEJCQnm+eefNwUKFDABAQEmKirK7Nu3L9lr7+o9kzHGDBs2zEgypUuXtpz3unXrTO3atU2uXLlMkSJFzJAhQ8x3332XbHtdunQxJUqUSLZ+Rr9/dfWeMyVbtmwxjz32mOPYzZs3r2ncuLGZO3euSUhIcIxL7fGVZNasWY73nHnz5jUNGjQw0dHRKc7/6tWrZty4caZSpUqO9SIjI82oUaPMuXPnUtyPm4+hgIAAEx4eblq3bm0+++wzp31xNYfExETz+uuvmxIlShg/Pz9z1113mW+++SbZ65eU92+88YbT9lz9feDqd9Tq1atNVFSUyZ07t/H39zcRERGma9euZvPmzY4xKf3eMSZ9x9DKlStNy5YtTZEiRYyvr68pUqSI6dChg9mzZ4/rQmchD2PS8c0NQA61fft23XXXXfr444/VsWPH7J4OAABIA3IcAJASciL95syZo27duungwYMKDw/P7ukAyADcExi2d/ny5WTLJk2aJE9PT6ebzQMAgNsPOQ4ASAk5AQCpwz2BYXvjx4/Xli1b1LBhQ3l7e2vZsmVatmyZnnzyyQz9xk4AAJDxyHEAQErICQBIHZrAsL26desqOjpao0eP1sWLF1W8eHGNHDlSw4YNy+6pAQCAWyDHAQApIScAIHW4JzAAAAAAAAAA2Bj3BAYAAAAAAAAAG6MJDAAAAAAAAAA2xj2BM0BiYqKOHTum4OBgeXh4ZPd0AADZwBijCxcuqEiRIvL05DPW1CJDAQBkqHvIUABAWjKUJnAGOHbsGN86CgCQJP31118qWrRodk8jxyBDAQBJyNC0IUMBAElSk6E0gTNAcHCwpOsFDwkJcWsb8fHxWrFihZo1ayYfH5+MnF6OR22sURfXqI016mIto+py/vx5FStWzJEJSB0yNHNRG2vUxTVqY426WCNDsxcZmrmojTXq4hq1sUZdrGVHhtIEzgBJl96EhISkK3wDAgIUEhLCP4qbUBtr1MU1amONuljL6LpwOWbakKGZi9pYoy6uURtr1MUaGZq9yNDMRW2sURfXqI016mItOzKUGy4BAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANhYjmsCT5kyReHh4fL391etWrW0adOmFMcvWrRI5cuXl7+/v6pUqaKlS5e6HPvUU0/Jw8NDkyZNyuBZAwCQ/chQAADcQ4YCAHK6HNUEXrBggQYMGKARI0Zo69atqlq1qqKionTy5EnL8evXr1eHDh3UvXt3bdu2Ta1atVKrVq30xx9/JBv7xRdf6Oeff1aRIkUyezcAAMhyZCgAAO4hQwEAdpCjmsATJ05Uz5491a1bN1WsWFHTp09XQECAZs2aZTl+8uTJat68uQYPHqwKFSpo9OjRql69ut59912ncUePHtWzzz6refPmycfHJyt2BQCALEWGAgDgHjIUAGAHOaYJfPXqVW3ZskVNmjRxLPP09FSTJk20YcMGy3U2bNjgNF6SoqKinMYnJiaqc+fOGjx4sCpVqpQ5kwcAIBuRoQAAuIcMBQDYhXd2TyC1Tp8+rYSEBBUqVMhpeaFChfTnn39arhMTE2M5PiYmxvHzuHHj5O3trb59+6Z6LnFxcYqLi3P8fP78eUlSfHy84uPjU72dGyWt5+76dkZtrFEX16iNNepiLaPqcjvXlQz976I21qiLa9TGGnWxRoaSoXZGbaxRF9eojTXqYi07MjTHNIEzw5YtWzR58mRt3bpVHh4eqV5vzJgxGjVqVLLlK1asUEBAQLrmFB0dna717YzaWKMurlEba9TFWnrrcunSpQyaSc5AhuYs1MYadXGN2lijLtbI0LQhQ3MWamONurhGbaxRF2tZmaE5pglcoEABeXl56cSJE07LT5w4obCwMMt1wsLCUhy/du1anTx5UsWLF3c8npCQoIEDB2rSpEk6dOiQ5XaHDh2qAQMGOH4+f/68ihUrpmbNmikkJMSd3VN8fLyio6PVtGlT7gd1E2pjjbq4Rm2sURdrGVWXpLNxbkdk6H8XtbFGXVyjNtaoizUylAy1M2pjjbq4Rm2sURdr2ZGhOaYJ7Ovrq8jISK1cuVKtWrWSdP0+SitXrlSfPn0s16lTp45Wrlypfv36OZZFR0erTp06kqTOnTtb3qupc+fO6tatm8u5+Pn5yc/PL9lyHx+fdB/QGbENu6I21qiLa9TGGnWxlt663M41JUNBbaxRF9eojTXqYo0MdUaG2gu1sUZdXKM21qiLtazM0BzTBJakAQMGqEuXLqpRo4Zq1qypSZMmKTY21hGUjz/+uO644w6NGTNGkvTcc8+pQYMGmjBhglq0aKH58+dr8+bNmjFjhiQpf/78yp8/v9Nz+Pj4KCwsTOXKlcvanQMAIBORoQAAuIcMBQDYQY5qArdv316nTp3Syy+/rJiYGFWrVk3Lly933HT/yJEj8vT0dIyvW7euPvnkEw0fPlwvvviiypQpoyVLlqhy5crZtQsAAGQLMhQAAPeQoQAAO8hRTWBJ6tOnj8vLbtasWZNsWdu2bdW2bdtUb9/V/ZcAAMjpyFAAANxDhgIAcjrPWw8BAAAAAAAAAORUNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN5bgm8JQpUxQeHi5/f3/VqlVLmzZtSnH8okWLVL58efn7+6tKlSpaunSp47H4+Hg9//zzqlKligIDA1WkSBE9/vjjOnbsWGbvBgAAWY4MBQDAPWQoACCny1FN4AULFmjAgAEaMWKEtm7dqqpVqyoqKkonT560HL9+/Xp16NBB3bt317Zt29SqVSu1atVKf/zxhyTp0qVL2rp1q1566SVt3bpVixcv1u7du/XQQw9l5W4BAJDpyFAAANxDhgIA7CBHNYEnTpyonj17qlu3bqpYsaKmT5+ugIAAzZo1y3L85MmT1bx5cw0ePFgVKlTQ6NGjVb16db377ruSpNy5cys6Olrt2rVTuXLlVLt2bb377rvasmWLjhw5kpW7BgBApiJDAQBwDxkKALCDHNMEvnr1qrZs2aImTZo4lnl6eqpJkybasGGD5TobNmxwGi9JUVFRLsdL0rlz5+Th4aE8efJkyLwBAMhuZCgAAO4hQwEAduGd3RNIrdOnTyshIUGFChVyWl6oUCH9+eefluvExMRYjo+JibEcf+XKFT3//PPq0KGDQkJCXM4lLi5OcXFxjp/Pnz8v6fq9neLj41O1PzdLWs/d9e2M2lijLq5RG2vUxVpG1eV2risZ+t9FbaxRF9eojTXqYo0MJUPtjNpYoy6uURtr1MVadmRojmkCZ7b4+Hi1a9dOxhhNmzYtxbFjxozRqFGjki1fsWKFAgIC0jWP6OjodK1vZ9TGGnVxjdpYoy7W0luXS5cuZdBMch4y9PZHbaxRF9eojTXqYo0MdR8ZevujNtaoi2vUxhp1sZaVGZpjmsAFChSQl5eXTpw44bT8xIkTCgsLs1wnLCwsVeOTgvfw4cNatWpVip++StLQoUM1YMAAx8/nz59XsWLF1KxZs1uu60p8fLyio6PVtGlT+fj4uLUNu6I21qiLa9TGGnWxllF1STob53ZEhv53URtr1MU1amONulgjQ8lQO6M21qiLa9TGGnWxlh0ZmmOawL6+voqMjNTKlSvVqlUrSVJiYqJWrlypPn36WK5Tp04drVy5Uv369XMsi46OVp06dRw/JwXv3r17tXr1auXPn/+Wc/Hz85Ofn1+y5T4+Puk+oDNiG3ZFbaxRF9eojTXqYi29dbmda0qGgtpYoy6uURtr1MUaGeqMDLUXamONurhGbaxRF2tZmaE5pgksSQMGDFCXLl1Uo0YN1axZU5MmTVJsbKy6desmSXr88cd1xx13aMyYMZKk5557Tg0aNNCECRPUokULzZ8/X5s3b9aMGTMkXQ/eNm3aaOvWrfrmm2+UkJDguE9Tvnz55Ovrmz07CgBABiNDAQBwDxkKALCDHNUEbt++vU6dOqWXX35ZMTExqlatmpYvX+646f6RI0fk6enpGF+3bl198sknGj58uF588UWVKVNGS5YsUeXKlSVJR48e1VdffSVJqlatmtNzrV69Wvfee2+W7BcAAJmNDAUAwD1kKADADnJUE1iS+vTp4/KymzVr1iRb1rZtW7Vt29ZyfHh4uIwxGTk9AABuW2QoAADuIUMBADmd562HAAAAAAAAAAByKprAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbMytJvCBAwcyeh4AAPwnkKEAALiHDAUAwH1uNYFLly6thg0b6uOPP9aVK1cyek4AANgWGQoAgHvIUAAA3OdWE3jr1q268847NWDAAIWFhalXr17atGlTRs8NAADbIUMBAHAPGQoAgPvcagJXq1ZNkydP1rFjxzRr1iwdP35c9erVU+XKlTVx4kSdOnUqo+cJAIAtkKEAALiHDAUAwH3p+mI4b29vPfLII1q0aJHGjRunffv2adCgQSpWrJgef/xxHT9+PKPmCQCArZChAAC4hwwFACDt0tUE3rx5s5555hkVLlxYEydO1KBBg7R//35FR0fr2LFjatmyZUbNEwAAWyFDAQBwDxkKAEDaebuz0sSJEzV79mzt3r1b999/vz788EPdf//98vS83lMuWbKk5syZo/Dw8IycKwAAOR4ZCgCAe8hQAADc51YTeNq0aXriiSfUtWtXFS5c2HJMaGioZs6cma7JAQBgN2QoAADuIUMBAHCfW03gvXv33nKMr6+vunTp4s7mAQCwLTIUAAD3kKEAALjPrXsCz549W4sWLUq2fNGiRZo7d266JwUAgF2RoQAAuIcMBQDAfW41gceMGaMCBQokWx4aGqrXX3893ZMCAMCuyFAAANxDhgIA4D63msBHjhxRyZIlky0vUaKEjhw5ku5JAQBgV2QoAADuIUMBAHCfW03g0NBQ/fbbb8mW//rrr8qfP3+6JwUAgF2RoQAAuIcMBQDAfW41gTt06KC+fftq9erVSkhIUEJCglatWqXnnntOjz76aEbPEQAA2yBDAQBwDxkKAID7vN1ZafTo0Tp06JAaN24sb+/rm0hMTNTjjz/OvZgAAEgBGQoAgHvIUAAA3OdWE9jX11cLFizQ6NGj9euvvypXrlyqUqWKSpQokdHzAwDAVshQAADcQ4YCAOA+t5rAScqWLauyZctm1FwAAPjPIEMBAHAPGQoAQNq51QROSEjQnDlztHLlSp08eVKJiYlOj69atSpDJgcAgN2QoQAAuIcMBQDAfW41gZ977jnNmTNHLVq0UOXKleXh4ZHR8wIAwJbIUAAA3EOGAgDgPreawPPnz9fChQt1//33Z/R8AACwNTIUAAD3kKEAALjP052VfH19Vbp06YyeCwAAtkeGAgDgHjIUAAD3udUEHjhwoCZPnixjTEbPBwAAWyNDAQBwDxkKAID73LodxE8//aTVq1dr2bJlqlSpknx8fJweX7x4cYZMDgAAuyFDAQBwDxkKAID73GoC58mTRw8//HBGzwUAANsjQwEAcA8ZCgCA+9xqAs+ePTuj5wEAwH8CGQoAgHvIUAAA3OfWPYEl6dq1a/r+++/13nvv6cKFC5KkY8eO6eLFixk2OQAA7IgMBQDAPWQoAADucetM4MOHD6t58+Y6cuSI4uLi1LRpUwUHB2vcuHGKi4vT9OnTM3qeAADYAhkKAIB7yFAAANzn1pnAzz33nGrUqKF///1XuXLlcix/+OGHtXLlygybHAAAdkOGAgDgHjIUAAD3udUEXrt2rYYPHy5fX1+n5eHh4Tp69GiGTMyVKVOmKDw8XP7+/qpVq5Y2bdqU4vhFixapfPny8vf3V5UqVbR06VKnx40xevnll1W4cGHlypVLTZo00d69ezNzFwAA/2FkKAAA7iFDAQBwn1tN4MTERCUkJCRb/vfffys4ODjdk3JlwYIFGjBggEaMGKGtW7eqatWqioqK0smTJy3Hr1+/Xh06dFD37t21bds2tWrVSq1atdIff/zhGDN+/Hi9/fbbmj59ujZu3KjAwEBFRUXpypUrmbYfAID/LjIUAAD3kKEAALjPrSZws2bNNGnSJMfPHh4eunjxokaMGKH7778/o+aWzMSJE9WzZ09169ZNFStW1PTp0xUQEKBZs2ZZjp88ebKaN2+uwYMHq0KFCho9erSqV6+ud999V9L1T18nTZqk4cOHq2XLlrrzzjv14Ycf6tixY1qyZEmm7QcA4L+LDAUAwD1kKAAA7nOrCTxhwgStW7dOFStW1JUrV/TYY485LsEZN25cRs9RknT16lVt2bJFTZo0cSzz9PRUkyZNtGHDBst1NmzY4DRekqKiohzjDx48qJiYGKcxuXPnVq1atVxuEwCA9CBDAQBwDxkKAID7vN1ZqWjRovr11181f/58/fbbb7p48aK6d++ujh07Ot2gPyOdPn1aCQkJKlSokNPyQoUK6c8//7RcJyYmxnJ8TEyM4/GkZa7GWImLi1NcXJzj5/Pnz0uS4uPjFR8fn8o9cpa03rZt2+Tp6VZv3sHT01OJiYnZvo2M2k7S+umtze20T7dTXTJqPrdTfTlmrHHMWEta193f30lSuz4ZSoZm5Xb4fWiN34euccxY45ixRoaSoTey07Et8fvQFX4fusYxY41jxlpWZ6jkZhNYkry9vdWpUyd3V8/RxowZo1GjRiVbvmLFCgUEBKRr28ePH0/X+nZGbaxRF9eojTXqYi06Ojpd61+6dCnVY8lQMjSrURtr1MU1amONulgjQ7MGGZo9qI016uIatbFGXaxlaYa68wQffvhhio8//vjj7mw2RQUKFJCXl5dOnDjhtPzEiRMKCwuzXCcsLCzF8Un/e+LECRUuXNhpTLVq1VzOZejQoRowYIDj5/Pnz6tYsWJq1qyZQkJC0rRfSeLj4xUdHa3ChQvzychNEhMTdfz48XTX5nbap9upLhk1n9upvhwz1jhmrCXVpWnTpvLx8XF7O0ln49wKGUqGZuV2+H1ojd+HrnHMWOOYsUaGkqE3stOxLfH70BV+H7rGMWONY8ZaVmeo5GYT+LnnnnP6OT4+XpcuXZKvr68CAgIyJXx9fX0VGRmplStXqlWrVpKuF2zlypXq06eP5Tp16tTRypUr1a9fP8ey6Oho1alTR5JUsmRJhYWFaeXKlY6wPX/+vDZu3Kinn37a5Vz8/Pzk5+eXbLmPj0+6XjhJuuuuu9K9DbuJj4/X8ePHqc1NqItr1MYadbGWVJf0/g5P7bpkKBmalfh3b426uEZtrFEXa2QoGWpn/Lu3Rl1cozbWqIu1rM5Qyc0m8L///pts2d69e/X0009r8ODB7mwyVQYMGKAuXbqoRo0aqlmzpiZNmqTY2Fh169ZN0vVPfu+44w6NGTNG0vU/Eho0aKAJEyaoRYsWmj9/vjZv3qwZM2ZIuv5tsv369dOrr76qMmXKqGTJknrppZdUpEgRR8ADAJCRyFAAANxDhgIA4D637wl8szJlymjs2LHq1KmTyxvkp1f79u116tQpvfzyy4qJiVG1atW0fPlyxw31jxw54nRqed26dfXJJ59o+PDhevHFF1WmTBktWbJElStXdowZMmSIYmNj9eSTT+rs2bOqV6+eli9fLn9//0zZBwAAbkaGAgDgHjIUAIDUybAmsHT9Jv3Hjh3LyE0m06dPH5eX3axZsybZsrZt26pt27Yut+fh4aFXXnlFr7zySkZNEQCANCNDAQBwDxkKAMCtudUE/uqrr5x+Nsbo+PHjevfdd3X33XdnyMQAALAjMhQAAPeQoQAAuM+tJvDN9yny8PBQwYIF1ahRI02YMCEj5gUAgC2RoQAAuIcMBQDAfW41gRMTEzN6HgAA/CeQoQAAuIcMBQDAfZ63HgIAAAAAAAAAyKncOhN4wIABqR47ceJEd54CAABbIkMBAHAPGQoAgPvcagJv27ZN27ZtU3x8vMqVKydJ2rNnj7y8vFS9enXHOA8Pj4yZJQAANkGGAgDgHjIUAAD3udUEfvDBBxUcHKy5c+cqb968kqR///1X3bp1U/369TVw4MAMnSQAAHZBhgIA4B4yFAAA97l1T+AJEyZozJgxjuCVpLx58+rVV1/lW1kBAEgBGQoAgHvIUAAA3OdWE/j8+fM6depUsuWnTp3ShQsX0j0pAADsigwFAMA9ZCgAAO5zqwn88MMPq1u3blq8eLH+/vtv/f333/r888/VvXt3PfLIIxk9RwAAbIMMBQDAPWQoAADuc+uewNOnT9egQYP02GOPKT4+/vqGvL3VvXt3vfHGGxk6QQAA7IQMBQDAPWQoAADuc6sJHBAQoKlTp+qNN97Q/v37JUkREREKDAzM0MkBAGA3ZCgAAO4hQwEAcJ9bt4NIcvz4cR0/flxlypRRYGCgjDEZNS8AAGyNDAUAwD1kKAAAaedWE/iff/5R48aNVbZsWd1///06fvy4JKl79+4aOHBghk4QAAA7IUMBAHAPGQoAgPvcagL3799fPj4+OnLkiAICAhzL27dvr+XLl2fY5AAAsBsyFAAA95ChAAC4z617Aq9YsULfffedihYt6rS8TJkyOnz4cIZMDAAAOyJDAQBwDxkKAID73DoTODY21umT1yRnzpyRn59fuicFAIBdkaEAALiHDAUAwH1uNYHr16+vDz/80PGzh4eHEhMTNX78eDVs2DDDJgcAgN2QoQAAuIcMBQDAfW7dDmL8+PFq3LixNm/erKtXr2rIkCHasWOHzpw5o3Xr1mX0HAEAsA0yFAAA95ChAAC4z60zgStXrqw9e/aoXr16atmypWJjY/XII49o27ZtioiIyOg5AgBgG2QoAADuIUMBAHBfms8Ejo+PV/PmzTV9+nQNGzYsM+YEAIAtkaEAALiHDAUAIH3SfCawj4+Pfvvtt8yYCwAAtkaGAgDgHjIUAID0cet2EJ06ddLMmTMzei4AANgeGQoAgHvIUAAA3OfWF8Ndu3ZNs2bN0vfff6/IyEgFBgY6PT5x4sQMmRwAAHZDhgIA4B4yFAAA96WpCXzgwAGFh4frjz/+UPXq1SVJe/bscRrj4eGRcbMDAMAmyFAAANxDhgIAkH5pagKXKVNGx48f1+rVqyVJ7du319tvv61ChQplyuQAALALMhQAAPeQoQAApF+a7glsjHH6edmyZYqNjc3QCQEAYEdkKAAA7iFDAQBIP7e+GC7JzWEMAABShwwFAMA9ZCgAAGmXpiawh4dHsnstce8lAABujQwFAMA9ZCgAAOmXpnsCG2PUtWtX+fn5SZKuXLmip556Ktm3si5evDjjZggAgA2QoQAAuIcMBQAg/dLUBO7SpYvTz506dcrQyQAAYFdkKAAA7iFDAQBIvzQ1gWfPnp1Z8wAAwNbIUAAA3EOGAgCQfun6YjgAAAAAAAAAwO2NJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwsRzTBD5z5ow6duyokJAQ5cmTR927d9fFixdTXOfKlSvq3bu38ufPr6CgILVu3VonTpxwPP7rr7+qQ4cOKlasmHLlyqUKFSpo8uTJmb0rAABkKTIUAAD3kKEAALvIMU3gjh07aseOHYqOjtY333yjH3/8UU8++WSK6/Tv319ff/21Fi1apB9++EHHjh3TI4884nh8y5YtCg0N1ccff6wdO3Zo2LBhGjp0qN59993M3h0AALIMGQoAgHvIUACAXXhn9wRSY9euXVq+fLl++eUX1ahRQ5L0zjvv6P7779ebb76pIkWKJFvn3Llzmjlzpj755BM1atRIkjR79mxVqFBBP//8s2rXrq0nnnjCaZ1SpUppw4YNWrx4sfr06ZP5OwYAQCYjQwEAcA8ZCgCwkxxxJvCGDRuUJ08eR/BKUpMmTeTp6amNGzdarrNlyxbFx8erSZMmjmXly5dX8eLFtWHDBpfPde7cOeXLly/jJg8AQDYiQwEAcA8ZCgCwkxxxJnBMTIxCQ0Odlnl7eytfvnyKiYlxuY6vr6/y5MnjtLxQoUIu11m/fr0WLFigb7/9NsX5xMXFKS4uzvHz+fPnJUnx8fGKj4+/1e5YSlrP3fXtjNpYoy6uURtr1MVaRtXldq0rGfrfRm2sURfXqI016mKNDLVehwy1B2pjjbq4Rm2sURdr2ZGh2doEfuGFFzRu3LgUx+zatStL5vLHH3+oZcuWGjFihJo1a5bi2DFjxmjUqFHJlq9YsUIBAQHpmkd0dHS61rczamONurhGbaxRF2vprculS5cyaCapQ4Ymx7HtGrWxRl1cozbWqIs1MtR9ZOjtj9pYoy6uURtr1MVaVmZotjaBBw4cqK5du6Y4plSpUgoLC9PJkyedll+7dk1nzpxRWFiY5XphYWG6evWqzp496/Qp7IkTJ5Kts3PnTjVu3FhPPvmkhg8ffst5Dx06VAMGDHD8fP78eRUrVkzNmjVTSEjILde3Eh8fr+joaDVt2lQ+Pj5ubcOuqI016uIatbFGXaxlVF2SzsbJKmTo/+PYdo3aWKMurlEba9TFGhmaHBlqH9TGGnVxjdpYoy7WsiNDs7UJXLBgQRUsWPCW4+rUqaOzZ89qy5YtioyMlCStWrVKiYmJqlWrluU6kZGR8vHx0cqVK9W6dWtJ0u7du3XkyBHVqVPHMW7Hjh1q1KiRunTpotdeey1V8/bz85Ofn1+y5T4+Puk+oDNiG3ZFbaxRF9eojTXqYi29dcnqmpKhyXFsu0ZtrFEX16iNNepijQz9f2So/VAba9TFNWpjjbpYy8oMzRFfDFehQgU1b95cPXv21KZNm7Ru3Tr16dNHjz76qOMbWY8ePary5ctr06ZNkqTcuXOre/fuGjBggFavXq0tW7aoW7duqlOnjmrXri3p+qU3DRs2VLNmzTRgwADFxMQoJiZGp06dyrZ9BQAgI5GhAAC4hwwFANhJjvhiOEmaN2+e+vTpo8aNG8vT01OtW7fW22+/7Xg8Pj5eu3fvdroXxltvveUYGxcXp6ioKE2dOtXx+GeffaZTp07p448/1scff+xYXqJECR06dChL9gsAgMxGhgIA4B4yFABgFzmmCZwvXz598sknLh8PDw+XMcZpmb+/v6ZMmaIpU6ZYrjNy5EiNHDkyI6cJAMBthwwFAMA9ZCgAwC5yxO0gAAAAAAAAAADuoQkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsLMc0gc+cOaOOHTsqJCREefLkUffu3XXx4sUU17ly5Yp69+6t/PnzKygoSK1bt9aJEycsx/7zzz8qWrSoPDw8dPbs2UzYAwAAsgcZCgCAe8hQAIBd5JgmcMeOHbVjxw5FR0frm2++0Y8//qgnn3wyxXX69++vr7/+WosWLdIPP/ygY8eO6ZFHHrEc2717d915552ZMXUAALIVGQoAgHvIUACAXeSIJvCuXbu0fPlyffDBB6pVq5bq1aund955R/Pnz9exY8cs1zl37pxmzpypiRMnqlGjRoqMjNTs2bO1fv16/fzzz05jp02bprNnz2rQoEFZsTsAAGQZMhQAAPeQoQAAO8kRTeANGzYoT548qlGjhmNZkyZN5OnpqY0bN1qus2XLFsXHx6tJkyaOZeXLl1fx4sW1YcMGx7KdO3fqlVde0YcffihPzxxRDgAAUo0MBQDAPWQoAMBOvLN7AqkRExOj0NBQp2Xe3t7Kly+fYmJiXK7j6+urPHnyOC0vVKiQY524uDh16NBBb7zxhooXL64DBw6kaj5xcXGKi4tz/Hz+/HlJUnx8vOLj41O7W06S1nN3fTujNtaoi2vUxhp1sZZRdbld60qG/rdRG2vUxTVqY426WCNDrdchQ+2B2lijLq5RG2vUxVp2ZGi2NoFfeOEFjRs3LsUxu3btyrTnHzp0qCpUqKBOnTqlab0xY8Zo1KhRyZavWLFCAQEB6ZpTdHR0uta3M2pjjbq4Rm2sURdr6a3LpUuXMmgmqUOGJsex7Rq1sUZdXKM21qiLNTI0bcjQnIXaWKMurlEba9TFWlZmaLY2gQcOHKiuXbumOKZUqVIKCwvTyZMnnZZfu3ZNZ86cUVhYmOV6YWFhunr1qs6ePev0KeyJEycc66xatUq///67PvvsM0mSMUaSVKBAAQ0bNswyYKXroT1gwADHz+fPn1exYsXUrFkzhYSEpLg/rsTHxys6OlpNmzaVj4+PW9uwK2pjjbq4Rm2sURdrGVWXpLNxsgoZ+v84tl2jNtaoi2vUxhp1sUaGJkeG2ge1sUZdXKM21qiLtezI0GxtAhcsWFAFCxa85bg6dero7Nmz2rJliyIjIyVdD87ExETVqlXLcp3IyEj5+Pho5cqVat26tSRp9+7dOnLkiOrUqSNJ+vzzz3X58mXHOr/88oueeOIJrV27VhERES7n4+fnJz8/v2TLfXx80n1AZ8Q27IraWKMurlEba9TFWnrrktU1JUOT49h2jdpYoy6uURtr1MUaGfr/yFD7oTbWqItr1MYadbGWlRmaI+4JXKFCBTVv3lw9e/bU9OnTFR8frz59+ujRRx9VkSJFJElHjx5V48aN9eGHH6pmzZrKnTu3unfvrgEDBihfvnwKCQnRs88+qzp16qh27dqSlCxgT58+7Xi+m+/hBABATkSGAgDgHjIUAGAnOaIJLEnz5s1Tnz591LhxY3l6eqp169Z6++23HY/Hx8dr9+7dTvfCeOuttxxj4+LiFBUVpalTp2bH9AEAyDZkKAAA7iFDAQB2kWOawPny5dMnn3zi8vHw8HDHvZSS+Pv7a8qUKZoyZUqqnuPee+9Ntg0AAHI6MhQAAPeQoQAAu/DM7gkAAAAAAAAAADIPTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICN0QQGAAAAAAAAABujCQwAAAAAAAAANkYTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAAAAAALAxmsAAAAAAAAAAYGM0gQEAAAAAAADAxmgCAwAAAAAAAICNeWf3BOzAGCNJOn/+vNvbiI+P16VLl3T+/Hn5+Phk1NRsgdpYoy6uURtr1MVaRtUlKQOSMgGpQ4ZmLmpjjbq4Rm2sURdrZGj2IkMzF7WxRl1cozbWqIu17MhQmsAZ4MKFC5KkYsWKZfNMAADZ7cKFC8qdO3d2TyPHIEMBAEnI0LQhQwEASVKToR6Gj1vTLTExUceOHVNwcLA8PDzc2sb58+dVrFgx/fXXXwoJCcngGeZs1MYadXGN2lijLtYyqi7GGF24cEFFihSRpyd3W0otMjRzURtr1MU1amONulgjQ7MXGZq5qI016uIatbFGXaxlR4ZyJnAG8PT0VNGiRTNkWyEhIfyjcIHaWKMurlEba9TFWkbUhbOX0o4MzRrUxhp1cY3aWKMu1sjQ7EGGZg1qY426uEZtrFEXa1mZoXzMCgAAAAAAAAA2RhMYAAAAAAAAAGyMJvBtws/PTyNGjJCfn192T+W2Q22sURfXqI016mKNuuR8vIauURtr1MU1amONulijLjkfr6Fr1MYadXGN2lijLtayoy58MRwAAAAAAAAA2BhnAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmcBaaMmWKwsPD5e/vr1q1amnTpk0pjl+0aJHKly8vf39/ValSRUuXLs2imWa9tNTm/fffV/369ZU3b17lzZtXTZo0uWUtc6q0HjNJ5s+fLw8PD7Vq1SpzJ5hN0lqXs2fPqnfv3ipcuLD8/PxUtmxZ2/57SmttJk2apHLlyilXrlwqVqyY+vfvrytXrmTRbLPGjz/+qAcffFBFihSRh4eHlixZcst11qxZo+rVq8vPz0+lS5fWnDlzMn2eSBkZ6hoZao0MtUaGukaGJkeG2gMZ6hoZao0MtUaGukaGJndbZqhBlpg/f77x9fU1s2bNMjt27DA9e/Y0efLkMSdOnLAcv27dOuPl5WXGjx9vdu7caYYPH258fHzM77//nsUzz3xprc1jjz1mpkyZYrZt22Z27dplunbtanLnzm3+/vvvLJ555kprXZIcPHjQ3HHHHaZ+/fqmZcuWWTPZLJTWusTFxZkaNWqY+++/3/z000/m4MGDZs2aNWb79u1ZPPPMl9bazJs3z/j5+Zl58+aZgwcPmu+++84ULlzY9O/fP4tnnrmWLl1qhg0bZhYvXmwkmS+++CLF8QcOHDABAQFmwIABZufOneadd94xXl5eZvny5VkzYSRDhrpGhlojQ62Roa6RodbI0JyPDHWNDLVGhlojQ10jQ63djhlKEziL1KxZ0/Tu3dvxc0JCgilSpIgZM2aM5fh27dqZFi1aOC2rVauW6dWrV6bOMzuktTY3u3btmgkODjZz587NrClmC3fqcu3aNVO3bl3zwQcfmC5dutgyfNNal2nTpplSpUqZq1evZtUUs01aa9O7d2/TqFEjp2UDBgwwd999d6bOMzulJnyHDBliKlWq5LSsffv2JioqKhNnhpSQoa6RodbIUGtkqGtk6K2RoTkTGeoaGWqNDLVGhrpGht7a7ZKh3A4iC1y9elVbtmxRkyZNHMs8PT3VpEkTbdiwwXKdDRs2OI2XpKioKJfjcyp3anOzS5cuKT4+Xvny5cusaWY5d+vyyiuvKDQ0VN27d8+KaWY5d+ry1VdfqU6dOurdu7cKFSqkypUr6/XXX1dCQkJWTTtLuFObunXrasuWLY5LdQ4cOKClS5fq/vvvz5I5367+K79/cwoy1DUy1BoZao0MdY0MzTj/ld+/OQUZ6hoZao0MtUaGukaGZpys+P3rnWFbgkunT59WQkKCChUq5LS8UKFC+vPPPy3XiYmJsRwfExOTafPMDu7U5mbPP/+8ihQpkuwfS07mTl1++uknzZw5U9u3b8+CGWYPd+py4MABrVq1Sh07dtTSpUu1b98+PfPMM4qPj9eIESOyYtpZwp3aPPbYYzp9+rTq1asnY4yuXbump556Si+++GJWTPm25er37/nz53X58mXlypUrm2b230SGukaGWiNDrZGhrpGhGYcMvb2Qoa6RodbIUGtkqGtkaMbJigzlTGDkaGPHjtX8+fP1xRdfyN/fP7unk20uXLigzp076/3331eBAgWyezq3lcTERIWGhmrGjBmKjIxU+/btNWzYME2fPj27p5bt1qxZo9dff11Tp07V1q1btXjxYn377bcaPXp0dk8NQBYgQ68jQ10jQ10jQ4H/NjL0OjLUNTLUNTI0+3AmcBYoUKCAvLy8dOLECaflJ06cUFhYmOU6YWFhaRqfU7lTmyRvvvmmxo4dq++//1533nlnZk4zy6W1Lvv379ehQ4f04IMPOpYlJiZKkry9vbV7925FRERk7qSzgDvHS+HCheXj4yMvLy/HsgoVKigmJkZXr16Vr69vps45q7hTm5deekmdO3dWjx49JElVqlRRbGysnnzySQ0bNkyenv/Nzwld/f4NCQnhDKZsQIa6RoZaI0OtkaGukaEZhwy9vZChrpGh1shQa2Soa2RoxsmKDP1vVjaL+fr6KjIyUitXrnQsS0xM1MqVK1WnTh3LderUqeM0XpKio6Ndjs+p3KmNJI0fP16jR4/W8uXLVaNGjayYapZKa13Kly+v33//Xdu3b3f899BDD6lhw4bavn27ihUrlpXTzzTuHC9333239u3b5/hjRJL27NmjwoUL2yZ4Jfdqc+nSpWQBm/RHyvV71/83/Vd+/+YUZKhrZKg1MtQaGeoaGZpx/iu/f3MKMtQ1MtQaGWqNDHWNDM04WfL7N8O+Yg4pmj9/vvHz8zNz5swxO3fuNE8++aTJkyePiYmJMcYY07lzZ/PCCy84xq9bt854e3ubN9980+zatcuMGDHC+Pj4mN9//z27diHTpLU2Y8eONb6+vuazzz4zx48fd/x34cKF7NqFTJHWutzMrt/Kmta6HDlyxAQHB5s+ffqY3bt3m2+++caEhoaaV199Nbt2IdOktTYjRowwwcHB5tNPPzUHDhwwK1asMBEREaZdu3bZtQuZ4sKFC2bbtm1m27ZtRpKZOHGi2bZtmzl8+LAxxpgXXnjBdO7c2TH+wIEDJiAgwAwePNjs2rXLTJkyxXh5eZnly5dn1y7855GhrpGh1shQa2Soa2SoNTI05yNDXSNDrZGh1shQ18hQa7djhtIEzkLvvPOOKV68uPH19TU1a9Y0P//8s+OxBg0amC5dujiNX7hwoSlbtqzx9fU1lSpVMt9++20WzzjrpKU2JUqUMJKS/TdixIisn3gmS+sxcyO7hq8xaa/L+vXrTa1atYyfn58pVaqUee2118y1a9eyeNZZIy21iY+PNyNHjjQRERHG39/fFCtWzDzzzDPm33//zfqJZ6LVq1db/s5IqkWXLl1MgwYNkq1TrVo14+vra0qVKmVmz56d5fOGMzLUNTLUGhlqjQx1jQxNjgy1BzLUNTLUGhlqjQx1jQxN7nbMUA9j/sPnWgMAAAAAAACAzXFPYAAAAAAAAACwMZrAAAAAAAAAAGBjNIEBAAAAAAAAwMZoAgMAAAAAAACAjdEEBgAAAAAAAAAbowkMAAAAAAAAADZGExgAAAAAAAAAbIwmMAAAAAAAAADYGE1gAAAAIAvce++96tevX3ZPI0UrV65UhQoVlJCQ4HLMCy+8oGeffTYLZwUAyE4eHh5asmRJdk8jy3Tu3Fmvv/56pj8PeYqsRhMYQI7z119/6d5771XFihV15513atGiRdk9JQDAbaBr167y8PCQh4eHfHx8VLJkSQ0ZMkRXrlzJ7qnlGEOGDNHw4cPl5eXlcsygQYM0d+5cHThwIAtnBgDISDdnZqFChdS0aVPNmjVLiYmJTmOPHz+u++67L5tm+v9GjhypatWqZepz/Prrr1q6dKn69u2bqc8jkafIejSBAeQ43t7emjRpknbu3KkVK1aoX79+io2Nze5pAQBuA82bN9fx48d14MABvfXWW3rvvfc0YsSI7J5WjvDTTz9p//79at26dYrjChQooKioKE2bNi2LZgYAyAxJmXno0CEtW7ZMDRs21HPPPacHHnhA165dc4wLCwuTn59fNs40Y129etXlY++8847atm2roKCgTJ8HeYqsRhMYQIb5559/FBoaqkOHDmXq8xQuXNjxCXBYWJgKFCigM2fOOB5/9NFHNWHChEydAwDg9uTn56ewsDAVK1ZMrVq1UpMmTRQdHe14PC4uTn379lVoaKj8/f1Vr149/fLLL47H58yZozx58jhtc8mSJfLw8HD8nHQm0kcffaTw8HDlzp1bjz76qC5cuOAYExsbq8cff1xBQUEqXLiwZS6Fh4fr9ddf1xNPPKHg4GAVL15cM2bMcBrz119/qV27dsqTJ4/y5cunli1bOuXsmjVrVLNmTQUGBipPnjy6++67dfjwYUnXz2Zq2LChgoODFRISosjISG3evNll7ebPn6+mTZvK398/5SJLevDBBzV//vxbjgMA3L6SMvOOO+5Q9erV9eKLL+rLL7/UsmXLNGfOHMe4m28H8fzzz6ts2bIKCAhQqVKl9NJLLyk+Pt7xeFJOzpo1S8WLF1dQUJCeeeYZJSQkaPz48QoLC1NoaKhee+01p/mcPXtWPXr0UMGCBRUSEqJGjRrp119/lXQ9n0eNGqVff/3VcQZz0hxTWu/G+XzwwQcqWbKky5xLSEjQZ599pgcffNBp+a3y+tChQ/Lw8NDChQtVv3595cqVS//73/+0Z88e/fLLL6pRo4aCgoJ033336dSpU07bJk+RlWgCAznchg0b5OXlpRYtWmT3VPTaa6+pZcuWCg8Pz7Ln3LJlixISElSsWDHHsuHDh+u1117TuXPnsmweAIDbzx9//KH169fL19fXsWzIkCH6/PPPNXfuXG3dulWlS5dWVFSU04eJqbF//34tWbJE33zzjb755hv98MMPGjt2rOPxwYMH64cfftCXX36pFStWaM2aNdq6dWuy7UyYMEE1atTQtm3b9Mwzz+jpp5/W7t27JUnx8fGKiopScHCw1q5dq3Xr1ikoKEjNmzfX1atXde3aNbVq1UoNGjTQb7/9pg0bNujJJ590NKw7duyookWL6pdfftGWLVv0wgsvyMfHx+U+rV27VjVq1EjV/tesWVN///13pn/wCwDIWo0aNVLVqlW1ePFil2OCg4M1Z84c7dy5U5MnT9b777+vt956y2nM/v37tWzZMi1fvlyffvqpZs6cqRYtWujvv//WDz/8oHHjxmn48OHauHGjY522bdvq5MmTWrZsmbZs2aLq1aurcePGOnPmjNq3b6+BAweqUqVKOn78uI4fP6727dvfcr0k+/bt0+eff67Fixdr+/btlvv122+/6dy5c5ZZmFJeJxkxYoSGDx+urVu3ytvbW4899piGDBmiyZMna+3atdq3b59efvllp3XIU2QpAyBH6969u3nuuedMUFCQOXr0aIpj4+LiMm0esbGxJiQkxGzYsCHTnuNm//zzj6lYsaJZt25dssdq1Khh3n333SybCwAg+3Xp0sV4eXmZwMBA4+fnZyQZT09P89lnnxljjLl48aLx8fEx8+bNc6xz9epVU6RIETN+/HhjjDGzZ882uXPndtruF198YW78s3nEiBEmICDAnD9/3rFs8ODBplatWsYYYy5cuGB8fX3NwoULHY//888/JleuXOa5555zLCtRooTp1KmT4+fExEQTGhpqpk2bZowx5qOPPjLlypUziYmJjjFxcXEmV65c5rvvvjP//POPkWTWrFljWY/g4GAzZ86cVNXOGGNy585tPvzww1SNPXfuXIrPDQC4vXXp0sW0bNnS8rH27dubChUqOH6WZL744guX23rjjTdMZGSk42ernIyKijLh4eEmISHBsaxcuXJmzJgxxhhj1q5da0JCQsyVK1ecth0REWHee+89x3arVq3q9Hhq1/Px8TEnT550uQ/GXM97Ly8vp9w15tZ5ffDgQSPJfPDBB44xn376qZFkVq5c6Vg2ZswYU65cOadtk6fISpwJDORgFy9e1IIFC/T000+rRYsWTpfsSNe/hbxPnz7q16+f435DkpSYmKgxY8aoZMmSypUrl6pWrarPPvvMsd7y5ctVr1495cmTR/nz59cDDzyg/fv3pziXpUuXys/PT7Vr1042h759+2rIkCHKly+fwsLCNHLkSKfHn332WfXr10958+ZVoUKF9P777ys2NlbdunVTcHCwSpcurWXLljltNy4uTq1atdILL7ygunXrJpsPl9UAwH9Tw4YNtX37dm3cuFFdunRRt27dHPe43b9/v+Lj43X33Xc7xvv4+KhmzZratWtXmp4nPDxcwcHBjp8LFy6skydPOp7n6tWrqlWrluPxfPnyqVy5csm2c+eddzr+v4eHh8LCwhzb+fXXX7Vv3z4FBwcrKChIQUFBypcvn65cuaL9+/crX7586tq1q6KiovTggw9q8uTJOn78uGN7AwYMUI8ePdSkSRONHTv2lll++fJlp0tkjxw54njeoKAgp29Kz5UrlyTp0qVLqaoXACDnMMY43QbpZgsWLNDdd9+tsLAwBQUFafjw4Tpy5IjTmJtzslChQqpYsaI8PT2dlt2YeRcvXlT+/PmdsufgwYMp5ldq1ytRooQKFiyY4n5fvnxZfn5+lvueUl5bjSlUqJAkqUqVKpb7m4Q8RVaiCQzkYAsXLlT58uVVrlw5derUSbNmzZIxxmnM3Llz5evrq3Xr1mn69OmSpDFjxujDDz/U9OnTtWPHDvXv31+dOnXSDz/8IOn6fQwHDBigzZs3a+XKlfL09NTDDz+c7Ftib7R27VpFRkZaPjZ37lwFBgZq48aNGj9+vF555RWn+zPOnTtXBQoU0KZNm/Tss8/q6aefVtu2bVW3bl1t3bpVzZo1U+fOnR3BaIxR165d1ahRI3Xu3NnyOWvWrKlNmzYpLi4u9QUFAOR4gYGBKl26tKpWrapZs2Zp48aNmjlzZqrX9/T0TJalN97nMMnNt1Xw8PBIMSddSWk7Fy9eVGRkpLZv3+703549e/TYY49JkmbPnq0NGzaobt26WrBggcqWLauff/5Z0vV7IO7YsUMtWrTQqlWrVLFiRX3xxRcu51KgQAH9+++/jp+LFCni9LxPPfWU47GkS2xv9YYaAJDz7Nq1SyVLlrR8bMOGDerYsaPuv/9+ffPNN9q2bZuGDRuW7MvWrPLtVplXuHDhZJm3e/duDR482OVcU7teYGDgLfe7QIECunTpkuUXx6Um928ck9RIvnnZzeuQp8hK3tk9AQDumzlzpjp16iTp+je7njt3Tj/88IPuvfdex5gyZcpo/Pjxjp/j4uL0+uuv6/vvv1edOnUkSaVKldJPP/2k9957Tw0aNEj2reCzZs1SwYIFtXPnTlWuXNlyLocPH1aRIkUsH7vzzjsd38xepkwZvfvuu1q5cqWaNm0qSapataqGDx8uSRo6dKjGjh2rAgUKqGfPnpKkl19+WdOmTdNvv/2m2rVra926dVqwYIHuvPNOxxcUfPTRR06fshYpUkRXr15VTEyMSpQokap6AgDsxdPTUy+++KIGDBigxx57TBEREY4PRpOyIT4+Xr/88ov69esn6fqbsAsXLig2NtbxhtHVvQNdiYiIkI+PjzZu3KjixYtLkv7991/t2bNHDRo0SPV2qlevrgULFig0NFQhISEux91111266667NHToUNWpU0effPKJ48qcsmXLqmzZsurfv786dOig2bNn6+GHH3a5nZ07dzp+9vb2VunSpS3H/vHHH/Lx8VGlSpVSvT8AgNvfqlWr9Pvvv6t///6Wj69fv14lSpTQsGHDHMuSvpA0PapXr66YmBh5e3u7/I4ZX19fJSQkpHm91Er68vGdO3c6/n9mI0+RlTgTGMihdu/erU2bNqlDhw6Srr9Ra9++fbKznW4+O3ffvn26dOmSmjZt6nS5zIcffui4XGbv3r3q0KGDSpUqpZCQEEeY3nyJz41uvoT0RjdeFiM5XzJ78+NeXl7Knz9/sstmJDnWqVevnhITE50+6b1xvMRlNQCA69q2bSsvLy9NmTJFgYGBevrppzV48GAtX75cO3fuVM+ePXXp0iV1795dklSrVi0FBAToxRdf1P79+/XJJ58ku93SrQQFBal79+4aPHiwVq1apT/++ENdu3Z1ugQ2NTp27KgCBQqoZcuWWrt2rQ4ePKg1a9aob9+++vvvv3Xw4EENHTpUGzZs0OHDh7VixQrt3btXFSpU0OXLl9WnTx+tWbNGhw8f1rp16/TLL7+oQoUKLp8vKipKP/30U6rmtnbtWsc3oAMAcqa4uDjFxMTo6NGj2rp1q15//XW1bNlSDzzwgB5//HHLdcqUKaMjR45o/vz52r9/v95+++0UrzJJrSZNmqhOnTpq1aqVVqxYoUOHDmn9+vUaNmyYNm/eLOn6LSYOHjyo7du36/Tp04qLi0vVeqlVsGBBVa9ePdVZmBHIU2QlmsBADjVz5kxdu3ZNRYoUkbe3t7y9vTVt2jR9/vnnOnfunGPczZe9XLx4UZL07bffOjVRd+7c6bgv8IMPPqgzZ87o/fff18aNGx3f2Gp1WUySmy8hvdGtLp251aVCSZfSpOUyWy6rAQBI1z8k7dOnj8aPH6/Y2FiNHTtWrVu3VufOnVW9enXt27dP3333nfLmzSvp+r17P/74Yy1dulRVqlTRp59+6nQv+9R64403VL9+fT344INq0qSJ6tWr5/K2Sa4EBAToxx9/VPHixfXII4+oQoUK6t69u65cuaKQkBAFBATozz//VOvWrVW2bFk9+eST6t27t3r16iUvLy/9888/evzxx1W2bFm1a9dO9913n0aNGuXy+Tp27KgdO3Yk+7ZzK/Pnz3dcsQMAyJmWL1+uwoULKzw8XM2bN9fq1av19ttv68svv5SXl5flOg899JD69++vPn36qFq1alq/fr1eeumldM/Fw8NDS5cu1T333KNu3bqpbNmyevTRR3X48GHHSUGtW7dW8+bN1bBhQxUsWFCffvppqtZLix49emjevHnp3p/UIk+RlTzMzTc9A3Dbu3btmooWLaohQ4aoWbNmTo+1atVKgwYN0lNPPaV7771X1apV06RJkxyPX7hwQQULFtT7779veT/df/75RwUKFNCPP/6o+vXrS5J++ukn1a9fX1988YVatWplOac333xTH3/8cbJLZq3m0KpVK+XJk0dz5syxfDw8PFz9+vVzXJorXf+jIKXnv9nMmTM1cuRI/fXXX6kaDwAApMGDB+v8+fN67733XI5ZtmyZBg4cqN9++03e3txdDgBgH5cvX1a5cuW0YMECx+0TMwt5iqzGUQbkQN98843+/fdfde/eXblz53Z6rHXr1po5c6bTl7fcKDg4WIMGDVL//v2VmJioevXq6dy5c1q3bp1CQkLUuXNn5c+fXzNmzFDhwoV15MgRvfDCC7ecU1RUlIYOHap///3XcTZVdlq7dm2yBjkAAEjZsGHDNHXqVCUmJrq8fUVsbKxmz57NG1YAgO3kypVLH374oU6fPp3pz0WeIqtxpAE50MyZM9WkSZNkDWDpehN4/Pjx+u2331yuP3r0aBUsWFBjxozRgQMHlCdPHlWvXl0vvviiPD09NX/+fPXt21eVK1dWuXLl9Pbbbzt92ZyVKlWqqHr16lq4cKF69eqV3l1MlytXrmjJkiVavnx5ts4DAICcJk+ePHrxxRdTHNOmTZssmg0AAFnvVu99Mwp5iqzG7SAAZJhvv/1WgwcP1h9//JHmL7/JSNOmTdMXX3yhFStWZNscAAAAAAAAbhecCQwgw7Ro0UJ79+7V0aNHVaxYsWybh4+Pj955551se34AAAAAAIDbCWcCAwAAAAAAAICNZd/12gAAAAAAAACATEcTGAAAAAAAAABsjCYwAAAAAAAAANgYTWAAAAAAAAAAsDGawAAAAAAAAABgYzSBAQAAAAAAAMDGaAIDAAAAAAAAgI3RBAYAAAAAAAAAG6MJDAAAAAAAAAA2RhMYAAAAAAAAAGyMJjAAAAAAAAAA2BhNYAAAAAAAAACwsf8DWGPPTBMtZzYAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wavelet filtering debug\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "images = os.listdir(image_folder)\n",
        "image_name = images[0]\n",
        "image = cv.imread(os.path.join(image_folder, image_name), 0)\n",
        "# cv2_imshow(image)\n",
        "\n",
        "denoised_image = wavelet_denoise(image)\n",
        "# cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "#def cropper(og_tensor, target_tensor):\n",
        "#    og_shape = og_tensor.shape[2]\n",
        "#    target_shape = target_tensor.shape[2]\n",
        "#    delta = (og_shape - target_shape) // 2\n",
        "#    cropped_og_tensor = og_tensor[:,:,delta:og_shape-delta,delta:og_shape-delta]\n",
        "#    return cropped_og_tensor\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIOOv16zNmOp"
      },
      "source": [
        "# Training function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "z4GQhPsENrxX"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler, train_losses):\n",
        "    loop = tqdm(loader)\n",
        "    running_loss = 0\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "    # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data) # TODO: shoud this be wrapped in sigmoid???\n",
        "            loss = loss_fn(torch.sigmoid(predictions), targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        # print(batch_idx)\n",
        "    number_of_batches = batch_idx+1\n",
        "    train_losses.append(running_loss/number_of_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Patch creation"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE_NEW_PATCHES = False\n",
        "SAVE_PATCHES_TO_DISK = False\n",
        "# Example usage:\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "mask_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_masks\"\n",
        "\n",
        "if CREATE_NEW_PATCHES:\n",
        "    patch_size = 512  # Define your patch size here\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\" # TODO: save created patches somewhere on disk\n",
        "    else:\n",
        "        output_folder = os.getcwd()\n",
        "    image_patches_path, mask_patches_path = create_image_patches(image_folder, mask_folder, output_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\"\n",
        "    image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, 'mask_patches')\n"
      ],
      "metadata": {
        "id": "UzznzOTP4s53"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MVM6cZYK0Oy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "4yt1dWbZiNkm"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-4\n",
        "batch_size = 5\n",
        "num_epochs = 2\n",
        "num_workers = 2\n",
        "pin_memory = False # TODO: check\n",
        "load_model = False\n",
        "\n",
        "# Define dataloaders for training\n",
        "data_split = 0.1\n",
        "\n",
        "image_patches_path = image_folder\n",
        "mask_patches_path = mask_folder\n",
        "\n",
        "train_loader, val_loader, train_indices = get_loaders(\n",
        "    image_patches_path,\n",
        "    mask_patches_path,\n",
        "    data_split,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers,\n",
        "    pin_memory\n",
        ")\n",
        "\n",
        "model = UNET(in_channels=1, out_channels=1, device=DEVICE).to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb debug"
      ],
      "metadata": {
        "id": "ocAb9yySYXo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFk7zxOVU6FH",
        "outputId": "92e1a940-1584-426b-a206-2dc336e75d69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.4)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "!pip install wandb\n",
        "# train.py\n",
        "import wandb\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "mHPH9t7TRq6-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576,
          "referenced_widgets": [
            "a572916ece2049c097a73aedb83447d2",
            "619303bc7b3a47fc8421ff6b6e49f235",
            "8f0464eedc2b4c5595f054d5c09ffbce",
            "75cb8943e5824bfa88d57d8db245114b",
            "e3ca190210ed42f0a3e733c857660415",
            "91e74d8e5de04428a0370ff79e7baf41",
            "54ba282ea30d4a46b0fbc9a8985d63f7",
            "b9ba28b3d63444d3919301ddb05051f3"
          ]
        },
        "outputId": "5bb7e23f-9002-4481-c65e-ab19c9e53c88"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:zhpsp3r9) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.010 MB uploaded\\r'), FloatProgress(value=0.11856232939035487, max=1.…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a572916ece2049c097a73aedb83447d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">thriving-snake-21</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/zhpsp3r9' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/zhpsp3r9</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240219_140835-zhpsp3r9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:zhpsp3r9). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240219_141021-wetz4wg1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/wetz4wg1' target=\"_blank\">lucky-firecracker-22</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/wetz4wg1' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/wetz4wg1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 97.06 MiB is free. Process 4300 has 14.65 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d6d6cbb73a06>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-1bab8c719578>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# x = x.to(self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_conv_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_conv_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 640.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 97.06 MiB is free. Process 4300 has 14.65 GiB memory in use. Of the allocated memory 14.27 GiB is allocated by PyTorch, and 250.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "weights = getClassWeights(mask_patches_path, train_indices)\n",
        "w1 = weights[0]\n",
        "w2 = weights[1]\n",
        "# print(w2/w1)\n",
        "\n",
        "# Launch 5 experiments, trying different dropout rates\n",
        "batch_sizes = [5]\n",
        "for i in range(len(batch_sizes)):\n",
        "    wandb.init(\n",
        "        project=\"LSEC_segmentation\",\n",
        "        config={\n",
        "            \"epochs\": 10,\n",
        "            \"batch_size\": batch_sizes[i],\n",
        "            \"lr\": 1e-3,\n",
        "            # \"dropout\": random.uniform(0.01, 0.80),\n",
        "            })\n",
        "    config = wandb.config\n",
        "    model = UNET(in_channels=1, out_channels=1, device=DEVICE).to(DEVICE)\n",
        "    # n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
        "\n",
        "    # # Make the loss and optimizer\n",
        "    # loss_func = nn.CrossEntropyLoss()\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "\n",
        "\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=w2/w1) # cross entropy loss for more than one class(out channels)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "   # Training\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "\n",
        "    example_ct = 0\n",
        "    step_ct = 0\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        for step, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "\n",
        "            images = images.to(device=DEVICE)\n",
        "            labels = labels.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "            # forward\n",
        "            with torch.cuda.amp.autocast():\n",
        "                predictions = torch.sigmoid(model(images))\n",
        "                predictions = (predictions > 0.5).float()\n",
        "                train_loss = loss_fn(predictions, labels)\n",
        "\n",
        "            # backward\n",
        "            optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "            scaler.scale(train_loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "\n",
        "            # images, labels = images.to(DEVICE), labels.to(DEVICE).unsqueeze(1)\n",
        "            # preds = torch.sigmoid(model(images)) # TODO: vystupy modelu bez sigmoidy jsou zaporny hodnoty\n",
        "            # preds = (preds > 0.5).float()\n",
        "            # train_loss = loss_func(preds, labels)\n",
        "            # optimizer.zero_grad()\n",
        "            # train_loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            example_ct += len(images)\n",
        "            metrics = {\"train/train_loss\": train_loss,\n",
        "                    #    \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,\n",
        "                       \"train/example_ct\": example_ct}\n",
        "\n",
        "            # if step + 1 < n_steps_per_epoch: # TODO: why this condition?\n",
        "                # 🐝 Log train metrics to wandb\n",
        "            wandb.log(metrics)\n",
        "\n",
        "            step_ct += 1\n",
        "        accuracy = check_accuracy(val_loader, model, val_losses, dice_scores, DEVICE, loss_fn)\n",
        "        # val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))\n",
        "\n",
        "        # 🐝 Log train and validation metrics to wandb\n",
        "        # val_metrics = {\"val/val_loss\": val_loss,\n",
        "        #                \"val/val_accuracy\": accuracy}\n",
        "        val_metrics = {\"val/val_accuracy\": accuracy}\n",
        "        wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "        # print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "    # If you had a test set, this is how you could log it as a Summary metric\n",
        "    # wandb.summary['test_accuracy'] = 0.8\n",
        "\n",
        "    # 🐝 Close your wandb run\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VPuZwg7NzZ_q",
        "outputId": "e3d06ea6-f6ab-44cc-a853-d28e371d3084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/31 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-80a109e385d0>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-e202fbceb261>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, train_losses)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1294\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecv_handle\u001b[0;34m(conn)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;34m'''Receive a handle over a local connection.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAF_UNIX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mrecvfds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mDupFd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mrecvfds\u001b[0;34m(sock, size)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecvmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMSG_SPACE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mancdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mACKNOWLEDGE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# %%timeit\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth.tar\")\n",
        "LOAD_MODEL = False\n",
        "if LOAD_MODEL:\n",
        "    load_state_dict(model, model_path)\n",
        "    # load_checkpoint(torch.load(model_path), model)\n",
        "    # check_accuracy(val_loader, model, device=DEVICE)\n",
        "else:\n",
        "    weights = getClassWeights(mask_patches_path, train_indices)\n",
        "    w1 = weights[0]\n",
        "    w2 = weights[1]\n",
        "    # print(w2/w1)\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=w2/w1) # cross entropy loss for more than one class(out channels)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler() # ilastik\n",
        "    # summary(model, (3, 512, 512))\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler, train_losses)\n",
        "\n",
        "        # save model\n",
        "        checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict()\n",
        "        }\n",
        "        # check accuracy\n",
        "        accuracy = check_accuracy(val_loader, model, val_losses, dice_scores, DEVICE, loss_fn)\n",
        "        if accuracy > best_accuracy: # using dice score right now\n",
        "            save_state_dict(model, model_path)\n",
        "            # save_checkpoint(model, model_path)\n",
        "        accuracy = max(accuracy, best_accuracy)\n",
        "\n",
        "        # print some examples to a folder\n",
        "        # if(epoch % 5 == 0):\n",
        "        view_prediction(val_loader, model, device = DEVICE)\n",
        "        # save_predictions_as_imgs(val_loader, model, folder=model_folder, device = DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En"
      },
      "outputs": [],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz"
      },
      "outputs": [],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01.tif\"\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "patch_size = 512\n",
        "stride = 20\n",
        "\n",
        "# display(test_image_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a572916ece2049c097a73aedb83447d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_619303bc7b3a47fc8421ff6b6e49f235",
              "IPY_MODEL_8f0464eedc2b4c5595f054d5c09ffbce"
            ],
            "layout": "IPY_MODEL_75cb8943e5824bfa88d57d8db245114b"
          }
        },
        "619303bc7b3a47fc8421ff6b6e49f235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3ca190210ed42f0a3e733c857660415",
            "placeholder": "​",
            "style": "IPY_MODEL_91e74d8e5de04428a0370ff79e7baf41",
            "value": "0.010 MB of 0.010 MB uploaded\r"
          }
        },
        "8f0464eedc2b4c5595f054d5c09ffbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54ba282ea30d4a46b0fbc9a8985d63f7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b9ba28b3d63444d3919301ddb05051f3",
            "value": 1
          }
        },
        "75cb8943e5824bfa88d57d8db245114b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ca190210ed42f0a3e733c857660415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e74d8e5de04428a0370ff79e7baf41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54ba282ea30d4a46b0fbc9a8985d63f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9ba28b3d63444d3919301ddb05051f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}