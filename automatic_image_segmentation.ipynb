{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for training a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECS)."
      ],
      "metadata": {
        "id": "-aHjwiD8IkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "J-Z80u6TN3Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a network, first connect to a GPU (**Runtime -> Change runtime time -> Hardware accelerator -> GPU**).\n",
        "\n",
        "If you are using a pretrained network for inference and not training, being connected only to a **CPU** is slower, but possible."
      ],
      "metadata": {
        "id": "NUZeORlUN_LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "-gq1-hflPdMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "LHteKyDySYvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "211480eb-3231-47e7-dc5b-abe52f2297ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N5QvbqMfiA4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1c1c223-393c-478a-988c-a851aa38859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.17.0-py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.2.0-py2.py3-none-any.whl (281 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.1/281.1 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
            "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.2.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.17.0\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.4.0.post0\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.17.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=d40904cedea6f90fd6eaeb0ed4f65ed08fb64c5dffb2be30ef30f1d7e8dcad32\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=46112ab8ad1ec1640814e852602df370a1e0365cf875dc80a0c3b92b8ee14d72\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from numba import njit\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "\n",
        "# gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = False\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask /= 255\n",
        "        mask[mask == 255.0] = 1\n",
        "\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def normalize_hist(img):\n",
        "    clahe = cv.createCLAHE(10, tileGridSize=(11, 11))\n",
        "    img = clahe.apply(img)\n",
        "    img = cv.medianBlur(img, 3)\n",
        "    return img\n",
        "\n",
        "\n",
        "def get_loaders(img_train, mask_train, img_val, mask_val, batch_size, num_workers=0, pin_memory=True):\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_data = MyDataset(\n",
        "        image_dir=img_train,\n",
        "        mask_dir=mask_train,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_data = MyDataset(\n",
        "        image_dir=img_val,\n",
        "        mask_dir=mask_val,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose(\n",
        "        [\n",
        "            # A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Affine(scale=(0.9, 1.1)),\n",
        "            A.Normalize(\n",
        "                mean = 0.5,\n",
        "                std = 0.5,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.5,\n",
        "                std = 0.5,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "        mean = 0.5,\n",
        "        std = 0.5,\n",
        "        max_pixel_value=255.0,\n",
        "        ),\n",
        "            ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    print('merging masks')\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - (border_size+bottom_edge):height, :])\n",
        "    # bottom_zeros = np.zeros((bottom_edge-border_size, width), dtype = image.dtype)\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - (border_size+right_edge):width])\n",
        "    # right_zeros = np.zeros((top_bottom_mirrored.shape[0], right_edge-border_size), dtype = image.dtype)\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path):\n",
        "    # window_size = 512\n",
        "    # oh, ow = 50, 50\n",
        "    window_size = 224\n",
        "    oh, ow = 20, 20\n",
        "\n",
        "    # out_crop =\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    # bottom_edge = (image_height + oh) % (window_size - oh)\n",
        "    # right_edge = (image_height + ow) % (window_size - ow)\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    # print(mirrored_image.shape)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    # tryout = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "            # print(x, y)\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    # if original_height != window_size:\n",
        "                    weighting_window = top_left\n",
        "                    # print('top left')\n",
        "                elif y == image_width - window_size:\n",
        "                    # print('top right')\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "                    # print('top ')\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                    # print('bottom left')\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                    # print('bottom right')\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "                    # print('bottom')\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "                # print('left')\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "                # print('right')\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "                # print('middle')\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "            square_section = normalize_hist(square_section)\n",
        "            square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch and channel dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            # output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # cv2_imshow(output_pil)\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "            # with torch.no_grad():\n",
        "            #     output = torch.sigmoid(model(square_tensor))\n",
        "            #     output = (output > 0.5).float()\n",
        "            #     output = output.cpu().data.numpy().squeeze(0).squeeze()\n",
        "            #     output = output*255.0\n",
        "            #     # cv2_imshow(output)\n",
        "            # # # Forward pass through the model\n",
        "            # # with torch.no_grad():\n",
        "            # #     output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # # Scale the probablity to 0-255\n",
        "\n",
        "            # # output = output.to(torch.uint8)\n",
        "            # # output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # # cv2_imshow(output_pil)\n",
        "            # output_probs[x:x+window_size, y:y+window_size] += output*weighting_window\n",
        "    # Crop\n",
        "    # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    # weights = weights[:original_height, :original_width]*255\n",
        "    # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # Apply weights\n",
        "    # output_probs /= weights\n",
        "\n",
        "    # Create image from mask\n",
        "    # output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    threshold = int(255*0.4)\n",
        "    output_mask = np.where(output_probs > threshold, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    return output_mask\n",
        "\n",
        "    # filename_ext = os.path.basename(image_path)\n",
        "    # filename, ext = os.path.splitext(filename_ext)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_mirrored\"+ext), mirrored_image)\n",
        "\n",
        "    # # Merge image with created mask\n",
        "    # out_mask_path = os.path.join(output_folder, filename+\"_new_mask\"+ext)\n",
        "    # merge = merge_images(input_image, output_mask)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "    # cv.imwrite(out_mask_path, output_mask)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "    # return out_mask_path\n",
        "\n",
        "# def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "#     window_size = 512\n",
        "#     oh, ow = 124, 124\n",
        "#     input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#     image_height, image_width = input_image.shape\n",
        "#     original_height, original_width = image_height, image_width\n",
        "#     bottom_edge = image_height % (window_size - oh)\n",
        "#     right_edge = image_width % (window_size - ow)\n",
        "#     mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "#     mirrored_image[:image_height, :image_width] = input_image\n",
        "#     mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "#     mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "#     image_height += bottom_edge\n",
        "#     image_width += right_edge\n",
        "#     weights = np.zeros((image_height, image_width))\n",
        "#     # tryout = np.zeros((image_height, image_width))\n",
        "#     output_probs = np.zeros((image_height, image_width))\n",
        "#     output_mask = np.zeros((image_height, image_width))\n",
        "#     middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "#     for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "#         for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "#             # Choose weighting window\n",
        "#             if x == 0:\n",
        "#                 if y == 0:\n",
        "#                     if original_height != window_size:\n",
        "#                         weighting_window = top_left\n",
        "#                     else:\n",
        "#                         weighting_window = np.ones((window_size, window_size))\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = top_right\n",
        "#                 else:\n",
        "#                     weighting_window = top\n",
        "#             elif x == window_size - oh - 1:\n",
        "#                 if y == 0:\n",
        "#                     weighting_window = bottom_left\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = bottom_right\n",
        "#                 else:\n",
        "#                     weighting_window = bottom\n",
        "#             elif y == 0:\n",
        "#                 weighting_window = left\n",
        "#             elif y == window_size - ow - 1:\n",
        "#                 weighting_window = right\n",
        "#             else:\n",
        "#                 weighting_window = middle\n",
        "#             square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "#             weights[x:x + window_size, y:y + window_size] = weighting_window\n",
        "#             # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "#             square_section = preprocess_image(square_section)\n",
        "#             square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "#             # Forward pass through the model\n",
        "#             with torch.no_grad():\n",
        "#                 output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "#             # Scale the probablity to 0-255\n",
        "#             output = output*255\n",
        "#             output = output.to(torch.uint8)\n",
        "#             output_pil = output.squeeze(0).cpu().numpy()\n",
        "#             output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()*weighting_window\n",
        "#     # Crop\n",
        "#     output_probs = output_probs[:original_height, :original_width]\n",
        "#     # weights = weights[:original_height, :original_width]*255\n",
        "#     # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "#     # Apply weights\n",
        "#     # output_probs /= weights\n",
        "\n",
        "#     # Create image from mask\n",
        "#     output_mask = np.where(output_probs > 127, 255, 0)\n",
        "#     output_mask = output_mask.astype(np.uint8)\n",
        "#     filename_ext = os.path.basename(image_path)\n",
        "#     filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "#     # Merge image with created mask\n",
        "#     out_mask_path = os.path.join(output_folder, filename+\"_mask\"+ext)\n",
        "#     merge = merge_images(input_image, output_mask)\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_merge\"+ext), merge)\n",
        "\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "#     cv.imwrite(out_mask_path, output_mask)\n",
        "#     # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "#     return out_mask_path\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = normalize_hist(image)\n",
        "    # image = nlm_filt(image)\n",
        "    # image = wavelet_denoise(image, threshold=1.5)\n",
        "    # image = apply_clahe(image)\n",
        "    # image = cv.medianBlur(image, 5)\n",
        "    return image\n",
        "\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_train_val_patches(train_image_folder, train_mask_folder, val_image_folder, val_mask_folder, output_folder, patch_size):\n",
        "    train_image_patches_path, train_mask_patches_path = create_image_patches(train_image_folder, train_mask_folder, output_folder, patch_size, img_type='train')\n",
        "    val_image_patches_path, val_mask_patches_path = create_image_patches(val_image_folder, val_mask_folder, output_folder, patch_size, img_type='val')\n",
        "    return train_image_patches_path, train_mask_patches_path, val_image_patches_path, val_mask_patches_path\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size, img_type):\n",
        "    image_patches_path = os.path.join(output_folder, img_type+'_image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, img_type+'_mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    fenestration_area_thresh = 0.0 #0.01\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        # if image_name.endswith(\".tif\"): # TODO: tohle mozna odstranit\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "        # strides = (patch_size * width , patch_size)\n",
        "\n",
        "        # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "        #              strides=img.strides + img.strides, writeable=False)\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "        k = 0\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                if k % 2 == 0:\n",
        "                    img_patch = img_strided[i, j]\n",
        "                    mask_patch = mask_strided[i, j]\n",
        "                    # Compute the percentage of white pixels\n",
        "                    fenestration_area = np.sum(mask_patch == 255)\n",
        "                    # print(fenestration_area)\n",
        "                    # fenestration_percentage = fenestration_area/patch_area\n",
        "                    if fenestration_area >= fenestration_area_thresh:\n",
        "                        patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                        # preprocess image\n",
        "                        img_patch = preprocess_image(img_patch)\n",
        "                        cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                        cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                        # print(\"written patch \", patch_filename)\n",
        "                    else:\n",
        "                        print(\"not writing patch\")\n",
        "                k += 1\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "# This is the official implementation of BoundaryDOULoss https://arxiv.org/pdf/2308.00220.pdf\n",
        "# Taken from: https://github.com/sunfan-bvb/BoundaryDoULoss/tree/main\n",
        "class BoundaryDoULoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BoundaryDoULoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i\n",
        "            tensor_list.append(temp_prob.unsqueeze(1))\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _adaptive_size(self, score, target):\n",
        "        kernel = torch.Tensor([[0,1,0], [1,1,1], [0,1,0]])\n",
        "        padding_out = torch.zeros((target.shape[0], target.shape[-2]+2, target.shape[-1]+2))\n",
        "        padding_out[:, 1:-1, 1:-1] = target\n",
        "        h, w = 3, 3\n",
        "\n",
        "        Y = torch.zeros((padding_out.shape[0], padding_out.shape[1] - h + 1, padding_out.shape[2] - w + 1)).cuda()\n",
        "        for i in range(Y.shape[0]):\n",
        "            Y[i, :, :] = torch.conv2d(target[i].unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0).cuda(), padding=1)\n",
        "        Y = Y * target\n",
        "        Y[Y == 5] = 0\n",
        "        C = torch.count_nonzero(Y)\n",
        "        S = torch.count_nonzero(target)\n",
        "        smooth = 1e-5\n",
        "        alpha = 1 - (C + smooth) / (S + smooth)\n",
        "        alpha = 2 * alpha - 1\n",
        "\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        alpha = min(alpha, 0.8)  ## We recommend using a truncated alpha of 0.8, as using truncation gives better results on some datasets and has rarely effect on others.\n",
        "        loss = (z_sum + y_sum - 2 * intersect + smooth) / (z_sum + y_sum - (1 + alpha) * intersect + smooth)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "        target = target.squeeze(1)\n",
        "\n",
        "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            loss += self._adaptive_size(inputs[:, 0], target[:, 0])#(inputs[:, i], target[:, i])\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_dice_score = 0.0\n",
        "    total_samples = 0\n",
        "    eps = 1e-8\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            out = model(x)\n",
        "            loss = get_loss(out, y, loss_fn)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            if WANDB_CONNECTED or WANDB_LOG:\n",
        "                wandb.log({\"val/batch loss\": loss.item()})\n",
        "\n",
        "            predicted_probs = torch.sigmoid(out)\n",
        "            predicted = (predicted_probs > 0.5).float()\n",
        "            intersection = torch.sum(predicted * y)\n",
        "            dice_score = (2.0 * intersection + eps) / (torch.sum(predicted) + torch.sum(y) + eps)\n",
        "            total_dice_score += dice_score.item() * x.size(0)\n",
        "\n",
        "            total_samples += x.size(0)\n",
        "    model.train()\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    average_dice_score = total_dice_score / total_samples\n",
        "\n",
        "    return average_loss, average_dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def validate_model(model, loader, loss_fn):\n",
        "#     num_correct = 0\n",
        "#     num_pixels = 0\n",
        "#     dice_score = 0\n",
        "#     model.eval()\n",
        "#     running_loss = 0\n",
        "#     losses = []\n",
        "#     dice_scores = []\n",
        "#     with torch.no_grad():\n",
        "#         for idx, (x, y) in enumerate(loader):\n",
        "#             x = x.to(DEVICE)\n",
        "#             y = y.to(DEVICE).unsqueeze(1)\n",
        "#             # Forward\n",
        "#             preds = model(x)\n",
        "#             loss = get_loss(preds, y, loss_fn)\n",
        "#             # running_loss += loss.cpu()\n",
        "#             losses.append(loss.cpu())\n",
        "\n",
        "#             preds = torch.sigmoid(preds)\n",
        "#             preds = (preds > 0.5).float()\n",
        "\n",
        "#             # num_correct += (preds == y).sum()\n",
        "#             # num_pixels += torch.numel(preds)\n",
        "#             dice_score += (2*(preds*y).sum()) / ((preds+y).sum() + 1e-8) # this is a better predictor\n",
        "#             dice_scores.append(dice_score.cpu())\n",
        "#     # print(\n",
        "#     #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "#     # )\n",
        "#     # dice_score = dice_score/(idx+1)\n",
        "#     # val_loss = running_loss/(idx+1)\n",
        "#     val_loss = np.mean(np.array(losses))\n",
        "#     dice_score = np.mean(np.array(dice_scores))\n",
        "\n",
        "#     # dice_score = dice_score/len(loader)\n",
        "#     # val_loss = running_loss/len(loader) #TODO: not sure this is correct(dividing by batch size?)\n",
        "#     # print(f\"Dice score is {dice_score}\")\n",
        "#     # val_losses.append(running_loss/len(loader))\n",
        "#     # dice_scores.append(dice_score.cpu())\n",
        "#     model.train()\n",
        "#     return val_loss, dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# def getClassWeights(mask_path, train_indices):\n",
        "#     mask_dir_list = sorted(os.listdir(mask_path))\n",
        "#     class_count = np.zeros(2, dtype=int)\n",
        "#     for i in train_indices:\n",
        "#         mask = cv.imread(os.path.join(mask_path, mask_dir_list[i]), cv.IMREAD_GRAYSCALE) #np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "#         mask[mask == 255.0] = 1\n",
        "#         class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "#         class_count[1] += mask.sum()\n",
        "\n",
        "#     n_samples = class_count.sum()\n",
        "#     n_classes = 2\n",
        "\n",
        "#     class_weights = n_samples / (n_classes * class_count)\n",
        "#     return torch.from_numpy(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    num_ellipses = 0\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "                num_ellipses += 1\n",
        "            else:\n",
        "                ellipses.append((None, None, None))\n",
        "        else:\n",
        "            ellipses.append((None, None, None))\n",
        "    # print(len(filtered_contours), len(ellipses))\n",
        "    return ellipses, num_ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "    # image = cv.cvtColor(seg_mask, cv.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    # min_contour_area = 10\n",
        "    # filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
        "    # return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    # print(contour_centers)\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01_original_mask.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "fenestration_areas_from_ellipses = []\n",
        "\n",
        "# for ellipse in ellipses:\n",
        "#     print(ellipse)\n",
        "#     center, axes, angle = ellipse\n",
        "#     # center_x, center_y = center\n",
        "#     major_axis_length, minor_axis_length = axes\n",
        "#     roundness = minor_axis_length/major_axis_length\n",
        "#     roundness_of_ellipses.append(roundness)\n",
        "#     # rotation_angle = angle\n",
        "#     diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "#     equivalent_diameters.append(diameter)\n",
        "#     fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "\n",
        "# show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ],
      "metadata": {
        "id": "BtPrBpQBcsmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "583d7542-2cca-48d4-926a-8825a2660e5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fenestrations: 0\n",
            "Number of fitted ellipses: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Wavelet filtering debug\n",
        "\n",
        "# image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "# images = os.listdir(image_folder)\n",
        "# image_name = images[0]\n",
        "# image = cv.imread(os.path.join(image_folder, image_name), cv.IMREAD_GRAYSCALE)\n",
        "# # cv2_imshow(image)\n",
        "\n",
        "# denoised_image = wavelet_denoise(image)\n",
        "# # cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch, activation):\n",
        "    if activation == 'ReLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    elif activation == 'GeLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none'),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none')\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device, dropout_probability, activations, out_activation):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(p=dropout_probability)\n",
        "        self.activations = activations\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64, activation=activations)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128, activation=activations)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256, activation=activations)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512, activation=activations)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024, activation=activations)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512, activation=activations)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256, activation=activations)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128, activation=activations)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64, activation=activations)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "        self.out_activation = out_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        p2 = self.dropout(p2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        p3 = self.dropout(p3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        p4 = self.dropout(p4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        cat1 = self.dropout(cat1)\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        cat2 = self.dropout(cat2)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        if self.out_activation == 'sigmoid':\n",
        "            conv_1x1 = torch.sigmoid(conv_1x1)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Create training patches**"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "#@markdown If you want to create new 512x512 patches, check the following box. If you already have image patches, insert the folders below.\n",
        "\n",
        "\n",
        "# training_images = './gdrive/MyDrive/lsecs/masks_added_small/train_images' #@param {type:\"string\"}\n",
        "# training_masks = './gdrive/MyDrive/lsecs/masks_added_small/train_masks' #@param {type:\"string\"}\n",
        "# validation_images = './gdrive/MyDrive/lsecs/masks_added_small/val_images' #@param {type:\"string\"}\n",
        "# validation_masks = './gdrive/MyDrive/lsecs/masks_added_small/val_masks' #@param {type:\"string\"}\n",
        "training_images = './gdrive/MyDrive/lsecs/cropped_cells/patches/train_image_patches' #@param {type:\"string\"}\n",
        "training_masks = './gdrive/MyDrive/lsecs/cropped_cells/patches/train_mask_patches' #@param {type:\"string\"}\n",
        "validation_images = './gdrive/MyDrive/lsecs/cropped_cells/patches/val_image_patches' #@param {type:\"string\"}\n",
        "validation_masks = './gdrive/MyDrive/lsecs/cropped_cells/patches/val_mask_patches' #@param {type:\"string\"}\n",
        "\n",
        "create_patches = True # @param {type:\"boolean\"}\n",
        "output_patches_folder = './gdrive/MyDrive/lsecs/masks_added_small/patches' #@param {type:\"string\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "training_masks = training_masks.strip()\n",
        "validation_images = validation_images.strip()\n",
        "validation_masks = validation_masks.strip()\n",
        "\n",
        "output_patches_folder = output_patches_folder.strip()\n",
        "\n",
        "if not os.path.exists(training_images):\n",
        "    print(f'{training_images} does not exist.')\n",
        "if not os.path.exists(training_masks):\n",
        "    print(f'{training_masks} does not exist.')\n",
        "if not os.path.exists(validation_images):\n",
        "    print(f'{validation_images} does not exist.')\n",
        "if not os.path.exists(validation_masks):\n",
        "    print(f'{validation_masks} does not exist.')\n"
      ],
      "metadata": {
        "id": "4qR6zmj4U-pC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369e31bf-eb74-470f-e0c8-9304fed757c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./gdrive/MyDrive/lsecs/masks_added_small/train_images does not exist.\n",
            "./gdrive/MyDrive/lsecs/masks_added_small/val_images does not exist.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATCHES_TO_DISK = True\n",
        "patch_size = 224\n",
        "\n",
        "if create_patches:\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        # output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "        print(f'Saving patches to {output_patches_folder}')\n",
        "    else:\n",
        "        output_patches_folder = os.getcwd()\n",
        "    train_img_patches_path, train_mask_patches_path, val_img_patches_path, val_mask_patches_path = create_train_val_patches(training_images, training_masks, validation_images, validation_masks, output_patches_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    train_img_patches_path = training_images\n",
        "    train_mask_patches_path = training_masks\n",
        "    val_img_patches_path = validation_images\n",
        "    val_mask_patches_path = validation_masks\n",
        "    # output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "    # image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    # mask_patches_path = os.path.join(output_folder, 'mask_patches')\n",
        "\n",
        "print(f'Training image patches are located in {train_img_patches_path}, {len(os.listdir(train_img_patches_path))} patches.')\n",
        "print(f'Training mask patches are located in {train_mask_patches_path}')\n",
        "print(f'Validation image patches are located in {val_img_patches_path}, {len(os.listdir(val_img_patches_path))} patches.')\n",
        "print(f'Validation mask patches are located in {val_mask_patches_path}')\n",
        "\n",
        "# print(len(os.listdir(train_img_patches_path)))\n",
        "# print(len(os.listdir(val_img_patches_path)))"
      ],
      "metadata": {
        "id": "UzznzOTP4s53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f3daa4f-89cf-4c88-acf0-8dc9ba61d3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving patches to ./gdrive/MyDrive/lsecs/masks_added_small/patches\n",
            "Training image patches are located in ./gdrive/MyDrive/lsecs/masks_added_small/patches/train_image_patches, 1334 patches.\n",
            "Training mask patches are located in ./gdrive/MyDrive/lsecs/masks_added_small/patches/train_mask_patches\n",
            "Validation image patches are located in ./gdrive/MyDrive/lsecs/masks_added_small/patches/val_image_patches, 472 patches.\n",
            "Validation mask patches are located in ./gdrive/MyDrive/lsecs/masks_added_small/patches/val_mask_patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # reduce the number of patches\n",
        "# import shutil\n",
        "# train_im = [f for f in sorted(os.listdir(train_img_patches_path)) if os.path.isfile(os.path.join(train_img_patches_path, f))]\n",
        "# train_mask = [f for f in sorted(os.listdir(train_mask_patches_path)) if os.path.isfile(os.path.join(train_mask_patches_path, f))]\n",
        "# val_im = [f for f in sorted(os.listdir(val_img_patches_path)) if os.path.isfile(os.path.join(val_img_patches_path, f))]\n",
        "# val_mask = [f for f in sorted(os.listdir(val_mask_patches_path)) if os.path.isfile(os.path.join(val_mask_patches_path, f))]\n",
        "\n",
        "# new_train_img_patches_path = train_img_patches_path + '_reduced'\n",
        "# new_train_mask_patches_path = train_mask_patches_path + '_reduced'\n",
        "# new_val_img_patches_path = val_img_patches_path + '_reduced'\n",
        "# new_val_mask_patches_path = val_mask_patches_path + '_reduced'\n",
        "\n",
        "# if not os.path.exists(new_train_img_patches_path):\n",
        "#     os.makedirs(new_train_img_patches_path)\n",
        "# if not os.path.exists(new_train_mask_patches_path):\n",
        "#     os.makedirs(new_train_mask_patches_path)\n",
        "# if not os.path.exists(new_val_img_patches_path):\n",
        "#     os.makedirs(new_val_img_patches_path)\n",
        "# if not os.path.exists(new_val_mask_patches_path):\n",
        "#     os.makedirs(new_val_mask_patches_path)\n",
        "\n",
        "# for i in range(len(train_im)):\n",
        "#     if i % 4 == 0:\n",
        "#         shutil.copyfile(os.path.join(train_img_patches_path, train_im[i]), os.path.join(new_train_img_patches_path, train_im[i]))\n",
        "#         shutil.copyfile(os.path.join(train_mask_patches_path, train_mask[i]), os.path.join(new_train_mask_patches_path, train_mask[i]))\n",
        "# for i in range(len(val_im)):\n",
        "#     if i % 4 == 0:\n",
        "#         shutil.copyfile(os.path.join(val_img_patches_path, val_im[i]), os.path.join(new_val_img_patches_path, val_im[i]))\n",
        "#         shutil.copyfile(os.path.join(val_mask_patches_path, val_mask[i]), os.path.join(new_val_mask_patches_path, val_mask[i]))\n",
        "\n",
        "# train_img_patches_path = new_train_img_patches_path\n",
        "# train_mask_patches_path = new_train_mask_patches_path\n",
        "# val_img_patches_path = new_val_img_patches_path\n",
        "# val_mask_patches_path = new_val_mask_patches_path\n",
        "\n",
        "# print(f'Training image patches are located in {train_img_patches_path}, {len(os.listdir(train_img_patches_path))} patches.')\n",
        "# print(f'Training mask patches are located in {train_mask_patches_path}')\n",
        "# print(f'Validation image patches are located in {val_img_patches_path}, {len(os.listdir(val_img_patches_path))} patches.')\n",
        "# print(f'Validation mask patches are located in {val_mask_patches_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfxmNFcN5EjJ",
        "outputId": "c2676415-ac35-4c02-d0be-0d8e036042aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training image patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5_224/train_image_patches_reduced, 666 patches.\n",
            "Training mask patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5_224/train_mask_patches_reduced\n",
            "Validation image patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5_224/val_image_patches_reduced, 236 patches.\n",
            "Validation mask patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5_224/val_mask_patches_reduced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "WAJp45Xo8p_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def build_optimizer(model, config, beta1=None, beta2=None):\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=config.learning_rate,\n",
        "                              weight_decay=config.weight_decay,\n",
        "                              momentum=config.momentum)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=config.learning_rate,\n",
        "                            #    betas=(config.beta1, config.beta2),\n",
        "                               weight_decay=config.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "# TRAIN_LOADER = train_loader\n",
        "# VAL_LOADER = val_loader\n",
        "def build_dataloaders(config): # TODO: check if there is a better way to do this\n",
        "    train_image_patches_path = config.train_image_patches_path\n",
        "    train_mask_patches_path = config.train_mask_patches_path\n",
        "    val_image_patches_path = config.val_image_patches_path\n",
        "    val_mask_patches_path = config.val_mask_patches_path\n",
        "    # image_patches_path = os.path.join(image_patches_path, 'image_patches')\n",
        "    # mask_patches_path = os.path.join(mask_patches_path, 'mask_patches')\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        train_image_patches_path,\n",
        "        train_mask_patches_path,\n",
        "        val_image_patches_path,\n",
        "        val_mask_patches_path,\n",
        "        config.batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return train_loader, val_loader # this is the simplest way to do it, wandb train cannot take any arguments\n",
        "\n",
        "class EarlyStopper():\n",
        "    def __init__(self, patience):\n",
        "        self.patience = patience\n",
        "        # self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > self.min_validation_loss:\n",
        "            self.counter += 1\n",
        "            print(self.counter)\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "    losses = []\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = get_loss(predictions, targets, loss_fn)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # running_loss += loss.item()\n",
        "        # losses.append(loss.item())\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "        if WANDB_CONNECTED or WANDB_LOG:\n",
        "            wandb.log({\"train/batch loss\": loss.item()})\n",
        "\n",
        "    # number_of_batches = batch_idx+1\n",
        "    # mean_loss = np.mean(np.array(losses))\n",
        "    mean_loss = total_loss / total_samples\n",
        "    model.eval()\n",
        "    return mean_loss\n",
        "    # return running_loss/number_of_batches\n",
        "\n",
        "def build_model(model_name):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    if '+' in model_name:\n",
        "        name_parts = model_name.split('+')\n",
        "        encoder = name_parts[-2]\n",
        "        if name_parts[-1] == 'imagenet' or name_parts[-1] == 'ssl':\n",
        "            weights = name_parts[-1]\n",
        "        else:\n",
        "            weights = None\n",
        "    out_activation = None\n",
        "    # print(weights)\n",
        "    # print(out_activation)\n",
        "    if model_name == 'plain_unet':\n",
        "        model = UNET(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                device=DEVICE,\n",
        "                dropout_probability=0,\n",
        "                activations='ReLU',\n",
        "                out_activation=out_activation).to(DEVICE)\n",
        "    elif 'Unet++' in model_name:\n",
        "        model = smp.UnetPlusPlus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'Linknet' in model_name:\n",
        "        model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'FPN' in model_name:\n",
        "        model = smp.FPN(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'DeepLabV3' in model_name:\n",
        "        model = smp.DeepLabV3(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    else:\n",
        "        model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def get_loss(pred, target, func_name):\n",
        "    loss_func = None\n",
        "    if func_name == 'dice':\n",
        "        loss_func = smp.losses.DiceLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'bcelog':\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'jaccard':\n",
        "        loss_func = smp.losses.JaccardLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'weighted_bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'focal':\n",
        "        loss_func = smp.losses.FocalLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'dice+bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == '5dice+95bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.05*loss1 + 0.95*loss2\n",
        "    elif func_name == '20dice+80bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.2*loss1 + 0.8*loss2\n",
        "    elif func_name == 'dice+focal':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = smp.losses.FocalLoss(mode='binary')\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == 'tversky':\n",
        "        loss_func = smp.losses.TverskyLoss(mode='binary', alpha=0.7, beta=0.3)\n",
        "        loss = loss_func(pred, target)\n",
        "    # elif func_name == 'hausdorff':\n",
        "\n",
        "    return loss\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, val_loader = build_dataloaders(config)\n",
        "        model = build_model(config.model_type)\n",
        "        optimizer = build_optimizer(model, config)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "        # best_dice_score = 0\n",
        "        smallest_val_loss = 1000.0\n",
        "        early_stopper = EarlyStopper(patience=10)\n",
        "        for epoch in range(config.epochs):\n",
        "            print(f'Epoch {epoch}')\n",
        "            avg_loss = train_epoch(model, train_loader, optimizer, config.loss_function)#, loss_fn)\n",
        "            metrics = {\"train/loss\": avg_loss, \"train/epoch\": epoch}\n",
        "            val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if early_stopper.early_stop(val_loss):\n",
        "                print(f\"early stop on epoch {epoch}\")\n",
        "                with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                    file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "                break\n",
        "\n",
        "            if val_loss < smallest_val_loss:\n",
        "                torch.save(model.state_dict(), os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "            smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "            val_metrics = {\"val/val_loss\": val_loss,\n",
        "                           \"val/dice_score\": dice_score}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "class DictObject:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "def train(config, model_out_path):\n",
        "    if WANDB_LOG:\n",
        "        wandb.init(\n",
        "            project=\"LSEC_segmentation\",\n",
        "            config=config)\n",
        "        config = wandb.config\n",
        "    else:\n",
        "        config = DictObject(**config)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "    model = build_model(config.model_type)\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    smallest_val_loss = 1000.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "\n",
        "    early_stopper = EarlyStopper(patience=10)\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.loss_function)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(\"Current learning rate:\", current_lr)\n",
        "\n",
        "        if early_stopper.early_stop(val_loss):\n",
        "            print(f\"early stop on epoch {epoch}\")\n",
        "            with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "            break\n",
        "        if val_loss < smallest_val_loss:\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "        smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "        dice_scores.append(dice_score)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Dice score: {dice_score}')\n",
        "        # view_prediction(val_loader, model, device = DEVICE)\n",
        "        print(train_loss, val_loss)\n",
        "        if WANDB_LOG:\n",
        "            wandb.log({\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": epoch,\n",
        "                       \"val/val_loss\": val_loss,\n",
        "                       \"val/dice_score\":dice_score,\n",
        "                       })\n",
        "    if WANDB_LOG:\n",
        "        wandb.finish()\n",
        "\n",
        "    return train_losses, val_losses, dice_scores"
      ],
      "metadata": {
        "id": "c6sxUMdmjwo6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_images = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# train_masks = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# val_images = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# val_masks = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "\n",
        "# wandb sweep config\n",
        "sweep_config = {\n",
        "    'method': 'grid'#'grid'#\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'val/dice_score',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        # 'values': ['adam', 'sgd']\n",
        "        'value': 'sgd'\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'value': 0.04,\n",
        "        # a flat distribution between min and max\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.001,\n",
        "        # 'max': 0.01\n",
        "      },\n",
        "    'weight_decay': {\n",
        "        # 'value': 0.0189,\n",
        "        'value': 0.01\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max' : 0.02,\n",
        "    },\n",
        "    # sgd parameters\n",
        "    'momentum':{\n",
        "        'value': 0.07,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.06,\n",
        "        # 'max' : 0.09,\n",
        "    },\n",
        "\n",
        "    # 'dropout': {\n",
        "    #     'value': 0.5,\n",
        "    #     #   'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "    #     },\n",
        "    'epochs': {\n",
        "        'value': 200,\n",
        "        },\n",
        "\n",
        "    # Dataloader params\n",
        "    'train_image_patches_path': {\n",
        "        'value': train_img_patches_path\n",
        "        },\n",
        "    'train_mask_patches_path': {\n",
        "        'value': train_mask_patches_path\n",
        "        },\n",
        "    'val_image_patches_path': {\n",
        "        'value': val_img_patches_path\n",
        "        },\n",
        "    'val_mask_patches_path': {\n",
        "        'value': val_mask_patches_path\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'value': 32,\n",
        "        # # integers between min and max\n",
        "        # # with evenly-distributed logarithms\n",
        "        # 'distribution': 'q_log_uniform_values',\n",
        "        # 'q': 2, # the discrete step of the distribution\n",
        "        # 'min': 4,\n",
        "        # 'max': 8,\n",
        "      },\n",
        "    # Adam parameters\n",
        "    # 'beta1': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "    # 'beta2': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "        # 'fc_layer_size': {\n",
        "    #     'values': [128, 256, 512]\n",
        "    #     },\n",
        "    'image_denoising_methods': {\n",
        "        'value': '11_10_indiv',\n",
        "        # 'values': ['no_denoise', 'med5']\n",
        "        # 'values': ['nlm', 'med5']\n",
        "        # 'values': ['clahe+median5', 'med7', 'median5', 'median5+clahe', 'wave1_5+med3', 'wave2_5', 'wave2_5+med5'],#['wavelet', 'wavelet+median', 'advanced median'] # k waveletu jeste pridat ruzne thresholdy\n",
        "    },\n",
        "    'loss_function':{\n",
        "        # 'value': 'bcelog',\n",
        "        # 'values': ['dice', 'dice+bce', 'dice+focal', 'tversky'],#['dice', 'bcelog', 'jaccard', 'weighted_bce', 'focal'],#, 'tversky', 'hausdorff']\n",
        "        # 'values': ['dice', 'dice+bce', 'focal','bcelog', 'dice+focal'],\n",
        "        'values': ['bcelog'],\n",
        "\n",
        "\n",
        "    },\n",
        "    'model_type':{\n",
        "        # 'values': ['plain_unet', 'resnet34+imagenet', 'resnet50+imagenet', 'inceptionv4+imagenet', 'efficientnet-b7+imagenet', 'resnet18+swsl', 'resnet18+imagenet','vgg11+imagenet'], # not great\n",
        "        # 'values': ['vgg11+imagenet', 'vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl','resnet34+imagenet','resnet50+ssl', 'resnext50_32x4d+ssl'], # good\n",
        "        # 'values': ['vgg11+imagenet','vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl',  'resnet34+imagenet','resnet50+ssl', 'efficientnet-b7+imagenet'], # the best so far\n",
        "        # 'value': 'vgg11+imagenet',\n",
        "        # 'values':['vgg11+imagenet','vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', 'efficientnet-b7+imagenet'],\n",
        "        # 'values':['vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', [\n",
        "        'values': ['resnet34+imagenet']\n",
        "\n",
        "    },\n",
        "    'model_path':{\n",
        "        'value': './gdrive/MyDrive/lsecs',\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LSEC_segmentation\")"
      ],
      "metadata": {
        "id": "Y851Q6gvcd8h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "157df277-43d5-4653-904a-e8c0a3ea0d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "wandb: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: d6svk90u\n",
            "Sweep URL: https://wandb.ai/dpd/LSEC_segmentation/sweeps/d6svk90u\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_CONNECTED = True\n",
        "wandb.agent(sweep_id, wandb_train, count=510)"
      ],
      "metadata": {
        "id": "EXcfFX2wo9P7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d0e19f028b2745698adb0e8c28e58e8c",
            "459e80e05d954faeb9c63d2dd2e0b74d",
            "66498da59bb440aca1a2b6e50567650a",
            "6b9d67f359504d20bd5e391d36d78810",
            "28ce302d13844beaa3354f1fee187ee0",
            "dc8392879dc543f68f91027e54011f52",
            "794263da4acb47478112123408e0ba57",
            "99e519ec40714b229cb029bc9ab24c88"
          ]
        },
        "outputId": "afd6c39c-d22a-4094-ed1f-027f0a3a646b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Agent Starting Run: qd872sax with config:\n",
            "wandb: \tbatch_size: 32\n",
            "wandb: \tepochs: 200\n",
            "wandb: \timage_denoising_methods: 11_10_indiv\n",
            "wandb: \tlearning_rate: 0.04\n",
            "wandb: \tloss_function: bcelog\n",
            "wandb: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "wandb: \tmodel_type: resnet34+imagenet\n",
            "wandb: \tmomentum: 0.07\n",
            "wandb: \toptimizer: sgd\n",
            "wandb: \ttrain_image_patches_path: ./gdrive/MyDrive/lsecs/masks_added_small/patches/train_image_patches\n",
            "wandb: \ttrain_mask_patches_path: ./gdrive/MyDrive/lsecs/masks_added_small/patches/train_mask_patches\n",
            "wandb: \tval_image_patches_path: ./gdrive/MyDrive/lsecs/masks_added_small/patches/val_image_patches\n",
            "wandb: \tval_mask_patches_path: ./gdrive/MyDrive/lsecs/masks_added_small/patches/val_mask_patches\n",
            "wandb: \tweight_decay: 0.01\n",
            "wandb: Currently logged in as: marketakvasova1 (dpd). Use `wandb login --relogin` to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.17.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240510_163806-qd872sax</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/qd872sax' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/d6svk90u' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/d6svk90u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/d6svk90u' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/d6svk90u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/qd872sax' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/qd872sax</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\n",
            "100%|██████████| 83.3M/83.3M [00:00<00:00, 231MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "Epoch 21\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "1\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "Epoch 27\n",
            "Epoch 28\n",
            "Epoch 29\n",
            "1\n",
            "Epoch 30\n",
            "Epoch 31\n",
            "Epoch 32\n",
            "1\n",
            "Epoch 33\n",
            "2\n",
            "Epoch 34\n",
            "Epoch 35\n",
            "Epoch 36\n",
            "1\n",
            "Epoch 37\n",
            "2\n",
            "Epoch 38\n",
            "3\n",
            "Epoch 39\n",
            "Epoch 40\n",
            "Epoch 41\n",
            "Epoch 42\n",
            "Epoch 43\n",
            "1\n",
            "Epoch 44\n",
            "Epoch 45\n",
            "1\n",
            "Epoch 46\n",
            "Epoch 47\n",
            "1\n",
            "Epoch 48\n",
            "Epoch 49\n",
            "Epoch 50\n",
            "Epoch 51\n",
            "1\n",
            "Epoch 52\n",
            "2\n",
            "Epoch 53\n",
            "3\n",
            "Epoch 54\n",
            "4\n",
            "Epoch 55\n",
            "5\n",
            "Epoch 56\n",
            "6\n",
            "Epoch 57\n",
            "Epoch 58\n",
            "Epoch 59\n",
            "Epoch 60\n",
            "1\n",
            "Epoch 61\n",
            "2\n",
            "Epoch 62\n",
            "3\n",
            "Epoch 63\n",
            "4\n",
            "Epoch 64\n",
            "5\n",
            "Epoch 65\n",
            "Epoch 66\n",
            "1\n",
            "Epoch 67\n",
            "2\n",
            "Epoch 68\n",
            "3\n",
            "Epoch 69\n",
            "Epoch 70\n",
            "1\n",
            "Epoch 71\n",
            "2\n",
            "Epoch 72\n",
            "3\n",
            "Epoch 73\n",
            "Epoch 74\n",
            "1\n",
            "Epoch 75\n",
            "2\n",
            "Epoch 76\n",
            "3\n",
            "Epoch 77\n",
            "4\n",
            "Epoch 78\n",
            "Epoch 79\n",
            "Epoch 80\n",
            "1\n",
            "Epoch 81\n",
            "Epoch 82\n",
            "Epoch 83\n",
            "1\n",
            "Epoch 84\n",
            "2\n",
            "Epoch 85\n",
            "3\n",
            "Epoch 86\n",
            "4\n",
            "Epoch 87\n",
            "5\n",
            "Epoch 88\n",
            "Epoch 89\n",
            "Epoch 90\n",
            "1\n",
            "Epoch 91\n",
            "2\n",
            "Epoch 92\n",
            "3\n",
            "Epoch 93\n",
            "4\n",
            "Epoch 94\n",
            "5\n",
            "Epoch 95\n",
            "6\n",
            "Epoch 96\n",
            "7\n",
            "Epoch 97\n",
            "8\n",
            "Epoch 98\n",
            "9\n",
            "Epoch 99\n",
            "10\n",
            "early stop on epoch 99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0e19f028b2745698adb0e8c28e58e8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>█▅▃▂▂▃▂▂▂▂▂▃▂▃▁▁▂▂▂▂▁▁▂▁▁▁▂▁▂▂▁▂▁▂▂▁▁▁▁▂</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/batch loss</td><td>█▅▃▃▂▃▁▄▃▂▂▂▂▂▃▂▂▂▁▁▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▁▂</td></tr><tr><td>val/dice_score</td><td>▁▆▇█████████████████████████████████████</td></tr><tr><td>val/val_loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/batch loss</td><td>0.01965</td></tr><tr><td>train/epoch</td><td>98</td></tr><tr><td>train/loss</td><td>0.02393</td></tr><tr><td>val/batch loss</td><td>0.01276</td></tr><tr><td>val/dice_score</td><td>0.91417</td></tr><tr><td>val/val_loss</td><td>0.0262</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fast-sweep-1</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/qd872sax' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/qd872sax</a><br/> View project at: <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240510_163806-qd872sax/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wandb: Sweep Agent: Waiting for job.\n",
            "wandb: Sweep Agent: Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "KPwZ2wIG8htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = False\n",
        "WANDB_CONNECTED = True\n",
        "WANDB_LOG = True\n",
        "# image_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\" # + patches_image_denoising_methods\n",
        "# mask_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg11_dice+bce_no_pre_checkpoint.pth\")\n",
        "config = {\n",
        "    'batch_size' : 32,\n",
        "    'dropout' : 0.0,\n",
        "    'optimizer' : 'adam',\n",
        "    'num_epochs' : 200,\n",
        "    'learning_rate' : 0.02,\n",
        "    'weight_decay' : 0.01,\n",
        "    'momentum' : 0.07,\n",
        "    'train_image_patches_path': train_img_patches_path,\n",
        "    'train_mask_patches_path': train_mask_patches_path,\n",
        "    'val_image_patches_path': val_img_patches_path,\n",
        "    'val_mask_patches_path': val_mask_patches_path,\n",
        "    'image_denoising_methods': 'none',#'nlm'\n",
        "    'loss_function': 'dice+bce',\n",
        "    'model_type': 'vgg11+imagenet',\n",
        "    'model_path':'./gdrive/MyDrive/lsecs',\n",
        "}\n"
      ],
      "metadata": {
        "id": "-0Al6T1fdX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "    model = build_model(config['model_type'], config['dropout'], config['loss_function'])\n",
        "    model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activations = 'ReLU', out_activation=None).to(DEVICE)\n",
        "    load_state_dict(model, model_path)\n",
        "else:\n",
        "    # WANDB_LOG = False\n",
        "    train_losses, val_losses, dice_scores = train(config, model_path)"
      ],
      "metadata": {
        "id": "IaxEOPPRbMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En"
      },
      "outputs": [],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0c88f9-a6f2-4042-9de2-32a81584d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:27<00:00, 19.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05.tif\"\n",
        "test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "# model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activation=None).to(DEVICE)\n",
        "model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "out_mask_path = inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "merge_original_mask(test_image_path, test_mask_path, output_folder)\n",
        "merge_masks(out_mask_path, test_mask_path, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Inference loop**"
      ],
      "metadata": {
        "id": "ZiGwUlrqBx1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = preprocess_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "# class SigmoidWrapper(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super(SigmoidWrapper, self).__init__()\n",
        "#         self.model = model\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.model(x)\n",
        "#         x = self.sigmoid(x)\n",
        "#         return x\n",
        "\n",
        "# model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_checkpoint.pth\")\n",
        "# device = torch.device('cuda')\n",
        "# model = build_model('vgg13+none', 0.0, 'dice+bce')\n",
        "# if torch.cuda.is_available():\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "# model_with_sigmoid = SigmoidWrapper(model)\n",
        "# model_with_sigmoid.to(device=device)\n",
        "# wrapper = FunctionWrapper(model_with_sigmoid)\n",
        "# wrapper.to(device=device)\n",
        "# full_model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_sigmoid_checkpoint.pth\")\n",
        "# torch.save(wrapper, full_model_path)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)"
      ],
      "metadata": {
        "id": "XK2mzMWMK02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "input_images_folder = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "output_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "model_path = './gdrive/MyDrive/lsecs/resnet34+imagenet_bcelog_11_10_indiv.pth' #@param {type:\"string\"}\n",
        "\n",
        "input_images_folder = input_images_folder.strip()\n",
        "output_mask_folder = output_mask_folder.strip()\n",
        "model_path = model_path.strip()\n",
        "\n",
        "if not os.path.exists(output_mask_folder):\n",
        "    os.makedirs(output_mask_folder)\n",
        "if not os.path.exists(input_images_folder):\n",
        "    print(f'{input_images_folder} does not exist)')"
      ],
      "metadata": {
        "id": "9JALgAucLM_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13+imagenet_dice+bce_nlm.pth\")\n",
        "# # Inference on full images\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/sem_images\"\n",
        "# # # test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/new_masks\"\n",
        "\n",
        "\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/sem_images\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/zapotoczny_test/edited_masks\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/whole_cells/fen_mask\"\n",
        "\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/new_masks\"\n",
        "# # if not os.path.exists(output_folder):\n",
        "# #     os.makedirs(output_folder)\n",
        "\n",
        "# image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "# mask_names = [f for f in sorted(os.listdir(masks_path)) if os.path.isfile(os.path.join(masks_path, f))]\n",
        "\n",
        "# if len(image_names) != len(mask_names):\n",
        "#     print(f'There are {len(image_names)} images, but {len(mask_names)} masks.')\n",
        "# # print(image_names, mask_names)\n",
        "# # model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "# # if torch.cuda.is_available():\n",
        "# #     model.load_state_dict(torch.load(model_path))\n",
        "# # else:\n",
        "# #     model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "# model = torch.load(model_path)"
      ],
      "metadata": {
        "id": "5YvVBtOmB2aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image_name, mask_name in zip(image_names, mask_names):\n",
        "image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "# model = torch.load_state_dict(model_path)\n",
        "basename = os.path.basename(model_path).split('+')[0]\n",
        "# encoder = 'vgg11'\n",
        "# if 'vgg11' in basename:\n",
        "#     encoder='vgg11'\n",
        "encoder = basename\n",
        "model = build_model(encoder+'+none')\n",
        "loaded_state_dict = torch.load(model_path)\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "\n",
        "for image_name in image_names:\n",
        "    print(image_name)\n",
        "    # print(f'{image_name} - {mask_name}')\n",
        "    image_path = os.path.join(input_images_folder, image_name)\n",
        "    # mask_path = os.path.join(masks_path, mask_name)\n",
        "    filter = None\n",
        "    output_mask = inference_on_image_with_overlap(model, image_path)\n",
        "\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    out_mask_path = os.path.join(output_mask_folder, filename+\"_new_mask_resnet34\"+ext)\n",
        "    # Save created mask\n",
        "    cv.imwrite(out_mask_path, output_mask)\n",
        "    # # Merge image with created mask\n",
        "    merge = merge_images(cv.imread(image_path, cv.IMREAD_GRAYSCALE), output_mask)\n",
        "    cv.imwrite(os.path.join(output_mask_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "    # # print(out_mask_path)\n",
        "    # # print(image_path, mask_path)\n",
        "    # merge_original_mask(image_path, mask_path, output_mask_folder)\n",
        "    # merge_masks(out_mask_path, mask_path, output_mask_folder)\n",
        "    # # break"
      ],
      "metadata": {
        "id": "OyROv0wvZRUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Apply cell mask**"
      ],
      "metadata": {
        "id": "vDkdVH2o-xRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "\n",
        "images = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "if len(images) != len(cells):\n",
        "    print('The number of ground truths and created masks differs.')\n",
        "\n",
        "for image_name, cell_name in zip(images, cells):\n",
        "    print(f'Image: {image_name} - cell: {cell_name}')\n",
        "    im_path = os.path.join(mask_path, image_name)\n",
        "    cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "    image = cv.imread(im_path, cv.IMREAD_GRAYSCALE)\n",
        "    cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "    image[cell == 0] = 0\n",
        "    if rewrite_images:\n",
        "        new_name = image_name\n",
        "        cv.imwrite(os.path.join(mask_path, new_name), image)\n",
        "    else:\n",
        "        new_name = image_name\n",
        "        if not os.path.exists(os.path.join(mask_path, 'single_cell')):\n",
        "            os.makedirs(os.path.join(mask_path, 'single_cell'))\n",
        "        print(os.path.join(mask_path, 'single_cell', new_name))\n",
        "        cv.imwrite(os.path.join(mask_path, 'single_cell', new_name), image)\n",
        "\n"
      ],
      "metadata": {
        "id": "gSHpG-sO-urM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Exclusion of fenestrations based on diameter and roundness**"
      ],
      "metadata": {
        "id": "StzWoL-FyWjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "def remove_fenestrations(mask, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    # contours = find_fenestration_contours(mask_path)\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    # print(len(ellipses), len(contours))\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # # Also remove all contours that were too small to fit an ellipse\n",
        "    # print(len(ellipses), len(contours))\n",
        "    # print(ellipses[0])\n",
        "    # print(ellipses[1])\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse != (None, None, None) and ellipse is not None:\n",
        "            # print(ellipse)\n",
        "            center, axes, _ = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            # print(axes)\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length:\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                # rotation_angle = angle\n",
        "                diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "                # print(contour)\n",
        "                # print(diameter)\n",
        "                if (diameter < min_d or diameter > max_d) or  (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    mask = remove_contour_from_mask(contour, mask)\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "            else:\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    # # cv2_imshow(mask)\n",
        "    # # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    # equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # # print(equivalent_diameters)\n",
        "    # if len(equivalent_diameters) > 0:\n",
        "    #     mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "    #     std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "    #     print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    return mask"
      ],
      "metadata": {
        "id": "EKJrxqjN6WqR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert the pixel size, and min and max fenestration diameters in nanometers:**\n",
        "\n",
        "#@markdown All fenestration with a smaller or larger diameter than the chosen range will be removed from the crated masks.\n",
        "#@markdown (Use dot '.' as the decimal separator, not comma ',').\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "pixel_size_nm = 9.259 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "min_diameter_nm = 105 #@param {type:\"number\"}\n",
        "max_diameter_nm = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_roundness = False # @param {type:\"boolean\"}\n",
        "min_roundness = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "# #@markdown ---\n",
        "# filter_by_fenestration_area = False # @param {type:\"boolean\"}\n",
        "# min_area_nm2 = 105 #@param {type:\"number\"}\n",
        "# max_area_nm2 = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "mask_path = './gdrive/MyDrive/lsecs/mask_edit_test' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "mask_names = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "\n",
        "# def remove_contour_from_mask(contour, mask):\n",
        "#     # Fill the contour with black pixels\n",
        "#     cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "#     return mask\n",
        "\n",
        "# def remove_fenestrations(image, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "#     # contours = find_fenestration_contours(mask_path)\n",
        "#     contours, _ = cv.findContours(image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "#     fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "#     contour_centers = find_contour_centers(contours)\n",
        "#     ellipses = fit_ellipses(contours, contour_centers)\n",
        "#     roundness_of_ellipses = []\n",
        "#     equivalent_diameters = []\n",
        "#     fenestration_areas_from_ellipses = []\n",
        "#     mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "#     # Remove all contours that do not fit the chosen conditions\n",
        "#     # Also remove all contours that were too small to fit an ellipse\n",
        "#     for contour, ellipse in zip(contours, ellipses):\n",
        "#         if ellipse is not None:\n",
        "#             center, axes, angle = ellipse\n",
        "#             # center_x, center_y = center\n",
        "#             minor_axis_length, major_axis_length = axes\n",
        "#             # print(axes)\n",
        "#             if major_axis_length != 0 or major_axis_length > 20*minor_axis_length:\n",
        "#                 roundness = minor_axis_length/major_axis_length\n",
        "#                 if roundness >= min_roundness:\n",
        "#                     roundness_of_ellipses.append(roundness)\n",
        "#                 # rotation_angle = angle\n",
        "#                 diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "#                 # print(contour)\n",
        "#                 # print(diameter)\n",
        "#                 if filter_by_diameter and (diameter < min_d or diameter > max_d) or filter_by_roundness and (roundness < min_roundness) or np.isnan(diameter):\n",
        "#                     mask = remove_contour_from_mask(contour, mask)\n",
        "#                 else:\n",
        "#                     equivalent_diameters.append(diameter)\n",
        "#                     fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "#             else:\n",
        "#                 mask = remove_contour_from_mask(contour, mask)\n",
        "#         else:\n",
        "#             mask = remove_contour_from_mask(contour, mask)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "#     equivalent_diameters = np.array(equivalent_diameters)\n",
        "#     # print(equivalent_diameters)\n",
        "#     if len(equivalent_diameters) > 0:\n",
        "#         mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "#         std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "#         print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "#     return mask\n",
        "\n",
        "# def remove_fenestrations(mask_path, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "#     contours = find_fenestration_contours(mask_path)\n",
        "#     fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "#     contour_centers = find_contour_centers(contours)\n",
        "#     ellipses = fit_ellipses(contours, contour_centers)\n",
        "#     roundness_of_ellipses = []\n",
        "#     equivalent_diameters = []\n",
        "#     fenestration_areas_from_ellipses = []\n",
        "#     mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "#     # Remove all contours that do not fit the chosen conditions\n",
        "#     # Also remove all contours that were too small to fit an ellipse\n",
        "#     for contour, ellipse in zip(contours, ellipses):\n",
        "#         if ellipse is not None:\n",
        "#             center, axes, angle = ellipse\n",
        "#             # center_x, center_y = center\n",
        "#             minor_axis_length, major_axis_length = axes\n",
        "#             # print(axes)\n",
        "#             if major_axis_length != 0 or major_axis_length > 20*minor_axis_length:\n",
        "#                 roundness = minor_axis_length/major_axis_length\n",
        "#                 if roundness >= min_roundness:\n",
        "#                     roundness_of_ellipses.append(roundness)\n",
        "#                 # rotation_angle = angle\n",
        "#                 diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "#                 # print(contour)\n",
        "#                 # print(diameter)\n",
        "#                 if filter_by_diameter and (diameter < min_d or diameter > max_d) or filter_by_roundness and (roundness < min_roundness) or np.isnan(diameter):\n",
        "#                     mask = remove_contour_from_mask(contour, mask)\n",
        "#                 else:\n",
        "#                     equivalent_diameters.append(diameter)\n",
        "#                     fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "#             else:\n",
        "#                 mask = remove_contour_from_mask(contour, mask)\n",
        "#         else:\n",
        "#             mask = remove_contour_from_mask(contour, mask)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "#     equivalent_diameters = np.array(equivalent_diameters)\n",
        "#     # print(equivalent_diameters)\n",
        "#     if len(equivalent_diameters) > 0:\n",
        "#         mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "#         std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "#         print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "#     return mask\n",
        "\n",
        "\n",
        "#TODO: ukazat statistiky pro celou slozku obrazku\n",
        "\n",
        "if not rewrite_images:\n",
        "    new_mask_path = os.path.join(mask_path, 'edited_masks')\n",
        "    os.makedirs(new_mask_path, exist_ok=True)\n",
        "else:\n",
        "    new_mask_path = mask_path\n",
        "# print(new_mask_path)\n",
        "for mask_name in mask_names:\n",
        "    # print(mask_name)\n",
        "    mask_path_full = os.path.join(mask_path, mask_name)\n",
        "    # print(mask_path)\n",
        "    mask = cv.imread(mask_path_full, cv.IMREAD_GRAYSCALE)\n",
        "    edited_mask = remove_fenestrations(mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    # print(os.path.join(new_mask_path, mask_name))\n",
        "    cv.imwrite(os.path.join(new_mask_path, mask_name), edited_mask)\n",
        "    # cv.imwrite(os.path.join(new_mask_path, mask_name), mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Display the number of circles and their fitted ellipses\n",
        "# print(\"Number of fenestrations:\", len(contours))\n",
        "# print(\"Number of fitted ellipses:\", len(ellipses))\n"
      ],
      "metadata": {
        "id": "tRq0VTv6yrgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e1749b-72a0-4a92-c1bd-bd1b2427b25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean equavalent diameter: 171 nm, std: 45 nm \n",
            "Mean equavalent diameter: 193 nm, std: 47 nm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Inference evaluation**"
      ],
      "metadata": {
        "id": "RxiOKWQb6aFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert folders with cell images and their ground truth masks for comparison:\n",
        "images_path = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "ground_truth_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/ground_truth_masks_1205' #@param {type:\"string\"}\n",
        "semiauto_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks' #@param {type:\"string\"}\n",
        "\n",
        "models_path = './gdrive/MyDrive/lsecs' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "\n",
        "# filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "\n",
        "remove_false_fenestrations = True # @param {type:\"boolean\"}\n",
        "pixel_size_nm = 9.28 #@param {type:\"number\"}\n",
        "min_diameter_nm = 50 #@param {type:\"number\"}\n",
        "max_diameter_nm = 350 #@param {type:\"number\"}\n",
        "min_roundness = 0.4 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "log_file_path = './gdrive/MyDrive/lsecs/dice_score_test/log.txt'\n",
        "\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "\n",
        "\n",
        "\n",
        "ground_truth_mask_folder = ground_truth_mask_folder.strip()\n",
        "images_path = images_path.strip()\n",
        "models_path = models_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "semiauto_mask_folder = semiauto_mask_folder.strip()\n",
        "\n",
        "\n",
        "model_names = sorted([f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f)) and 'pt' in f])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "print(model_names)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(images_path):\n",
        "    print(\"Images folder does not exist\")\n",
        "    # exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    print(\"Folder with ground truth masks does not exist\")\n",
        "    # exit()\n",
        "\n",
        "ground_truth_images = sorted([f for f in os.listdir(ground_truth_mask_folder) if os.path.isfile(os.path.join(ground_truth_mask_folder, f))])\n",
        "images = sorted([f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))])\n",
        "semiauto_images = sorted([f for f in os.listdir(semiauto_mask_folder) if os.path.isfile(os.path.join(semiauto_mask_folder, f))])\n",
        "\n",
        "if len(ground_truth_images) != len(images) or len(images) != len(semiauto_images):\n",
        "    print('The number of ground truths and images differs.')\n",
        "    # exit()\n",
        "\n",
        "def compute_dice_score(image1, image2):\n",
        "    eps = 1e-8\n",
        "    image1[image1 == 255] = 1\n",
        "    image2[image2 == 255] = 1\n",
        "    intersection_sum = np.logical_and(image1, image2).sum()\n",
        "    dice_score = (2*intersection_sum+eps)/(image1.sum() + image2.sum() + eps)\n",
        "    return dice_score\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "# dice_scores = []\n",
        "# with open(log_file_path, \"a+\") as file:\n",
        "#     file.write(f'{len(images)} images\\n')\n",
        "#     for ground_truth_mask_name, image_name, cell_name  in zip(ground_truth_images, images, cells):\n",
        "#         print(f'Compare: {ground_truth_mask_name} - {image_name} - {cell_name}')\n",
        "#         file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "#         ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "#         image_path = os.path.join(images_path, image_name)\n",
        "#         cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "#         cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "#         ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask[cell == 0] = 0\n",
        "#         current_dice_score = compute_dice_score(ground_truth_mask, image_mask)\n",
        "#         print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "#         file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "#         dice_scores.append(current_dice_score)\n",
        "\n",
        "#     dice_scores = np.array(dice_scores)\n",
        "#     mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "#     std_dice = round(np.std(dice_scores)*100, 1)\n",
        "\n",
        "#     print(f'Semiautomatic Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "#     file.write(f'Semiautomatic Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "\n",
        "def get_fenestrations_from_image(mask):\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    # fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    return ellipses, num_ellipses, contours, len(contours)\n",
        "\n",
        "def get_image_stats(contours, ellipses, pixel_size_nm, min_d=0, max_d=100000, min_roundness=0):\n",
        "    # contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    # contour_centers = find_contour_centers(contours)\n",
        "    # ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None and ellipse != (None, None, None):\n",
        "            # print(ellipse)\n",
        "            center, axes, _ = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length: # The fitting algorithm can fail sometimes\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                diameter_pix = equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "                diameter = pixel_size_nm * diameter_pix\n",
        "                # print(contour)\n",
        "                # print(diameter)\n",
        "                if (diameter < min_d or diameter > max_d) or (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    # mask = remove_contour_from_mask(contour, mask)\n",
        "                    continue\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter_pix**2)/4*math.pi)\n",
        "        #     else:\n",
        "        #         mask = remove_contour_from_mask(contour, mask)\n",
        "        # else:\n",
        "        #     mask = remove_contour_from_mask(contour, mask)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    # equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # roundness_of_ellipses = np.array(roundness_of_ellipses)\n",
        "    # fenestration_areas_from_ellipses = np.array(fenestration_areas_from_ellipses)\n",
        "    return equivalent_diameters, roundness_of_ellipses, fenestration_areas_from_ellipses\n",
        "    # # print(equivalent_diameters)\n",
        "    # if len(equivalent_diameters) > 0:\n",
        "    #     mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "    #     std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "    #     print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    # return len(contour), num_ellipses\n",
        "\n",
        "import seaborn as sns\n",
        "# Get stats\n",
        "\n",
        "# pixel_size_nm = 9.259\n",
        "\n",
        "# fig, ax = plt.subplots(len(model_names)*len(images), 2, figsize=(15, 7*len(images)))\n",
        "\n",
        "# Flatten the ax array\n",
        "# ax_flat = ax.flatten()\n",
        "\n",
        "# with open(log_file_path, \"a+\") as file:\n",
        "    # file.write(f'{len(images)} images\\n')\n",
        "# i = 0\n",
        "save_plots = True\n",
        "save_image_masks = False\n",
        "plot_path = './gdrive/MyDrive/lsecs/plots/'\n",
        "\n",
        "for model_name in model_names:\n",
        "    all_my_diameters = []\n",
        "    all_gt_diameters = []\n",
        "    all_s_diameters = []\n",
        "\n",
        "    all_my_means = []\n",
        "    all_gt_means = []\n",
        "    all_s_means = []\n",
        "\n",
        "    all_my_roundness = []\n",
        "    all_gt_roundness = []\n",
        "    all_s_roundness = []\n",
        "\n",
        "    num_all_my_ellipses = []\n",
        "    num_all_gt_ellipses = []\n",
        "    num_all_s_ellipses = []\n",
        "\n",
        "    my_dice_scores = []\n",
        "    s_dice_scores = []\n",
        "\n",
        "    all_gt_porosities_pix = []\n",
        "    all_s_porosities_pix = []\n",
        "    all_my_porosities_pix = []\n",
        "\n",
        "    all_gt_porosities_ell = []\n",
        "    all_s_porosities_ell = []\n",
        "    all_my_porosities_ell = []\n",
        "\n",
        "    all_gt_frequencies = []\n",
        "    all_s_frequencies = []\n",
        "    all_my_frequencies = []\n",
        "\n",
        "    # file.write(f'{model_name}\\n')\n",
        "    print(model_name)\n",
        "    model = build_model(model_name.split('+')[0]+'+none')\n",
        "    loaded_state_dict = torch.load(os.path.join(models_path, model_name)) # TODO:these models do not include sigmoid and preprocessing yet\n",
        "    model.load_state_dict(loaded_state_dict)\n",
        "    model.eval()\n",
        "    # dice_scores = []\n",
        "    # dice_scores_filt = []\n",
        "    for ground_truth_mask_name, image_name, semiauto_image_name, cell_name  in zip(ground_truth_images, images, semiauto_images, cells):\n",
        "        print(f'Compare: {ground_truth_mask_name} - {image_name} - {semiauto_image_name} -{cell_name}')\n",
        "        # file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "        ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "        image_path = os.path.join(images_path, image_name)\n",
        "        semiauto_image_path = os.path.join(semiauto_mask_folder, semiauto_image_name)\n",
        "        cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "        image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "        cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "        ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        semiauto_mask = cv.imread(semiauto_image_path, cv.IMREAD_GRAYSCALE)\n",
        "\n",
        "        new_mask = inference_on_image_with_overlap(model, image_path)\n",
        "\n",
        "        new_mask[cell == 0] = 0\n",
        "        semiauto_mask[cell == 0] = 0\n",
        "        ground_truth_mask[cell == 0] = 0\n",
        "\n",
        "\n",
        "        # I need to fill the cell nucleus to compute the area\n",
        "        contours, hierarchy = cv.findContours(\n",
        "            cell, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        # Get the largest object in the image(the cell)\n",
        "        empty_mask = np.zeros_like(cell)\n",
        "        areas = []\n",
        "        for cnt in contours:\n",
        "            area = cv.contourArea(cnt)\n",
        "            areas.append(area)\n",
        "        areas = np.array(areas)\n",
        "        max_area_idx = np.argmax(areas)\n",
        "\n",
        "        # Fill this object and compute its area\n",
        "        c = 0\n",
        "        for cnt in contours:\n",
        "            if c == max_area_idx:\n",
        "                cv.drawContours(empty_mask, [cnt], -1, 1, thickness=cv.FILLED)\n",
        "                cell_contour, hierarchy = cv.findContours(\n",
        "                    empty_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "            c += 1\n",
        "        cell_area = float(np.sum(empty_mask))\n",
        "\n",
        "\n",
        "        # Remove fenestrations\n",
        "        new_mask_filt = remove_fenestrations(new_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "        s_mask_filt = remove_fenestrations(semiauto_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "        gt_mask_filt = remove_fenestrations(ground_truth_mask.copy(), min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "\n",
        "        if save_image_masks:\n",
        "            cv.imwrite(os.path.join(plot_path, image_name), new_mask_filt)\n",
        "\n",
        "        # Save filtered masks\n",
        "            merge = np.zeros((new_mask_filt.shape[0], new_mask_filt.shape[1], 3))\n",
        "            merge = merge.astype('uint8')\n",
        "            merge[:, :, 2][gt_mask_filt == 255.0] = 255 #R\n",
        "            merge2 = merge.copy()\n",
        "            merge3 = merge.copy()\n",
        "            merge[:, :, 0][s_mask_filt == 255.0] = 255 #B\n",
        "            merge[:, :, 1][s_mask_filt == 255.0] = 100 #G, so the blue is more visible\n",
        "\n",
        "            merge[:, :, 1][(s_mask_filt == 255.0) & (gt_mask_filt == 255.0)] = 255 #G\n",
        "\n",
        "            cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare_semiautomatic\"+'.png'), merge)\n",
        "\n",
        "            merge2[:, :, 1][new_mask_filt == 255.0] = 255 #G\n",
        "            merge2[:, :, 0][(new_mask_filt == 255.0) & (gt_mask_filt == 255.0)] = 255 #B\n",
        "            cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare_automatic\"+'.png'), merge2)\n",
        "\n",
        "            # merge3[:, :, 0][s_mask_filt == 255.0] = 255 #B\n",
        "            # merge3[:, :, 1][s_mask_filt == 255.0] = 100 #G, so the blue is more visible\n",
        "            # merge3[:, :, 1][new_mask_filt == 255.0] = 255 #G\n",
        "            # cv.imwrite(os.path.join(plot_path, image_name+\"_mask_compare\"+'.png'), merge3)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        gt_cell_area_pix = np.sum(gt_mask_filt/255)\n",
        "        s_cell_area_pix = np.sum(s_mask_filt/255)\n",
        "        my_cell_area_pix = np.sum(new_mask_filt/255)\n",
        "\n",
        "        gt_fen_area = float(gt_cell_area_pix)\n",
        "        s_fen_area = float(s_cell_area_pix)\n",
        "        my_fen_area = float(my_cell_area_pix)\n",
        "\n",
        "        gt_porosity_pix = round(gt_fen_area/cell_area*100, 1)\n",
        "        s_porosity_pix = round(s_fen_area/cell_area*100, 1)\n",
        "        my_porosity_pix = round(my_fen_area/cell_area*100, 1)\n",
        "\n",
        "        all_gt_porosities_pix.append(gt_porosity_pix)\n",
        "        all_s_porosities_pix.append(s_porosity_pix)\n",
        "        all_my_porosities_pix.append(my_porosity_pix)\n",
        "\n",
        "        # print(f'gt: {gt_porosity_pix}, s: {s_porosity_pix}, my: {my_porosity_pix}')\n",
        "\n",
        "\n",
        "\n",
        "        # Compute Dice scores\n",
        "        my_current_dice = compute_dice_score(gt_mask_filt, new_mask_filt)\n",
        "        s_current_dice = compute_dice_score(gt_mask_filt, s_mask_filt)\n",
        "\n",
        "        my_dice_scores.append(my_current_dice)\n",
        "        s_dice_scores.append(s_current_dice)\n",
        "        # print(f'Image Dice score: {round(my_current_dice*100, 1)}')\n",
        "        #\n",
        "\n",
        "        # my data\n",
        "        my_ellipses, num_my_ellipses, my_objects, num_all_my_objects = get_fenestrations_from_image(new_mask)\n",
        "        my_equivalent_diameters, my_roundness_of_ellipses, my_fenestration_areas_from_ellipses = get_image_stats(my_objects, my_ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "        my_ell_area = np.sum(np.array(my_fenestration_areas_from_ellipses))\n",
        "        all_my_porosities_ell.append(round(my_ell_area/cell_area*100, 1))\n",
        "        # ground truth\n",
        "        ellipses, num_ellipses, objects, num_all_objects = get_fenestrations_from_image(ground_truth_mask)\n",
        "        equivalent_diameters, roundness_of_ellipses, fenestration_areas_from_ellipses = get_image_stats(objects, ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "        gt_ell_area = np.sum(np.array(fenestration_areas_from_ellipses))\n",
        "        all_gt_porosities_ell.append(round(gt_ell_area/cell_area*100, 1))\n",
        "        # semiautomatic_data\n",
        "        s_ellipses, s_num_ellipses, s_objects, s_num_all_objects = get_fenestrations_from_image(semiauto_mask)\n",
        "        s_equivalent_diameters, s_roundness_of_ellipses, s_fenestration_areas_from_ellipses = get_image_stats(s_objects, s_ellipses, pixel_size_nm, min_d=50, max_d=400, min_roundness=0.4)\n",
        "        s_ell_area = np.sum(np.array(s_fenestration_areas_from_ellipses))\n",
        "        all_s_porosities_ell.append(round(s_ell_area/cell_area*100, 1))\n",
        "\n",
        "        all_my_diameters.extend(my_equivalent_diameters)\n",
        "        all_gt_diameters.extend(equivalent_diameters)\n",
        "        all_s_diameters.extend(s_equivalent_diameters)\n",
        "\n",
        "        all_my_roundness.extend(my_roundness_of_ellipses)\n",
        "        all_gt_roundness.extend(roundness_of_ellipses)\n",
        "        all_s_roundness.extend(s_roundness_of_ellipses)\n",
        "\n",
        "\n",
        "        n_my_ellipses = len(my_equivalent_diameters)\n",
        "        n_s_ellipses = len(s_equivalent_diameters)\n",
        "        n_gt_ellipses = len(equivalent_diameters)\n",
        "\n",
        "        gt_freq = n_gt_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "        s_freq = n_s_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "        my_freq = n_my_ellipses/(cell_area*((pixel_size_nm/1000)**2))\n",
        "\n",
        "        all_gt_frequencies.append(round(gt_freq, 1))\n",
        "        all_s_frequencies.append(round(s_freq, 1))\n",
        "        all_my_frequencies.append(round(my_freq, 1))\n",
        "\n",
        "        num_all_my_ellipses.append(n_my_ellipses)\n",
        "        num_all_gt_ellipses.append(n_gt_ellipses)\n",
        "        num_all_s_ellipses.append(n_s_ellipses)\n",
        "\n",
        "        gt_mean = round(np.mean(np.array(equivalent_diameters)))\n",
        "        gt_std = round(np.std(np.array(equivalent_diameters)))\n",
        "        my_mean = round(np.mean(np.array(my_equivalent_diameters)))\n",
        "        my_std = round(np.std(np.array(my_equivalent_diameters)))\n",
        "        s_mean = round(np.mean(np.array(s_equivalent_diameters)))\n",
        "        s_std = round(np.std(np.array(s_equivalent_diameters)))\n",
        "\n",
        "\n",
        "        all_my_means.append(my_mean)\n",
        "        all_gt_means.append(gt_mean)\n",
        "        all_s_means.append(s_mean)\n",
        "\n",
        "        fig, ax = plt.subplots(3, 2, figsize=(15, 23))\n",
        "        ax = ax.flatten()\n",
        "        # plt.subplots(1, 2, figsize=(15, 7))\n",
        "        sns.set_theme()  # This changes the look of plots.\n",
        "        nbins = 20\n",
        "        # Calculate density for each dataset\n",
        "        hist_x, bin_edges_x  = np.histogram(equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "        hist_y, bin_edges_y = np.histogram(my_equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "        hist_z, bin_edges_z = np.histogram(s_equivalent_diameters, bins=20, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "        hist_x = hist_x*np.diff(bin_edges_x)\n",
        "        hist_y = hist_y*np.diff(bin_edges_y)\n",
        "        hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "        bin_width = (max_diameter_nm - min_diameter_nm)/3/nbins\n",
        "        shift1 = bin_width/2\n",
        "        shift2 = bin_width/2 + bin_width\n",
        "        shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "\n",
        "        ax[0].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "        ax[0].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "        ax[0].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "        # ax[0].legend([f'Ground truth, mean: {gt_mean} nm\\n{len(equivalent_diameters)} fenestrations', f'Automatic, mean: {my_mean} nm\\n{len(my_equivalent_diameters)} fenestrations', f'Semiautomatic, mean: {s_mean} nm\\n{len(s_equivalent_diameters)} fenestrations'], fontsize='small')\n",
        "        ax[0].legend([f'Ground truth\\nmean d = {gt_mean} nm', f'Automatic\\nmean d = {my_mean} nm', f'Semiautomatic\\nmean d = {s_mean} nm'], fontsize='medium')\n",
        "\n",
        "        # Add title and axis labels\n",
        "        # ax[0].set_title(image_name)\n",
        "        ax[0].set_xlabel('Equivalent diameter (nm)', fontsize=16)\n",
        "        ax[0].set_ylabel('Probability', fontsize=16)\n",
        "        ax[0].set_xlim(min_diameter_nm-20, max_diameter_nm+20)\n",
        "\n",
        "        # x_ticks = [50, 100, 150, 200, 250, 300, 350, 400]  # Define which x ticks to show\n",
        "        x_ticks = list(range(min_diameter_nm, max_diameter_nm+1, 50))\n",
        "        ax[0].set_xticks(x_ticks)  # Set the x ticks\n",
        "        ax[0].set_xticklabels(x_ticks)  # Set the labels for the x ticks\n",
        "        ax[0].tick_params(axis='x', labelsize=12)\n",
        "        ax[0].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "\n",
        "        # Calculate density for each dataset\n",
        "        nbins = 12\n",
        "        hist_x, bin_edges_x  = np.histogram(roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "        hist_y, bin_edges_y = np.histogram(my_roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "        hist_z, bin_edges_z = np.histogram(s_roundness_of_ellipses, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "        hist_x = hist_x*np.diff(bin_edges_x)\n",
        "        hist_y = hist_y*np.diff(bin_edges_y)\n",
        "        hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "        bin_width = (1 - min_roundness)/3/nbins\n",
        "        shift1 = bin_width/2\n",
        "        shift2 = bin_width/2 + bin_width\n",
        "        shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "        ax[1].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "        ax[1].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "        ax[1].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "        # # i += 1\n",
        "        # plt.subplot(1, 2)\n",
        "        # ax[1].hist([roundness_of_ellipses, my_roundness_of_ellipses, s_roundness_of_ellipses], color=['g','r','b'], alpha=0.8, density=True)\n",
        "        ax[1].legend(['Ground truth', 'Automatic', 'Semiautomatic'], fontsize='medium')\n",
        "        # Add title and axis labels\n",
        "        # ax[1].set_title(image_name)\n",
        "        ax[1].set_xlabel('Roundness of fitted ellipses', fontsize=16)\n",
        "        ax[1].set_ylabel('Probability', fontsize=16)\n",
        "        ax[1].set_xlim(min_roundness - 0.02, 1.02)\n",
        "        ax[1].tick_params(axis='x', labelsize=12)\n",
        "        ax[1].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "        # Show image patch\n",
        "        r = slice(3600,4200)\n",
        "        c = slice(2600,3200)\n",
        "        image_patch = image[r, c]\n",
        "        show_size = (300, 300)\n",
        "        image_patch = cv.resize(image_patch, show_size)\n",
        "        ax[2].imshow(image_patch, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "        ax[2].set_title('Image patch example (600x600 pixels)', fontsize=16)\n",
        "        ax[2].axis('off')\n",
        "\n",
        "        ax[3].axis('off')\n",
        "\n",
        "        gt_patch = ground_truth_mask[r, c]\n",
        "        s_patch = semiauto_mask[r, c]\n",
        "        my_patch = new_mask[r, c]\n",
        "        # print(gt_patch.shape)\n",
        "\n",
        "        gt_patch = remove_fenestrations(gt_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "        s_patch = remove_fenestrations(s_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "        my_patch = remove_fenestrations(my_patch, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "\n",
        "        # image_patch = cv.resize(image_patch, show_size, interpolation=cv.INTER_NEAREST)\n",
        "\n",
        "        merge = np.zeros((gt_patch.shape[0], gt_patch.shape[1], 3))\n",
        "        merge[:, :, 0][gt_patch == 255] = 255 # R channel\n",
        "        merge = merge.astype('uint8')\n",
        "        ax[4].imshow(merge, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "        ax[4].set_title('Ground truth mask', fontsize=16)\n",
        "        ax[4].axis('off')\n",
        "\n",
        "        merge[:, :, 2][s_patch == 255] = 255 # B channel\n",
        "        merge[:, :, 1][s_patch == 255] = 100 # G channel\n",
        "        merge[:, :, 1][my_patch == 255] = 255 # G channel\n",
        "        merge = merge.astype('uint8')\n",
        "\n",
        "        ax[5].imshow(merge, cmap='gray', vmin=0, vmax=255)  # Specify min and max values\n",
        "        ax[5].set_title('Mask comparison', fontsize=16)\n",
        "        ax[5].axis('off')\n",
        "        leg = f'R = automatic FN & semiautomatic FN\\nG = automatic FP\\nB = semiautomatic FP'\n",
        "        leg += '\\nC = automatic FN & semiautomatic FN'\n",
        "        leg += '\\nM = automatic FN & semiautomatic TP'\n",
        "        leg += '\\nY  = automatic TP & semiautomatic FN'\n",
        "        leg += '\\nW = automatic TP & semiautomatic TP'\n",
        "        ax[5].text(0, merge.shape[0]+210, leg, ha='left', fontsize=16)\n",
        "        if save_plots:\n",
        "            plt.savefig(os.path.join(plot_path, f'stats_{image_name}.svg'), format='svg')\n",
        "\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # Stats for whole dataset\n",
        "    gt_mean = round(np.mean(np.array(all_gt_diameters)))\n",
        "    gt_std = round(np.std(np.array(all_gt_diameters)))\n",
        "    my_mean = round(np.mean(np.array(all_my_diameters)))\n",
        "    my_std = round(np.std(np.array(all_my_diameters)))\n",
        "    s_mean = round(np.mean(np.array(all_s_diameters)))\n",
        "    s_std = round(np.std(np.array(all_s_diameters)))\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(13, 7))\n",
        "    ax = ax.flatten()\n",
        "    sns.set_theme()\n",
        "    # Calculate density for each dataset\n",
        "    nbins=20\n",
        "    hist_x, bin_edges_x  = np.histogram(all_gt_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_y, bin_edges_y = np.histogram(all_my_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_z, bin_edges_z = np.histogram(all_s_diameters, bins=nbins, range = (min_diameter_nm, max_diameter_nm), density=True)\n",
        "    hist_x = hist_x*np.diff(bin_edges_x)\n",
        "    hist_y = hist_y*np.diff(bin_edges_y)\n",
        "    hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "    bin_width = (max_diameter_nm - min_diameter_nm)/3/nbins\n",
        "    shift1 = bin_width/2\n",
        "    shift2 = bin_width/2 + bin_width\n",
        "    shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "    ax[0].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "    ax[0].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "    ax[0].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "    # ax[0].legend([f'Ground truth, mean: {gt_mean} nm\\n{len(all_gt_diameters)} fenestrations', f'Automatic, mean: {my_mean} nm\\n{len(all_my_diameters)} fenestrations', f'Semiautomatic, mean: {s_mean} nm\\n{len(all_s_diameters)} fenestrations'], fontsize='small')\n",
        "    ax[0].legend([f'Ground truth\\nmean d = {gt_mean} nm', f'Automatic\\nmean d = {my_mean} nm', f'Semiautomatic\\nmean d = {s_mean} nm'], fontsize='medium')\n",
        "\n",
        "    # Add title and axis labels\n",
        "    ax[0].set_title('Whole dataset', fontsize=22)\n",
        "    ax[0].set_xlabel('Equivalent diameter (nm)', fontsize=16)\n",
        "    ax[0].set_ylabel('Probability', fontsize=16)\n",
        "    ax[0].set_xlim(min_diameter_nm-20, max_diameter_nm+20)\n",
        "\n",
        "    # x_ticks = [50, 100, 150, 200, 250, 300, 350, 400]  # Define which x ticks to show\n",
        "    x_ticks = list(range(min_diameter_nm, max_diameter_nm+1, 50))\n",
        "    ax[0].set_xticks(x_ticks)  # Set the x ticks\n",
        "    ax[0].set_xticklabels(x_ticks)  # Set the labels for the x ticks\n",
        "    ax[0].tick_params(axis='x', labelsize=12)\n",
        "    ax[0].tick_params(axis='y', labelsize=12)\n",
        "\n",
        "\n",
        "    # Stats for whole dataset\n",
        "    gt_mean = round(np.mean(np.array(all_gt_roundness)))\n",
        "    gt_std = round(np.std(np.array(all_gt_roundness)))\n",
        "    my_mean = round(np.mean(np.array(all_my_roundness)))\n",
        "    my_std = round(np.std(np.array(all_my_roundness)))\n",
        "    s_mean = round(np.mean(np.array(all_s_roundness)))\n",
        "    s_std = round(np.std(np.array(all_s_roundness)))\n",
        "\n",
        "    # Calculate density for each dataset\n",
        "\n",
        "\n",
        "    nbins=12\n",
        "    hist_x, bin_edges_x  = np.histogram(all_gt_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_y, bin_edges_y = np.histogram(all_my_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_z, bin_edges_z = np.histogram(all_s_roundness, bins=nbins, range = (min_roundness, 1), density=True)\n",
        "    hist_x = hist_x*np.diff(bin_edges_x)\n",
        "    hist_y = hist_y*np.diff(bin_edges_y)\n",
        "    hist_z = hist_z*np.diff(bin_edges_z)\n",
        "\n",
        "    bin_width = (1 - min_roundness)/3/nbins\n",
        "    shift1 = bin_width/2\n",
        "    shift2 = bin_width/2 + bin_width\n",
        "    shift3 = bin_width/2 + 2*bin_width\n",
        "\n",
        "    ax[1].bar(bin_edges_x[:-1]+shift1, hist_x, color='r', alpha=0.9, width=bin_width)\n",
        "    ax[1].bar(bin_edges_y[:-1]+shift2, hist_y, color='g', alpha=0.9, width=bin_width)\n",
        "    ax[1].bar(bin_edges_z[:-1]+shift3, hist_z, color='b', alpha=0.9, width=bin_width)\n",
        "    # ax[1].legend([f'Ground truth\\nmean roundness = {gt_mean}', f'Automatic\\nmean roundness = {my_mean}', f'Semiautomatic\\nmean roundness = {s_mean}'], fontsize='small')\n",
        "    # Add title and axis labels\n",
        "    ax[1].set_title('Whole dataset', fontsize=22)\n",
        "    ax[1].set_xlabel('Roundness of fitted ellipses', fontsize=16)\n",
        "    ax[1].set_ylabel('Probability', fontsize=16)\n",
        "    ax[1].legend(['Ground truth', 'Automatic', 'Semiautomatic'], fontsize='medium')\n",
        "    ax[1].set_xlim(min_roundness-0.02, 1.02)\n",
        "    ax[1].tick_params(axis='x', labelsize=12)\n",
        "    ax[1].tick_params(axis='y', labelsize=12)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'whole.svg'), format='svg')\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation for the number of found ellipses\n",
        "    num_all_my_ellipses = np.array(num_all_my_ellipses)\n",
        "    num_all_gt_ellipses = np.array(num_all_gt_ellipses)\n",
        "    num_all_s_ellipses =  np.array(num_all_s_ellipses)\n",
        "\n",
        "    sorted_indices = np.argsort(num_all_gt_ellipses)\n",
        "    num_all_gt_ellipses = num_all_gt_ellipses[sorted_indices]\n",
        "    num_all_my_ellipses = num_all_my_ellipses[sorted_indices]\n",
        "    num_all_s_ellipses = num_all_s_ellipses[sorted_indices]\n",
        "\n",
        "    my_corr_coeff = np.corrcoef(num_all_gt_ellipses, num_all_my_ellipses)[0, 1]\n",
        "    my_r_squared = round(my_corr_coeff ** 2, 2)\n",
        "    my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "    s_corr_coeff = np.corrcoef(num_all_gt_ellipses, num_all_s_ellipses)[0, 1]\n",
        "    s_r_squared = round(s_corr_coeff ** 2, 2)\n",
        "    s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    my_coefficients = np.polyfit(num_all_gt_ellipses, num_all_my_ellipses, 1)\n",
        "    my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    s_coefficients = np.polyfit(num_all_gt_ellipses, num_all_s_ellipses, 1)\n",
        "    s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "\n",
        "    s_fitted_values = np.polyval(s_coefficients, num_all_gt_ellipses)\n",
        "    s_residuals = num_all_s_ellipses - s_fitted_values\n",
        "    s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "    my_fitted_values = np.polyval(my_coefficients, num_all_gt_ellipses)\n",
        "    my_residuals = num_all_my_ellipses - my_fitted_values\n",
        "    my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "    s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "    my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "    s_tg = np.polyval(s_derivative_coefficients, 1000)\n",
        "    my_tg = np.polyval(my_derivative_coefficients, 1000)\n",
        "    s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "    my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "    s_residuals_sd = round(s_residuals_sd)\n",
        "    my_residuals_sd = round(my_residuals_sd)\n",
        "\n",
        "\n",
        "    print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "    print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    # ax = ax.flatten()\n",
        "    sns.set_theme()\n",
        "    ax.plot(num_all_gt_ellipses, my_fit_line(num_all_gt_ellipses), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}±{my_residuals_sd}')\n",
        "    ax.plot(num_all_gt_ellipses, s_fit_line(num_all_gt_ellipses), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}±{s_residuals_sd}')\n",
        "    ax.scatter(num_all_gt_ellipses, num_all_my_ellipses, color='g', marker='o', label=f'Automatic: R² = {my_r_squared}', alpha=np.ones_like(num_all_gt_ellipses))\n",
        "    ax.scatter(num_all_gt_ellipses, num_all_s_ellipses, color='b', marker='s', label=f'Semiautomatic: R² = {s_r_squared}', alpha=np.ones_like(num_all_gt_ellipses))\n",
        "\n",
        "    ax.legend(fontsize='medium')\n",
        "    ax.set_xlabel('Ground truth number of fenestrations per image', fontsize=16)\n",
        "    ax.set_ylabel('Found number of fenestrations per image', fontsize=16)\n",
        "    ax.set_title(f'Correlation plot of the number of found fenestrations\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "    ax.grid(True)\n",
        "    ax.tick_params(axis='x', labelsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=12)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'corr_num.svg'), format='svg')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    print('num ellipses')\n",
        "    print(f'gt {np.sum(num_all_gt_ellipses)}, my {np.sum(num_all_my_ellipses)}, s {np.sum(num_all_s_ellipses)}')\n",
        "    print(my_dice_scores)\n",
        "    print(s_dice_scores)\n",
        "    print('my')\n",
        "    for dice in my_dice_scores:\n",
        "        print(round(dice, 2))\n",
        "    my_dice_scores = np.array(my_dice_scores)\n",
        "    my_mean_dice = round(np.mean(my_dice_scores), 2)\n",
        "    my_std_dice = round(np.std(my_dice_scores), 2)\n",
        "    print(f'Auto mean dice: {my_mean_dice} +- {my_std_dice}')\n",
        "    print('s')\n",
        "    for dice in s_dice_scores:\n",
        "        print(round(dice, 2))\n",
        "    s_dice_scores = np.array(s_dice_scores)\n",
        "    s_mean_dice = round(np.mean(s_dice_scores), 2)\n",
        "    s_std_dice = round(np.std(s_dice_scores), 2)\n",
        "    print(f'Semiauto mean dice: {s_mean_dice} +- {s_std_dice}')\n",
        "\n",
        "    # correlation of means\n",
        "\n",
        "\n",
        "    all_my_means = np.array(all_my_means)\n",
        "    all_gt_means = np.array(all_gt_means)\n",
        "    all_s_means =  np.array(all_s_means)\n",
        "\n",
        "    sorted_indices = np.argsort(all_gt_means)\n",
        "    all_gt_means = all_gt_means[sorted_indices]\n",
        "    all_my_means = all_my_means[sorted_indices]\n",
        "    all_s_means = all_s_means[sorted_indices]\n",
        "\n",
        "    my_corr_coeff = np.corrcoef(all_gt_means, all_my_means)[0, 1]\n",
        "    my_r_squared = round(my_corr_coeff ** 2, 2)\n",
        "    my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "    s_corr_coeff = np.corrcoef(all_gt_means, all_s_means)[0, 1]\n",
        "    s_r_squared = round(s_corr_coeff ** 2, 2)\n",
        "    s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    my_coefficients = np.polyfit(all_gt_means, all_my_means, 1)\n",
        "    my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    s_coefficients = np.polyfit(all_gt_means, all_s_means, 1)\n",
        "    s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "    s_fitted_values = np.polyval(s_coefficients, all_gt_means)\n",
        "    s_residuals = all_s_means - s_fitted_values\n",
        "    s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "    my_fitted_values = np.polyval(my_coefficients, all_gt_means)\n",
        "    my_residuals = all_my_means - my_fitted_values\n",
        "    my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "    s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "    my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "    s_tg = np.polyval(s_derivative_coefficients, 150)\n",
        "    my_tg = np.polyval(my_derivative_coefficients, 150)\n",
        "    s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "    my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "    s_residuals_sd = round(s_residuals_sd)\n",
        "    my_residuals_sd = round(my_residuals_sd)\n",
        "\n",
        "    print('mean values')\n",
        "    print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "    print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    sns.set_theme()\n",
        "    ax.plot(all_gt_means, my_fit_line(all_gt_means), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}±{my_residuals_sd}')\n",
        "    ax.plot(all_gt_means, s_fit_line(all_gt_means), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}±{s_residuals_sd}')\n",
        "    ax.scatter(all_gt_means, all_my_means, color='g', marker='o', label=f'Automatic: R² = {my_r_squared}', alpha=np.ones_like(all_gt_means))\n",
        "    ax.scatter(all_gt_means, all_s_means, color='b', marker='s', label=f'Semiautomatic: R² = {s_r_squared}', alpha=np.ones_like(all_gt_means))\n",
        "\n",
        "    ax.legend(fontsize='medium')\n",
        "    ax.set_xlabel('Ground truth mean fenestration\\nequivalent diameter per image (nm)', fontsize=16)\n",
        "    ax.set_ylabel('Found mean fenestration\\nequivalent diameter per image (nm)', fontsize=16)\n",
        "    ax.set_title(f'Correlation plot of the mean fenestration diameter\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "    ax.grid(True)\n",
        "    ax.tick_params(axis='x', labelsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=12)\n",
        "    x_ticks = list(range(110, 180+1, 10))\n",
        "    ax.set_xticks(x_ticks)\n",
        "    ax.set_yticks(x_ticks)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'corr_diameter.svg'), format='svg')\n",
        "    plt.show()\n",
        "    print('gt means')\n",
        "    for mean in all_gt_means:\n",
        "        print(mean)\n",
        "    print('my means')\n",
        "    for mean in all_my_means:\n",
        "        print(mean)\n",
        "    print('s means')\n",
        "    for mean in all_s_means:\n",
        "        print(mean)\n",
        "\n",
        "\n",
        "\n",
        "    print('porosity')\n",
        "    print('gt')\n",
        "    for p in all_gt_porosities_pix:\n",
        "        print(p)\n",
        "    print('s')\n",
        "    for p in all_s_porosities_pix:\n",
        "        print(p)\n",
        "    print('my')\n",
        "    for p in all_my_porosities_pix:\n",
        "        print(p)\n",
        "\n",
        "    # Porosity correlation\n",
        "    all_gt_porosities_pix = np.array(all_gt_porosities_pix)\n",
        "    all_s_porosities_pix = np.array(all_s_porosities_pix)\n",
        "    all_my_porosities_pix =  np.array(all_my_porosities_pix)\n",
        "\n",
        "    sorted_indices = np.argsort(all_gt_porosities_pix)\n",
        "    all_gt_porosities_pix = all_gt_porosities_pix[sorted_indices]\n",
        "    all_s_porosities_pix = all_s_porosities_pix[sorted_indices]\n",
        "    all_my_porosities_pix = all_my_porosities_pix[sorted_indices]\n",
        "\n",
        "    my_corr_coeff = np.corrcoef(all_gt_porosities_pix, all_my_porosities_pix)[0, 1]\n",
        "    my_r_squared = round(my_corr_coeff ** 2, 2)\n",
        "    my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "    s_corr_coeff = np.corrcoef(all_gt_porosities_pix, all_s_porosities_pix)[0, 1]\n",
        "    s_r_squared = round(s_corr_coeff ** 2, 2)\n",
        "    s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    my_coefficients = np.polyfit(all_gt_porosities_pix, all_my_porosities_pix, 1)\n",
        "    my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    s_coefficients = np.polyfit(all_gt_porosities_pix, all_s_porosities_pix, 1)\n",
        "    s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "    s_fitted_values = np.polyval(s_coefficients, all_gt_porosities_pix)\n",
        "    s_residuals = all_s_porosities_pix - s_fitted_values\n",
        "    s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "    my_fitted_values = np.polyval(my_coefficients, all_gt_porosities_pix)\n",
        "    my_residuals = all_my_porosities_pix - my_fitted_values\n",
        "    my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "    s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "    my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "    s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "    my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "    s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "    my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "    s_residuals_sd = round(s_residuals_sd, 2)\n",
        "    my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "    # print('mean values')\n",
        "    # print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "    # print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    sns.set_theme()\n",
        "    ax.plot(all_gt_porosities_pix, my_fit_line(all_gt_porosities_pix), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}±{my_residuals_sd}')\n",
        "    ax.plot(all_gt_porosities_pix, s_fit_line(all_gt_porosities_pix), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}±{s_residuals_sd}')\n",
        "    ax.scatter(all_gt_porosities_pix, all_my_porosities_pix, color='g', marker='o', label=f'Automatic: R² = {my_r_squared}', alpha=np.ones_like(all_gt_porosities_pix))\n",
        "    ax.scatter(all_gt_porosities_pix, all_s_porosities_pix, color='b', marker='s', label=f'Semiautomatic: R² = {s_r_squared}', alpha=np.ones_like(all_gt_porosities_pix))\n",
        "\n",
        "    ax.legend(fontsize='medium')\n",
        "    ax.set_xlabel('Ground truth porosity per image (%)', fontsize=16)\n",
        "    ax.set_ylabel('Found porosity per image (%)', fontsize=16)\n",
        "    ax.set_title(f'Correlation plot of cell porosity of fenestrations detected\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "    ax.grid(True)\n",
        "    ax.tick_params(axis='x', labelsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=12)\n",
        "    # x_ticks = list(range(110, 180+1, 10))\n",
        "    # ax.set_xticks(x_ticks)\n",
        "    # ax.set_yticks(x_ticks)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'corr_porosity_pix.svg'), format='svg')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    print('porosity')\n",
        "    print('gt')\n",
        "    for p in all_gt_porosities_ell:\n",
        "        print(p)\n",
        "    print('s')\n",
        "    for p in all_s_porosities_ell:\n",
        "        print(p)\n",
        "    print('my')\n",
        "    for p in all_my_porosities_ell:\n",
        "        print(p)\n",
        "\n",
        "    # Porosity correlation\n",
        "    all_gt_porosities_ell = np.array(all_gt_porosities_ell)\n",
        "    all_s_porosities_ell = np.array(all_s_porosities_ell)\n",
        "    all_my_porosities_ell =  np.array(all_my_porosities_ell)\n",
        "\n",
        "    sorted_indices = np.argsort(all_gt_porosities_ell)\n",
        "    all_gt_porosities_ell = all_gt_porosities_ell[sorted_indices]\n",
        "    all_s_porosities_ell = all_s_porosities_ell[sorted_indices]\n",
        "    all_my_porosities_ell = all_my_porosities_ell[sorted_indices]\n",
        "\n",
        "    my_corr_coeff = np.corrcoef(all_gt_porosities_ell, all_my_porosities_ell)[0, 1]\n",
        "    my_r_squared = round(my_corr_coeff ** 2, 2)\n",
        "    my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "    s_corr_coeff = np.corrcoef(all_gt_porosities_ell, all_s_porosities_ell)[0, 1]\n",
        "    s_r_squared = round(s_corr_coeff ** 2, 2)\n",
        "    s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    my_coefficients = np.polyfit(all_gt_porosities_ell, all_my_porosities_ell, 1)\n",
        "    my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    s_coefficients = np.polyfit(all_gt_porosities_ell, all_s_porosities_ell, 1)\n",
        "    s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "    s_fitted_values = np.polyval(s_coefficients, all_gt_porosities_ell)\n",
        "    s_residuals = all_s_porosities_ell - s_fitted_values\n",
        "    s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "    my_fitted_values = np.polyval(my_coefficients, all_gt_porosities_ell)\n",
        "    my_residuals = all_my_porosities_ell - my_fitted_values\n",
        "    my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "    s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "    my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "    s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "    my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "    s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "    my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "    s_residuals_sd = round(s_residuals_sd, 2)\n",
        "    my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "    # print('mean values')\n",
        "    # print(f's tangent {s_tg}+-{s_residuals_sd}')\n",
        "    # print(f'my tangent {my_tg}+-{my_residuals_sd}')\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    sns.set_theme()\n",
        "    ax.plot(all_gt_porosities_ell, my_fit_line(all_gt_porosities_ell), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}±{my_residuals_sd}')\n",
        "    ax.plot(all_gt_porosities_ell, s_fit_line(all_gt_porosities_ell), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}±{s_residuals_sd}')\n",
        "    ax.scatter(all_gt_porosities_ell, all_my_porosities_ell, color='g', marker='o', label=f'Automatic: R² = {my_r_squared}', alpha=np.ones_like(all_gt_porosities_ell))\n",
        "    ax.scatter(all_gt_porosities_ell, all_s_porosities_ell, color='b', marker='s', label=f'Semiautomatic: R² = {s_r_squared}', alpha=np.ones_like(all_gt_porosities_ell))\n",
        "\n",
        "    ax.legend(fontsize='medium')\n",
        "    ax.set_xlabel('Ground truth porosity per image (%)', fontsize=16)\n",
        "    ax.set_ylabel('Found porosity per image (%)', fontsize=16)\n",
        "    ax.set_title(f'Correlation plot of cell porosity computed with fitted ellipses\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "    ax.grid(True)\n",
        "    ax.tick_params(axis='x', labelsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=12)\n",
        "    # x_ticks = list(range(110, 180+1, 10))\n",
        "    # ax.set_xticks(x_ticks)\n",
        "    # ax.set_yticks(x_ticks)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'corr_porosity_ell.svg'), format='svg')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# fen freq\n",
        "    print('frequency')\n",
        "    print('gt')\n",
        "    for p in all_gt_frequencies:\n",
        "        print(p)\n",
        "    print('s')\n",
        "    for p in all_s_frequencies:\n",
        "        print(p)\n",
        "    print('my')\n",
        "    for p in all_my_frequencies:\n",
        "        print(p)\n",
        "\n",
        "    # Porosity correlation\n",
        "    all_gt_frequencies = np.array(all_gt_frequencies)\n",
        "    all_s_frequencies = np.array(all_s_frequencies)\n",
        "    all_my_frequencies =  np.array(all_my_frequencies)\n",
        "\n",
        "    sorted_indices = np.argsort(all_gt_frequencies)\n",
        "    all_gt_frequencies = all_gt_frequencies[sorted_indices]\n",
        "    all_s_frequencies = all_s_frequencies[sorted_indices]\n",
        "    all_my_frequencies = all_my_frequencies[sorted_indices]\n",
        "\n",
        "    my_corr_coeff = np.corrcoef(all_gt_frequencies, all_my_frequencies)[0, 1]\n",
        "    my_r_squared = round(my_corr_coeff ** 2, 2)\n",
        "    my_r_squared ='{:.2f}'.format(my_r_squared)\n",
        "\n",
        "    s_corr_coeff = np.corrcoef(all_gt_frequencies, all_s_frequencies)[0, 1]\n",
        "    s_r_squared = round(s_corr_coeff ** 2, 2)\n",
        "    s_r_squared ='{:.2f}'.format(s_r_squared)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    my_coefficients = np.polyfit(all_gt_frequencies, all_my_frequencies, 1)\n",
        "    my_fit_line = np.poly1d(my_coefficients)\n",
        "\n",
        "    # Fit a linear function to the data\n",
        "    s_coefficients = np.polyfit(all_gt_frequencies, all_s_frequencies, 1)\n",
        "    s_fit_line = np.poly1d(s_coefficients)\n",
        "\n",
        "    s_fitted_values = np.polyval(s_coefficients, all_gt_frequencies)\n",
        "    s_residuals = all_s_frequencies - s_fitted_values\n",
        "    s_residuals_sd = np.std(s_residuals)\n",
        "\n",
        "    my_fitted_values = np.polyval(my_coefficients, all_gt_frequencies)\n",
        "    my_residuals = all_my_frequencies - my_fitted_values\n",
        "    my_residuals_sd = np.std(my_residuals)\n",
        "\n",
        "\n",
        "    s_derivative_coefficients = np.polyder(s_coefficients)\n",
        "    my_derivative_coefficients = np.polyder(my_coefficients)\n",
        "\n",
        "    s_tg = np.polyval(s_derivative_coefficients, 5)\n",
        "    my_tg = np.polyval(my_derivative_coefficients, 5)\n",
        "    s_tg = '{:.2f}'.format(round(s_tg, 2))\n",
        "    my_tg = '{:.2f}'.format(round(my_tg, 2))\n",
        "    s_residuals_sd = round(s_residuals_sd, 2)\n",
        "    my_residuals_sd = round(my_residuals_sd, 2)\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    sns.set_theme()\n",
        "    ax.plot(all_gt_frequencies, my_fit_line(all_gt_frequencies), color='g', linestyle='--', alpha=0.6, label=f'Automatic linear fit: s = {my_tg}±{my_residuals_sd}')\n",
        "    ax.plot(all_gt_frequencies, s_fit_line(all_gt_frequencies), color='b', linestyle='--', alpha=0.6, label=f'Semiautomatic linear fit: s = {s_tg}±{s_residuals_sd}')\n",
        "    ax.scatter(all_gt_frequencies, all_my_frequencies, color='g', marker='o', label=f'Automatic: R² = {my_r_squared}', alpha=np.ones_like(all_gt_frequencies))\n",
        "    ax.scatter(all_gt_frequencies, all_s_frequencies, color='b', marker='s', label=f'Semiautomatic: R² = {s_r_squared}', alpha=np.ones_like(all_gt_frequencies))\n",
        "\n",
        "    ax.legend(fontsize='medium')\n",
        "    ax.set_xlabel('Fenestration frequency per image\\n(fenestrations/μm²)', fontsize=16)\n",
        "    ax.set_ylabel('Detected fenestration frequency per image\\n(fenestrations/μm²)', fontsize=16)\n",
        "    ax.set_title(f'Correlation plot of fenestration frequency detected\\nwith the automatic and semiautomatic method', fontsize=22)\n",
        "    ax.grid(True)\n",
        "    ax.tick_params(axis='x', labelsize=12)\n",
        "    ax.tick_params(axis='y', labelsize=12)\n",
        "    # x_ticks = list(range(110, 180+1, 10))\n",
        "    # ax.set_xticks(x_ticks)\n",
        "    # ax.set_yticks(x_ticks)\n",
        "    if save_plots:\n",
        "        plt.savefig(os.path.join(plot_path, 'corr_freq.svg'), format='svg')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #     # if remove_false_fenestrations:\n",
        "    #     #     new_mask_filt = remove_fenestrations(new_mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    #     #     # current_dice_score_filt = compute_dice_score(ground_truth_mask, new_mask_filt)\n",
        "    #     #     # dice_scores_filt.append(current_dice_score_filt)\n",
        "    #     #     # print(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})')\n",
        "    #     #     # file.write(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})\\n')\n",
        "    #     # else:\n",
        "    #     #     # print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "    #     #     # file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "\n",
        "    # # dice_scores = np.array(dice_scores)\n",
        "    # # mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "    # # std_dice = round(np.std(dice_scores)*100, 1)\n",
        "    # # if remove_false_fenestrations:\n",
        "    # #     dice_scores_filt = np.array(dice_scores_filt)\n",
        "    # #     mean_dice_filt = round(np.mean(dice_scores_filt)*100, 1)\n",
        "    # #     std_dice_filt = round(np.std(dice_scores_filt)*100, 1)\n",
        "    # #     print(f'{model_name} Mean dice: {mean_dice} +- {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n')\n",
        "    # #     file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n\\n')\n",
        "    # # else:\n",
        "    # #     print(f'{model_name} Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "    # #     file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U1JDFWEQ6hbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model('vgg11+none', 0.0, 'dice+bce')\n",
        "loaded_state_dict = torch.load('./gdrive/MyDrive/lsecs/vgg11+imagenet_dice+bce_med5.pth')\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "square_section = cv.imread('./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches/II_Y10_04_3_patch_1_0.tif', cv.IMREAD_GRAYSCALE)\n",
        "square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch  dimension\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = torch.sigmoid(model(square_tensor))\n",
        "    # output = (output > 0.5).float()\n",
        "    output = output.cpu().data.numpy().squeeze(0).squeeze()\n",
        "    output = output*255\n",
        "    cv2_imshow(square_section)\n",
        "    cv2_imshow(output)\n",
        "\n",
        "    # # # Forward pass through the model\n",
        "    # # with torch.no_grad():\n",
        "    # #     output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "    # # Scale the probablity to 0-255\n",
        "\n",
        "    # # output = output.to(torch.uint8)\n",
        "    # # output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "    # # cv2_imshow(output_pil)\n",
        "    # output_probs[x:x+window_size, y:y+window_size] += output*weighting_window\n",
        "    # # Crop\n",
        "    # # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    # output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    # weights *= 255\n",
        "    # # weights = weights[:original_height, :original_width]*255\n",
        "    # # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # # Apply weights\n",
        "    # # output_probs /= weights\n",
        "\n",
        "    # # Create image from mask\n",
        "    # # output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    # output_mask = output_mask.astype(np.uint8)\n",
        "    # return output_mask"
      ],
      "metadata": {
        "id": "du6wjpaN760E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define paths to your images\n",
        "image_paths = ['./gdrive/MyDrive/lsecs/dice_score_test/lc/2022-11-10_Pill18_C0_1h_M1_18.tif', './gdrive/MyDrive/lsecs/clahe/p2.tif']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read and normalize images\n",
        "images = []\n",
        "normalized_images = []\n",
        "for path in image_paths:\n",
        "    # Read image with OpenCV in grayscale\n",
        "    img = cv.imread(path, cv.IMREAD_GRAYSCALE)\n",
        "    # Convert image to float32\n",
        "    print(np.mean(img))\n",
        "    images.append(img)\n",
        "    img = normalize_hist(img)\n",
        "    # clahe = cv.createCLAHE(clipLimit=np.mean(img)/10, tileGridSize=(8, 8))\n",
        "    # img = clahe.apply(img)\n",
        "    # img = cv.medianBlur(img, 3)\n",
        "    img = img.astype('float32')\n",
        "    img = (img - 0.5 * 255.0) / (0.5 * 255.0)\n",
        "    print(np.mean(img))\n",
        "    # Normalize image to have mean 0 and std 1\n",
        "    # img /= 255.0  # Scale to range [0, 1]\n",
        "    # img -= np.mean(img)  # Subtract mean\n",
        "    # img /= np.std(img)   # Divide by std\n",
        "    # Append normalized image to the list\n",
        "\n",
        "\n",
        "\n",
        "    normalized_images.append(img)\n",
        "\n",
        "# Plot the images\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
        "\n",
        "for i in range(2):\n",
        "    # Plot original image\n",
        "    # axes[i, 0].imshow(images[i], cmap='gray')\n",
        "    # axes[i, 0].set_title('Original Image')\n",
        "    # axes[i, 0].axis('off')\n",
        "\n",
        "    # # Plot normalized image\n",
        "    # axes[i, 1].imshow(normalized_images[i], cmap='gray', vmin=-1, vmax=1)\n",
        "    # axes[i, 1].set_title('Normalized Image')\n",
        "    # axes[i, 1].axis('off')\n",
        "\n",
        "    axes[i, 2].hist(images[i].flatten(), bins=50, color='blue', alpha=0.5)\n",
        "    axes[i, 2].set_ylabel('Frequency', fontsize='medium')\n",
        "    axes[i, 2].set_xlabel('Intensity value', fontsize='medium')\n",
        "\n",
        "    axes[i, 3].hist(normalized_images[i].flatten(), bins=50, color='blue', alpha=0.5)\n",
        "    axes[i, 3].set_ylabel('Frequency', fontsize='medium')\n",
        "    axes[i, 3].set_xlabel('Intensity value', fontsize='medium')\n",
        "# plot_path = './gdrive/MyDrive/lsecs/plots/'\n",
        "# plt.savefig(os.path.join(plot_path, 'hists.svg'), format='svg')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# for i, ax in enumerate(axes.flat):\n",
        "#     ax.imshow(images[i], cmap='gray', vmin=-1, vmax=1)\n",
        "#     ax.axis('off')\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Br7Tf9tZEdH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d0e19f028b2745698adb0e8c28e58e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_459e80e05d954faeb9c63d2dd2e0b74d",
              "IPY_MODEL_66498da59bb440aca1a2b6e50567650a"
            ],
            "layout": "IPY_MODEL_6b9d67f359504d20bd5e391d36d78810"
          }
        },
        "459e80e05d954faeb9c63d2dd2e0b74d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28ce302d13844beaa3354f1fee187ee0",
            "placeholder": "​",
            "style": "IPY_MODEL_dc8392879dc543f68f91027e54011f52",
            "value": "0.013 MB of 0.013 MB uploaded\r"
          }
        },
        "66498da59bb440aca1a2b6e50567650a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_794263da4acb47478112123408e0ba57",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99e519ec40714b229cb029bc9ab24c88",
            "value": 1
          }
        },
        "6b9d67f359504d20bd5e391d36d78810": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ce302d13844beaa3354f1fee187ee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc8392879dc543f68f91027e54011f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "794263da4acb47478112123408e0ba57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99e519ec40714b229cb029bc9ab24c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}