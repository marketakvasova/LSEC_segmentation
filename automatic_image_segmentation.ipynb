{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDNFLvtqYw7o",
        "outputId": "8e05dec1-68bd-436d-930d-ca9c3332087c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "l!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5QvbqMfiA4o",
        "outputId": "620a8654-55e3-496a-c3fa-e5f226992b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transofrm=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transofrm\n",
        "        self.images = sorted(os.listdir(self.image_dir)) # listdir returns arbitrary order\n",
        "        self.masks = sorted(os.listdir(self.mask_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = np.array(Image.open(img_path).convert('L')) # TODO: only grayscale images\n",
        "        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32) #TODO, ten float asi neni potreba\n",
        "        mask[mask == 255.0] = 1\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_dir, mask_dir, split, batch_size, train_transform, val_transform, num_workers=4, pin_memory=True):\n",
        "    data = MyDataset(\n",
        "        image_dir=img_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        transofrm=None\n",
        "    )\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        range(len(data)),\n",
        "        test_size=split,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_data = TransformDataset(Subset(data, train_indices), train_transform)\n",
        "    val_data = TransformDataset(Subset(data, test_indices), val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_indices\n",
        "\n",
        "train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "    [\n",
        "        A.Rotate(limit=35, p=1.0),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.VerticalFlip(p=0.5),\n",
        "        # A.Affine(shear=(0.5,1)),\n",
        "        # A.Affine(scale=(-10, 10)),\n",
        "        A.Normalize(\n",
        "            mean = 0.0,\n",
        "            std = 1.0,\n",
        "            max_pixel_value=255.0, # normalization to [0, 1]\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "            mean = 0.0,\n",
        "            std = 1.0,\n",
        "            max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# test_transform = A.Compose(\n",
        "#     [\n",
        "#     A.Normalize(\n",
        "#       mean = 0.0,\n",
        "#       std = 1.0,\n",
        "#       max_pixel_value=255.0,\n",
        "#     ),\n",
        "#         ToTensorV2()\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations if needed\n",
        "])\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "    window_size = 512\n",
        "    oh, ow = 124, 124\n",
        "    input_image = cv.imread(image_path, 0)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "    bottom_edge = image_height % (window_size - oh)\n",
        "    right_edge = image_width % (window_size - ow)\n",
        "    mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "    mirrored_image[:image_height, :image_width] = input_image\n",
        "    mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "    mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "    image_height += bottom_edge\n",
        "    image_width += right_edge\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += 1\n",
        "            square_section = preprocess_image(square_section)\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy()\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()\n",
        "    output_probs = output_probs[:original_height, :original_width]\n",
        "    weights = weights[:original_height, :original_width]\n",
        "    output_probs /= weights\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+\".png\"), output_probs)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask\"+\".png\"), output_mask)\n",
        "\n",
        "def inference_on_image(model, image_path, output_folder):\n",
        "    # image = np.array(Image.open(image_path).convert('L'))\n",
        "    # input_size = next(model.parameters()).shape\n",
        "    window_size = 512\n",
        "    input_image = Image.open(image_path).convert('L')\n",
        "    image_width, image_height = input_image.size\n",
        "    output_probs = Image.new(\"L\", (image_width, image_height))\n",
        "    output_mask = Image.new(\"L\", (image_width, image_height))\n",
        "    for y in range(0, image_height, window_size):\n",
        "        for x in range(0, image_width, window_size):\n",
        "    # for y in range(0, 1):\n",
        "        # for x in range(0, 1):\n",
        "            square_section = input_image.crop((x, y, x + window_size, y + window_size))\n",
        "            square_section = preprocess_image(square_section)\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "                mask = (output > 0.5).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output = output.to(torch.uint8)\n",
        "            output_pil = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
        "            output_probs.paste(output_pil, (x, y))\n",
        "\n",
        "            mask_pil = transforms.ToPILImage()(mask.squeeze(0).cpu())\n",
        "            output_mask.paste(mask_pil, (x, y))\n",
        "\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "    output_mask.save(os.path.join(output_folder, filename+\"_mask\"+\".png\"))\n",
        "    output_probs.save(os.path.join(output_folder, filename+\"_probs\"+\".png\"))\n",
        "    # return output_image\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "#     image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "#     mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "#     # rejected_path = os.path.join(output_folder,'rejected')\n",
        "#     # print(image_path)\n",
        "\n",
        "#     if not os.path.exists(output_folder):\n",
        "#         os.makedirs(output_folder)\n",
        "\n",
        "#     if os.path.exists(image_patches_path):\n",
        "#         shutil.rmtree(image_patches_path)\n",
        "#     os.mkdir(image_patches_path)\n",
        "#     if os.path.exists(mask_patches_path):\n",
        "#         shutil.rmtree(mask_patches_path)\n",
        "#     os.mkdir(mask_patches_path)\n",
        "#     # if os.path.exists(rejected_path):\n",
        "#     #     shutil.rmtree(rejected_path)\n",
        "#     # os.mkdir(rejected_path)\n",
        "\n",
        "\n",
        "#     image_filenames = sorted(os.listdir(image_folder))\n",
        "#     mask_filenames = sorted(os.listdir(mask_folder))\n",
        "#     # def cut_and_save_paches(image_name, mask_name)\n",
        "\n",
        "#     for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "#         if image_name.endswith(\".tif\"):\n",
        "#             input_path = os.path.join(image_folder, image_name)\n",
        "#             mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "#             img = Image.open(input_path).convert('L')\n",
        "#             mask = Image.open(mask_path).convert('L')\n",
        "#             width, height = img.size\n",
        "\n",
        "#             for y in range(0, height, patch_size):\n",
        "#                 for x in range(0, width, patch_size):\n",
        "#                     # Define the coordinates of the patch\n",
        "#                     left = x\n",
        "#                     upper = y\n",
        "#                     right = min(x + patch_size, width)\n",
        "#                     lower = min(y + patch_size, height)\n",
        "\n",
        "#                     # Crop the patch from the image\n",
        "#                     image_patch = img.crop((left, upper, right, lower))\n",
        "#                     mask_patch = mask.crop((left, upper, right, lower))\n",
        "\n",
        "#                     patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{y // patch_size}_{x // patch_size}.tif\"\n",
        "\n",
        "#                     image_patch.save(os.path.join(image_patches_path, patch_filename))\n",
        "#                     mask_patch.save(os.path.join(mask_patches_path, patch_filename))\n",
        "#     return image_patches_path, mask_patches_path\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = wavelet_denoise(image)\n",
        "    image = apply_clahe(image)\n",
        "    return image\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "    image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "\n",
        "    image_filenames = sorted(os.listdir(image_folder))\n",
        "    mask_filenames = sorted(os.listdir(mask_folder))\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        if image_name.endswith(\".tif\"):\n",
        "            input_path = os.path.join(image_folder, image_name)\n",
        "            mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "            img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "            mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "            height, width = img.shape\n",
        "            # print(width, height)\n",
        "\n",
        "            shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "            strides = (patch_size * width , patch_size , width, 1)\n",
        "            # strides = (patch_size * width , patch_size)\n",
        "\n",
        "            # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "            #              strides=img.strides + img.strides, writeable=False)\n",
        "            img_strided = as_strided(img, shape=shape,\n",
        "                          strides=strides, writeable=False)\n",
        "            mask_strided = as_strided(mask, shape=shape,\n",
        "                          strides=strides, writeable=False)\n",
        "            # print(img_strided.shape)\n",
        "\n",
        "            for i in range(img_strided.shape[0]):\n",
        "                for j in range(img_strided.shape[1]):\n",
        "                    img_patch = img_strided[i, j]\n",
        "                    mask_patch = img_strided[i, j]\n",
        "\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Denoising\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2 * np.sqrt(data + 3/8)\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    data = (data / 2)**2 - 3/8\n",
        "    return data.astype(np.uint8)\n",
        "\n",
        "def wavelet_denoising(data, wavelet='db1', level=1, threshold_type='soft', sigma=None):\n",
        "    coeffs = pywt.wavedec2(data, wavelet, level=level)\n",
        "    # threshold = sigma * np.sqrt(2 * np.log(len(data))) if sigma is not None else None\n",
        "    threshold = 0.5\n",
        "    #print(threshold)\n",
        "    for i in range(len(coeffs) - 1, 0, -1):\n",
        "        threshold_value = threshold * np.nanmean(np.abs(coeffs[i]))\n",
        "        coeffs[i] = tuple(pywt.threshold(c, threshold_value, threshold_type) for c in coeffs[i])\n",
        "\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "def wavelet_denoise(image):\n",
        "    image_anscombe = anscombe_transform(image)\n",
        "    denoised_image = wavelet_denoising(image_anscombe, wavelet = 'db1', level=3, threshold_type='soft', sigma=1)\n",
        "\n",
        "    denoised_image = inverse_anscombe_transform(denoised_image)\n",
        "    return denoised_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model):#, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(biomodel_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), biomodel_path)\n",
        "\n",
        "def load_checkpoint(checkpoint, model):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "\n",
        "def check_accuracy(loader, model, val_losses, dice_scores, device, loss_fn):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device).unsqueeze(1) # label is grayscale\n",
        "            # preds = torch.softmax(model(x), dim=1)\n",
        "            preds = torch.sigmoid(model(x)) # TODO: vystupy modelu bez sigmoidy jsou zaporny hodnoty\n",
        "            loss = loss_fn(preds, y)\n",
        "            running_loss += loss.cpu()\n",
        "            preds = (preds > 0.5).float()\n",
        "            num_correct += (preds == y).sum()\n",
        "            num_pixels += torch.numel(preds)\n",
        "            dice_score += (2*(preds*y).sum()) / (preds+y).sum() + 1e-8 # this is a better predictor\n",
        "    print(\n",
        "        f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "    )\n",
        "    dice_score = dice_score/len(loader)\n",
        "    print(f\"Dice score is {dice_score}\")\n",
        "    val_losses.append(running_loss/len(loader))\n",
        "    dice_scores.append(dice_score.cpu())\n",
        "    model.train()\n",
        "    return dice_score\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            # output = torch.softmax(model(x), dim=1)\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "def getClassWeights(mask_path, train_indices):\n",
        "    mask_dir_list = sorted(os.listdir(mask_path))\n",
        "    class_count = np.zeros(2, dtype=int)\n",
        "    for i in train_indices:\n",
        "        mask = np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "        mask[mask == 255.0] = 1\n",
        "        class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "        class_count[1] += mask.sum()\n",
        "\n",
        "    n_samples = class_count.sum()\n",
        "    n_classes = 2\n",
        "\n",
        "    class_weights = n_samples / (n_classes * class_count)\n",
        "    return torch.from_numpy(class_weights)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Wavelet filtering debug\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "images = os.listdir(image_folder)\n",
        "image_name = images[0]\n",
        "image = cv.imread(os.path.join(image_folder, image_name), 0)\n",
        "# cv2_imshow(image)\n",
        "\n",
        "denoised_image = wavelet_denoise(image)\n",
        "# cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch):\n",
        "    conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "        nn.BatchNorm2d(out_ch),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return conv\n",
        "\n",
        "#def cropper(og_tensor, target_tensor):\n",
        "#    og_shape = og_tensor.shape[2]\n",
        "#    target_shape = target_tensor.shape[2]\n",
        "#    delta = (og_shape - target_shape) // 2\n",
        "#    cropped_og_tensor = og_tensor[:,:,delta:og_shape-delta,delta:og_shape-delta]\n",
        "#    return cropped_og_tensor\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIOOv16zNmOp"
      },
      "source": [
        "# Training function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z4GQhPsENrxX"
      },
      "outputs": [],
      "source": [
        "def train_fn(loader, model, optimizer, loss_fn, scaler, train_losses):\n",
        "    loop = tqdm(loader)\n",
        "    running_loss = 0\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "    # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data) # TODO: shoud this be wrapped in sigmoid???\n",
        "            loss = loss_fn(torch.sigmoid(predictions), targets)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "\n",
        "        # loss.backward()\n",
        "        # optimizer.step()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # update tqdm loop\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "        # print(batch_idx)\n",
        "    number_of_batches = batch_idx+1\n",
        "    train_losses.append(running_loss/number_of_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Patch creation"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE_NEW_PATCHES = False\n",
        "SAVE_PATCHES_TO_DISK = False\n",
        "# Example usage:\n",
        "\n",
        "image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "mask_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_masks\"\n",
        "\n",
        "if CREATE_NEW_PATCHES:\n",
        "    patch_size = 512  # Define your patch size here\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\" # TODO: save created patches somewhere on disk\n",
        "    else:\n",
        "        output_folder = os.getcwd()\n",
        "    image_patches_path, mask_patches_path = create_image_patches(image_folder, mask_folder, output_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    output_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/patches\"\n",
        "    image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, 'mask_patches')\n"
      ],
      "metadata": {
        "id": "UzznzOTP4s53"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MVM6cZYK0Oy"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4yt1dWbZiNkm",
        "outputId": "0e3965ed-c157-4aa2-f617-1a428e07ac0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-4\n",
        "batch_size = 5\n",
        "num_epochs = 2\n",
        "num_workers = 2\n",
        "pin_memory = False # TODO: check\n",
        "load_model = False\n",
        "\n",
        "# Define dataloaders for training\n",
        "data_split = 0.1\n",
        "\n",
        "train_loader, val_loader, train_indices = get_loaders(\n",
        "    image_patches_path,\n",
        "    mask_patches_path,\n",
        "    data_split,\n",
        "    batch_size,\n",
        "    train_transform,\n",
        "    val_transform,\n",
        "    num_workers,\n",
        "    pin_memory\n",
        ")\n",
        "\n",
        "model = UNET(in_channels=1, out_channels=1, device=DEVICE).to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPuZwg7NzZ_q",
        "outputId": "9ac12457-3afb-4a9a-9c42-4fcc437a050f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/22 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# %%timeit\n",
        "LOAD_MODEL = True\n",
        "if LOAD_MODEL:\n",
        "    load_checkpoint(torch.load(os.path.join(model_folder,\"my_checkpoint.pth.tar\")), model)\n",
        "    check_accuracy(val_loader, model, device=DEVICE)\n",
        "else:\n",
        "    weights = getClassWeights(mask_patches_path, train_indices)\n",
        "    w1 = weights[0]\n",
        "    w2 = weights[1]\n",
        "    # print(w2/w1)\n",
        "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=w2/w1) # cross entropy loss for more than one class(out channels)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    scaler = torch.cuda.amp.GradScaler() # ilastik\n",
        "    # summary(model, (3, 512, 512))\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "    best_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        train_fn(train_loader, model, optimizer, loss_fn, scaler, train_losses)\n",
        "\n",
        "        # save model\n",
        "        checkpoint = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict()\n",
        "        }\n",
        "        # check accuracy\n",
        "        accuracy = check_accuracy(val_loader, model, val_losses, dice_scores, DEVICE, loss_fn)\n",
        "        if accuracy > best_accuracy: # using dice score right now\n",
        "            save_state_dict(model)\n",
        "            # save_checkpoint(checkpoint)\n",
        "        accuracy = max(accuracy, best_accuracy)\n",
        "\n",
        "        # print some examples to a folder\n",
        "        # if(epoch % 5 == 0):\n",
        "        view_prediction(val_loader, model, device = DEVICE)\n",
        "        # save_predictions_as_imgs(val_loader, model, folder=model_folder, device = DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En"
      },
      "outputs": [],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz"
      },
      "outputs": [],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01.tif\"\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "patch_size = 512\n",
        "stride = 20\n",
        "\n",
        "# display(test_image_mask)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}