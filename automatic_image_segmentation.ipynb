{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/automatic_image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXBX4DqRE9h2"
      },
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for training a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECS)."
      ],
      "metadata": {
        "id": "-aHjwiD8IkQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "J-Z80u6TN3Uq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a network, first connect to a GPU (**Runtime -> Change runtime time -> Hardware accelerator -> GPU**).\n",
        "\n",
        "If you are using a pretrained network for inference and not training, being connected only to a **CPU** is slower, but possible."
      ],
      "metadata": {
        "id": "NUZeORlUN_LS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "-gq1-hflPdMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "LHteKyDySYvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0236473-8362-42ce-d1f8-e2de698bb745"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0RgOiEHFZyI"
      },
      "source": [
        "# **1. Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N5QvbqMfiA4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dfd1857-1f81-450b-8471-b4e87b055f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-2.0.1-py2.py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-2.0.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n",
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.3.3-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (0.17.1+cu121)\n",
            "Collecting pretrainedmodels==0.7.4 (from segmentation-models-pytorch)\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting efficientnet-pytorch==0.7.1 (from segmentation-models-pytorch)\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm==0.9.2 (from segmentation-models-pytorch)\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (4.66.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation-models-pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.1+cu121)\n",
            "Collecting munch (from pretrainedmodels==0.7.4->segmentation-models-pytorch)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation-models-pytorch) (0.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.31.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm==0.9.2->segmentation-models-pytorch) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation-models-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=d267a4624b8f870d1ba99f8a09078c6304a3230ef6eaf3f56ff9cdd639116688\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=cd8ba14e72a9d1b5d9e4cc39adcf0fab1f4c3182f5fcd63712c948c59126d46a\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/cb/a5/8f534c60142835bfc889f9a482e4a67e0b817032d9c6883b64\n",
            "Successfully built efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, efficientnet-pytorch, timm, pretrainedmodels, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 munch-4.0.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.3.3 timm-0.9.2\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from numba import njit\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "\n",
        "# gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = False\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om_n1-_pGegM"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M0WZPlvMjs0"
      },
      "source": [
        "## Data utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G5gyUZlsiNvB"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask[mask == 255.0] = 1\n",
        "        mask /= 255\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_train, mask_train, img_val, mask_val, batch_size, num_workers=0, pin_memory=True): # TODO: check these parameters\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_data = MyDataset(\n",
        "        image_dir=img_train,\n",
        "        mask_dir=mask_train,\n",
        "        transform=train_transform\n",
        "    )\n",
        "    val_data = MyDataset(\n",
        "        image_dir=img_val,\n",
        "        mask_dir=mask_val,\n",
        "        transform=val_transform\n",
        "    )\n",
        "\n",
        "    # train_indices, test_indices = train_test_split(\n",
        "    #     range(len(data)),\n",
        "    #     test_size=split,\n",
        "    #     random_state=1\n",
        "    # )\n",
        "    train_data = TransformDataset(train_data, train_transform)\n",
        "    val_data = TransformDataset(val_data, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "        [\n",
        "            # A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # A.Affine(shear=(0.5,1)),\n",
        "            A.Affine(scale=(0.8, 1.2)),\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0, # normalization to [0, 1]\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "    A.Normalize(\n",
        "      mean = 0.0,\n",
        "      std = 1.0,\n",
        "      max_pixel_value=255.0,\n",
        "    ),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "# test_transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     # Add more transformations if needed\n",
        "# ])\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    print('merging masks')\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - border_size:height, :])\n",
        "    bottom_zeros = np.zeros((bottom_edge-border_size, width), dtype = image.dtype)\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border, bottom_zeros))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - border_size:width])\n",
        "    right_zeros = np.zeros((top_bottom_mirrored.shape[0], right_edge-border_size), dtype = image.dtype)\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border, right_zeros))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, filter_type):\n",
        "    window_size = 512\n",
        "    oh, ow = 50, 50\n",
        "    # out_crop =\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    # bottom_edge = (image_height + oh) % (window_size - oh)\n",
        "    # right_edge = (image_height + ow) % (window_size - ow)\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    # print(mirrored_image.shape)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    # tryout = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "            # print(x, y)\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    # if original_height != window_size:\n",
        "                    weighting_window = top_left\n",
        "                    # print('top left')\n",
        "                elif y == image_width - window_size:\n",
        "                    # print('top right')\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "                    # print('top ')\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                    # print('bottom left')\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                    # print('bottom right')\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "                    # print('bottom')\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "                # print('left')\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "                # print('right')\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "                # print('middle')\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "            if filter_type == 'nlm':\n",
        "                square_section = nlm_filt(square_section)\n",
        "            elif filter_type == 'med5':\n",
        "                square_section = cv.medianBlur(square_section, 5) # TODO: prehodit tohle, at se to dela jednou pro celej obrazek, ne pro patche?\n",
        "            square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch and channel dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            # output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # cv2_imshow(output_pil)\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "            # with torch.no_grad():\n",
        "            #     output = torch.sigmoid(model(square_tensor))\n",
        "            #     output = (output > 0.5).float()\n",
        "            #     output = output.cpu().data.numpy().squeeze(0).squeeze()\n",
        "            #     output = output*255.0\n",
        "            #     # cv2_imshow(output)\n",
        "            # # # Forward pass through the model\n",
        "            # # with torch.no_grad():\n",
        "            # #     output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # # Scale the probablity to 0-255\n",
        "\n",
        "            # # output = output.to(torch.uint8)\n",
        "            # # output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # # cv2_imshow(output_pil)\n",
        "            # output_probs[x:x+window_size, y:y+window_size] += output*weighting_window\n",
        "    # Crop\n",
        "    # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    # weights = weights[:original_height, :original_width]*255\n",
        "    # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # Apply weights\n",
        "    # output_probs /= weights\n",
        "\n",
        "    # Create image from mask\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    return output_mask\n",
        "\n",
        "    # filename_ext = os.path.basename(image_path)\n",
        "    # filename, ext = os.path.splitext(filename_ext)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_mirrored\"+ext), mirrored_image)\n",
        "\n",
        "    # # Merge image with created mask\n",
        "    # out_mask_path = os.path.join(output_folder, filename+\"_new_mask\"+ext)\n",
        "    # merge = merge_images(input_image, output_mask)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "    # cv.imwrite(out_mask_path, output_mask)\n",
        "    # # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "    # return out_mask_path\n",
        "\n",
        "# def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "#     window_size = 512\n",
        "#     oh, ow = 124, 124\n",
        "#     input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#     image_height, image_width = input_image.shape\n",
        "#     original_height, original_width = image_height, image_width\n",
        "#     bottom_edge = image_height % (window_size - oh)\n",
        "#     right_edge = image_width % (window_size - ow)\n",
        "#     mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "#     mirrored_image[:image_height, :image_width] = input_image\n",
        "#     mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "#     mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "#     image_height += bottom_edge\n",
        "#     image_width += right_edge\n",
        "#     weights = np.zeros((image_height, image_width))\n",
        "#     # tryout = np.zeros((image_height, image_width))\n",
        "#     output_probs = np.zeros((image_height, image_width))\n",
        "#     output_mask = np.zeros((image_height, image_width))\n",
        "#     middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "#     for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "#         for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "#             # Choose weighting window\n",
        "#             if x == 0:\n",
        "#                 if y == 0:\n",
        "#                     if original_height != window_size:\n",
        "#                         weighting_window = top_left\n",
        "#                     else:\n",
        "#                         weighting_window = np.ones((window_size, window_size))\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = top_right\n",
        "#                 else:\n",
        "#                     weighting_window = top\n",
        "#             elif x == window_size - oh - 1:\n",
        "#                 if y == 0:\n",
        "#                     weighting_window = bottom_left\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = bottom_right\n",
        "#                 else:\n",
        "#                     weighting_window = bottom\n",
        "#             elif y == 0:\n",
        "#                 weighting_window = left\n",
        "#             elif y == window_size - ow - 1:\n",
        "#                 weighting_window = right\n",
        "#             else:\n",
        "#                 weighting_window = middle\n",
        "#             square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "#             weights[x:x + window_size, y:y + window_size] = weighting_window\n",
        "#             # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "#             square_section = preprocess_image(square_section)\n",
        "#             square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "#             # Forward pass through the model\n",
        "#             with torch.no_grad():\n",
        "#                 output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "#             # Scale the probablity to 0-255\n",
        "#             output = output*255\n",
        "#             output = output.to(torch.uint8)\n",
        "#             output_pil = output.squeeze(0).cpu().numpy()\n",
        "#             output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()*weighting_window\n",
        "#     # Crop\n",
        "#     output_probs = output_probs[:original_height, :original_width]\n",
        "#     # weights = weights[:original_height, :original_width]*255\n",
        "#     # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "#     # Apply weights\n",
        "#     # output_probs /= weights\n",
        "\n",
        "#     # Create image from mask\n",
        "#     output_mask = np.where(output_probs > 127, 255, 0)\n",
        "#     output_mask = output_mask.astype(np.uint8)\n",
        "#     filename_ext = os.path.basename(image_path)\n",
        "#     filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "#     # Merge image with created mask\n",
        "#     out_mask_path = os.path.join(output_folder, filename+\"_mask\"+ext)\n",
        "#     merge = merge_images(input_image, output_mask)\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_merge\"+ext), merge)\n",
        "\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "#     cv.imwrite(out_mask_path, output_mask)\n",
        "#     # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "#     return out_mask_path\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    # image = nlm_filt(image)\n",
        "    # image = wavelet_denoise(image, threshold=1.5)\n",
        "    # image = apply_clahe(image)\n",
        "    image = cv.medianBlur(image, 5)\n",
        "    return image\n",
        "\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_train_val_patches(train_image_folder, train_mask_folder, val_image_folder, val_mask_folder, output_folder, patch_size):\n",
        "    train_image_patches_path, train_mask_patches_path = create_image_patches(train_image_folder, train_mask_folder, output_folder, patch_size, img_type='train')\n",
        "    val_image_patches_path, val_mask_patches_path = create_image_patches(val_image_folder, val_mask_folder, output_folder, patch_size, img_type='val')\n",
        "    return train_image_patches_path, train_mask_patches_path, val_image_patches_path, val_mask_patches_path\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size, img_type):\n",
        "    image_patches_path = os.path.join(output_folder, img_type+'_image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder, img_type+'_mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    fenestration_area_thresh = 0.0 #0.01\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        # if image_name.endswith(\".tif\"): # TODO: tohle mozna odstranit\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "        # strides = (patch_size * width , patch_size)\n",
        "\n",
        "        # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "        #              strides=img.strides + img.strides, writeable=False)\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                img_patch = img_strided[i, j]\n",
        "                mask_patch = mask_strided[i, j]\n",
        "                # Compute the percentage of white pixels\n",
        "                fenestration_area = np.sum(mask_patch == 255)\n",
        "                # print(fenestration_area)\n",
        "                # fenestration_percentage = fenestration_area/patch_area\n",
        "                if fenestration_area >= fenestration_area_thresh:\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "                else:\n",
        "                    print(\"not writing patch\")\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLHlKdZ_MnGj"
      },
      "source": [
        "## Training utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dvOsCa6iiNrd"
      },
      "outputs": [],
      "source": [
        "# This is the official implementation of BoundaryDOULoss https://arxiv.org/pdf/2308.00220.pdf\n",
        "# Taken from: https://github.com/sunfan-bvb/BoundaryDoULoss/tree/main\n",
        "class BoundaryDoULoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BoundaryDoULoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i\n",
        "            tensor_list.append(temp_prob.unsqueeze(1))\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _adaptive_size(self, score, target):\n",
        "        kernel = torch.Tensor([[0,1,0], [1,1,1], [0,1,0]])\n",
        "        padding_out = torch.zeros((target.shape[0], target.shape[-2]+2, target.shape[-1]+2))\n",
        "        padding_out[:, 1:-1, 1:-1] = target\n",
        "        h, w = 3, 3\n",
        "\n",
        "        Y = torch.zeros((padding_out.shape[0], padding_out.shape[1] - h + 1, padding_out.shape[2] - w + 1)).cuda()\n",
        "        for i in range(Y.shape[0]):\n",
        "            Y[i, :, :] = torch.conv2d(target[i].unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0).cuda(), padding=1)\n",
        "        Y = Y * target\n",
        "        Y[Y == 5] = 0\n",
        "        C = torch.count_nonzero(Y)\n",
        "        S = torch.count_nonzero(target)\n",
        "        smooth = 1e-5\n",
        "        alpha = 1 - (C + smooth) / (S + smooth)\n",
        "        alpha = 2 * alpha - 1\n",
        "\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        alpha = min(alpha, 0.8)  ## We recommend using a truncated alpha of 0.8, as using truncation gives better results on some datasets and has rarely effect on others.\n",
        "        loss = (z_sum + y_sum - 2 * intersect + smooth) / (z_sum + y_sum - (1 + alpha) * intersect + smooth)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "        target = target.squeeze(1)\n",
        "\n",
        "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            loss += self._adaptive_size(inputs[:, 0], target[:, 0])#(inputs[:, i], target[:, i])\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_dice_score = 0.0\n",
        "    total_samples = 0\n",
        "    eps = 1e-8\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            out = model(x)\n",
        "            loss = get_loss(out, y, loss_fn)\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            if WANDB_CONNECTED or WANDB_LOG:\n",
        "                wandb.log({\"val/batch loss\": loss.item()})\n",
        "\n",
        "            predicted_probs = torch.sigmoid(out)\n",
        "            predicted = (predicted_probs > 0.5).float()\n",
        "            intersection = torch.sum(predicted * y)\n",
        "            dice_score = (2.0 * intersection + eps) / (torch.sum(predicted) + torch.sum(y) + eps)\n",
        "            total_dice_score += dice_score.item() * x.size(0)\n",
        "\n",
        "            total_samples += x.size(0)\n",
        "    model.train()\n",
        "\n",
        "    average_loss = total_loss / total_samples\n",
        "    average_dice_score = total_dice_score / total_samples\n",
        "\n",
        "    return average_loss, average_dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def validate_model(model, loader, loss_fn):\n",
        "#     num_correct = 0\n",
        "#     num_pixels = 0\n",
        "#     dice_score = 0\n",
        "#     model.eval()\n",
        "#     running_loss = 0\n",
        "#     losses = []\n",
        "#     dice_scores = []\n",
        "#     with torch.no_grad():\n",
        "#         for idx, (x, y) in enumerate(loader):\n",
        "#             x = x.to(DEVICE)\n",
        "#             y = y.to(DEVICE).unsqueeze(1)\n",
        "#             # Forward\n",
        "#             preds = model(x)\n",
        "#             loss = get_loss(preds, y, loss_fn)\n",
        "#             # running_loss += loss.cpu()\n",
        "#             losses.append(loss.cpu())\n",
        "\n",
        "#             preds = torch.sigmoid(preds)\n",
        "#             preds = (preds > 0.5).float()\n",
        "\n",
        "#             # num_correct += (preds == y).sum()\n",
        "#             # num_pixels += torch.numel(preds)\n",
        "#             dice_score += (2*(preds*y).sum()) / ((preds+y).sum() + 1e-8) # this is a better predictor\n",
        "#             dice_scores.append(dice_score.cpu())\n",
        "#     # print(\n",
        "#     #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "#     # )\n",
        "#     # dice_score = dice_score/(idx+1)\n",
        "#     # val_loss = running_loss/(idx+1)\n",
        "#     val_loss = np.mean(np.array(losses))\n",
        "#     dice_score = np.mean(np.array(dice_scores))\n",
        "\n",
        "#     # dice_score = dice_score/len(loader)\n",
        "#     # val_loss = running_loss/len(loader) #TODO: not sure this is correct(dividing by batch size?)\n",
        "#     # print(f\"Dice score is {dice_score}\")\n",
        "#     # val_losses.append(running_loss/len(loader))\n",
        "#     # dice_scores.append(dice_score.cpu())\n",
        "#     model.train()\n",
        "#     return val_loss, dice_score\n",
        "\n",
        "\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# def getClassWeights(mask_path, train_indices):\n",
        "#     mask_dir_list = sorted(os.listdir(mask_path))\n",
        "#     class_count = np.zeros(2, dtype=int)\n",
        "#     for i in train_indices:\n",
        "#         mask = cv.imread(os.path.join(mask_path, mask_dir_list[i]), cv.IMREAD_GRAYSCALE) #np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "#         mask[mask == 255.0] = 1\n",
        "#         class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "#         class_count[1] += mask.sum()\n",
        "\n",
        "#     n_samples = class_count.sum()\n",
        "#     n_classes = 2\n",
        "\n",
        "#     class_weights = n_samples / (n_classes * class_count)\n",
        "#     return torch.from_numpy(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FUoJD88eOFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "            else:\n",
        "                ellipses.append(None)\n",
        "        else:\n",
        "            ellipses.append(None)\n",
        "    return ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "    # image = cv.cvtColor(seg_mask, cv.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    # min_contour_area = 10\n",
        "    # filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
        "    # return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    # print(contour_centers)\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01_original_mask.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "fenestration_areas_from_ellipses = []\n",
        "\n",
        "for ellipse in ellipses:\n",
        "    center, axes, angle = ellipse\n",
        "    # center_x, center_y = center\n",
        "    major_axis_length, minor_axis_length = axes\n",
        "    roundness = minor_axis_length/major_axis_length\n",
        "    roundness_of_ellipses.append(roundness)\n",
        "    # rotation_angle = angle\n",
        "    diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "    equivalent_diameters.append(diameter)\n",
        "    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "\n",
        "# show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ],
      "metadata": {
        "id": "BtPrBpQBcsmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b1dc9c-980e-4bf8-d01f-b385a97541c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of fenestrations: 0\n",
            "Number of fitted ellipses: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # Wavelet filtering debug\n",
        "\n",
        "# image_folder = \"./gdrive/MyDrive/ROIs_manually_corrected/train_images\"\n",
        "# images = os.listdir(image_folder)\n",
        "# image_name = images[0]\n",
        "# image = cv.imread(os.path.join(image_folder, image_name), cv.IMREAD_GRAYSCALE)\n",
        "# # cv2_imshow(image)\n",
        "\n",
        "# denoised_image = wavelet_denoise(image)\n",
        "# # cv2_imshow(denoised_image)\n",
        "\n"
      ],
      "metadata": {
        "id": "P9hdx_pYOOjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w8Va0EXGIlq"
      },
      "source": [
        "# U-Net definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mSqH1xk-iNpJ"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as TF\n",
        "\n",
        "\n",
        "def double_conv(in_ch, out_ch, activation):\n",
        "    if activation == 'ReLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    elif activation == 'GeLU':\n",
        "        conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none'),\n",
        "            nn.Conv2d(in_channels=out_ch,out_channels=out_ch,kernel_size=3,stride=1,padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.GeLU(approximate='none')\n",
        "        )\n",
        "    return conv\n",
        "\n",
        "\n",
        "def padder(left_tensor, right_tensor, device: str):\n",
        "  # left_tensor is the tensor on the encoder side of UNET\n",
        "  # right_tensor is the tensor on the decoder side  of the UNET\n",
        "\n",
        "    if left_tensor.shape != right_tensor.shape:\n",
        "        padded = torch.zeros(left_tensor.shape)\n",
        "        padded[:, :, :right_tensor.shape[2], :right_tensor.shape[3]] = right_tensor\n",
        "        return padded.to(device)\n",
        "\n",
        "    return right_tensor.to(device)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, device, dropout_probability, activations, out_activation):\n",
        "        super(UNET, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.device = device\n",
        "        self.dropout = nn.Dropout(p=dropout_probability)\n",
        "        self.activations = activations\n",
        "\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "\n",
        "        self.down_conv_1 = double_conv(in_ch=self.in_channels,out_ch=64, activation=activations)\n",
        "        self.down_conv_2 = double_conv(in_ch=64,out_ch=128, activation=activations)\n",
        "        self.down_conv_3 = double_conv(in_ch=128,out_ch=256, activation=activations)\n",
        "        self.down_conv_4 = double_conv(in_ch=256,out_ch=512, activation=activations)\n",
        "        self.down_conv_5 = double_conv(in_ch=512,out_ch=1024, activation=activations)\n",
        "        #print(self.down_conv_1)\n",
        "\n",
        "        self.up_conv_trans_1 = nn.ConvTranspose2d(in_channels=1024,out_channels=512,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_2 = nn.ConvTranspose2d(in_channels=512,out_channels=256,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_3 = nn.ConvTranspose2d(in_channels=256,out_channels=128,kernel_size=2,stride=2)\n",
        "        self.up_conv_trans_4 = nn.ConvTranspose2d(in_channels=128,out_channels=64,kernel_size=2,stride=2)\n",
        "\n",
        "        self.up_conv_1 = double_conv(in_ch=1024,out_ch=512, activation=activations)\n",
        "        self.up_conv_2 = double_conv(in_ch=512,out_ch=256, activation=activations)\n",
        "        self.up_conv_3 = double_conv(in_ch=256,out_ch=128, activation=activations)\n",
        "        self.up_conv_4 = double_conv(in_ch=128,out_ch=64, activation=activations)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64,out_channels=self.out_channels,kernel_size=1,stride=1)\n",
        "        self.out_activation = out_activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.to(self.device)\n",
        "        x1 = self.down_conv_1(x)\n",
        "        p1 = self.max_pool(x1)\n",
        "        x2 = self.down_conv_2(p1)\n",
        "        p2 = self.max_pool(x2)\n",
        "        p2 = self.dropout(p2)\n",
        "        x3 = self.down_conv_3(p2)\n",
        "        p3 = self.max_pool(x3)\n",
        "        p3 = self.dropout(p3)\n",
        "        x4 = self.down_conv_4(p3)\n",
        "        p4 = self.max_pool(x4)\n",
        "        p4 = self.dropout(p4)\n",
        "        x5 = self.down_conv_5(p4)\n",
        "\n",
        "        # decoding\n",
        "        d1 = self.up_conv_trans_1(x5)  # up transpose convolution (\"up sampling\" as called in UNET paper)\n",
        "        pad1 = padder(x4,d1, self.device) # padding d1 to match x4 shape\n",
        "        cat1 = torch.cat([x4,pad1],dim=1) # concatenating padded d1 and x4 on channel dimension(dim 1) [batch(dim 0),channel(dim 1),height(dim 2),width(dim 3)]\n",
        "        cat1 = self.dropout(cat1)\n",
        "        uc1 = self.up_conv_1(cat1) # 1st up double convolution\n",
        "\n",
        "        d2 = self.up_conv_trans_2(uc1)\n",
        "        pad2 = padder(x3,d2, self.device)\n",
        "        cat2 = torch.cat([x3,pad2],dim=1)\n",
        "        cat2 = self.dropout(cat2)\n",
        "        uc2 = self.up_conv_2(cat2)\n",
        "\n",
        "        d3 = self.up_conv_trans_3(uc2)\n",
        "        pad3 = padder(x2,d3, self.device)\n",
        "        cat3 = torch.cat([x2,pad3],dim=1)\n",
        "        uc3 = self.up_conv_3(cat3)\n",
        "\n",
        "        d4 = self.up_conv_trans_4(uc3)\n",
        "        pad4 = padder(x1,d4, self.device)\n",
        "        cat4 = torch.cat([x1,pad4],dim=1)\n",
        "        uc4 = self.up_conv_4(cat4)\n",
        "\n",
        "        conv_1x1 = self.conv_1x1(uc4)\n",
        "        if self.out_activation == 'sigmoid':\n",
        "            conv_1x1 = torch.sigmoid(conv_1x1)\n",
        "        return conv_1x1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Create training patches**"
      ],
      "metadata": {
        "id": "4YW6LWTd45uB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "#@markdown If you want to create new 512x512 patches, check the following box. If you already have image patches, insert the folders below.\n",
        "\n",
        "\n",
        "# training_images = './gdrive/MyDrive/lsecs/cropped_selections/train_images' #@param {type:\"string\"}\n",
        "# training_masks = './gdrive/MyDrive/lsecs/cropped_selections/train_masks' #@param {type:\"string\"}\n",
        "# validation_images = './gdrive/MyDrive/lsecs/cropped_selections/val_images' #@param {type:\"string\"}\n",
        "# validation_masks = './gdrive/MyDrive/lsecs/cropped_selections/val_masks' #@param {type:\"string\"}\n",
        "training_images = './gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_image_patches' #@param {type:\"string\"}\n",
        "training_masks = './gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_mask_patches' #@param {type:\"string\"}\n",
        "validation_images = './gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches' #@param {type:\"string\"}\n",
        "validation_masks = './gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_mask_patches' #@param {type:\"string\"}\n",
        "\n",
        "create_patches = False # @param {type:\"boolean\"}\n",
        "output_patches_folder = './gdrive/MyDrive/lsecs/cropped_selections/patches_med5' #@param {type:\"string\"}\n",
        "\n",
        "training_images = training_images.strip()\n",
        "training_masks = training_masks.strip()\n",
        "validation_images = validation_images.strip()\n",
        "validation_masks = validation_masks.strip()\n",
        "\n",
        "output_patches_folder = output_patches_folder.strip()\n",
        "\n",
        "if not os.path.exists(training_images):\n",
        "    print(f'{training_images} does not exist.')\n",
        "if not os.path.exists(training_masks):\n",
        "    print(f'{training_masks} does not exist.')\n",
        "if not os.path.exists(validation_images):\n",
        "    print(f'{validation_images} does not exist.')\n",
        "if not os.path.exists(validation_masks):\n",
        "    print(f'{validation_masks} does not exist.')\n"
      ],
      "metadata": {
        "id": "4qR6zmj4U-pC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAVE_PATCHES_TO_DISK = True\n",
        "patch_size = 512\n",
        "\n",
        "if create_patches:\n",
        "    if SAVE_PATCHES_TO_DISK:\n",
        "        # output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "        print(f'Saving patches to {output_patches_folder}')\n",
        "    else:\n",
        "        output_patches_folder = os.getcwd()\n",
        "    train_img_patches_path, train_mask_patches_path, val_img_patches_path, val_mask_patches_path = create_train_val_patches(training_images, training_masks, validation_images, validation_masks, output_patches_folder, patch_size)\n",
        "else: # The patches will be read from disk\n",
        "    train_img_patches_path = training_images\n",
        "    train_mask_patches_path = training_masks\n",
        "    val_img_patches_path = validation_images\n",
        "    val_mask_patches_path = validation_masks\n",
        "    # output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections/patches\"\n",
        "    # image_patches_path = os.path.join(output_folder, 'image_patches')\n",
        "    # mask_patches_path = os.path.join(output_folder, 'mask_patches')\n",
        "\n",
        "print(f'Training image patches are located in {train_img_patches_path}')\n",
        "print(f'Training mask patches are located in {train_mask_patches_path}')\n",
        "print(f'Validation image patches are located in {val_img_patches_path}')\n",
        "print(f'Validation mask patches are located in {val_mask_patches_path}')"
      ],
      "metadata": {
        "id": "UzznzOTP4s53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e55eecf-6f85-44a5-92d5-73e0966c8a41"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training image patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_image_patches\n",
            "Training mask patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_mask_patches\n",
            "Validation image patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches\n",
            "Validation mask patches are located in ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_mask_patches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wandb sweep"
      ],
      "metadata": {
        "id": "WAJp45Xo8p_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def build_optimizer(model, config, beta1=None, beta2=None):\n",
        "    if config.optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(model.parameters(),\n",
        "                              lr=config.learning_rate,\n",
        "                              weight_decay=config.weight_decay,\n",
        "                              momentum=config.momentum)\n",
        "    elif config.optimizer == \"adam\":\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                               lr=config.learning_rate,\n",
        "                            #    betas=(config.beta1, config.beta2),\n",
        "                               weight_decay=config.weight_decay)\n",
        "    return optimizer\n",
        "\n",
        "# TRAIN_LOADER = train_loader\n",
        "# VAL_LOADER = val_loader\n",
        "def build_dataloaders(config): # TODO: check if there is a better way to do this\n",
        "    train_image_patches_path = config.train_image_patches_path\n",
        "    train_mask_patches_path = config.train_mask_patches_path\n",
        "    val_image_patches_path = config.val_image_patches_path\n",
        "    val_mask_patches_path = config.val_mask_patches_path\n",
        "    # image_patches_path = os.path.join(image_patches_path, 'image_patches')\n",
        "    # mask_patches_path = os.path.join(mask_patches_path, 'mask_patches')\n",
        "    train_loader, val_loader = get_loaders(\n",
        "        train_image_patches_path,\n",
        "        train_mask_patches_path,\n",
        "        val_image_patches_path,\n",
        "        val_mask_patches_path,\n",
        "        config.batch_size,\n",
        "        num_workers=0,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    return train_loader, val_loader # this is the simplest way to do it, wandb train cannot take any arguments\n",
        "\n",
        "class EarlyStopper():\n",
        "    def __init__(self, patience):\n",
        "        self.patience = patience\n",
        "        # self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = float('inf')\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > self.min_validation_loss:\n",
        "            self.counter += 1\n",
        "            print(self.counter)\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    running_loss = 0\n",
        "    losses = []\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
        "        data = data.to(device=DEVICE)\n",
        "        targets = targets.unsqueeze(1).to(device=DEVICE)\n",
        "\n",
        "        # forward\n",
        "        with torch.cuda.amp.autocast():\n",
        "            predictions = model(data)\n",
        "            loss = get_loss(predictions, targets, loss_fn)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad() # Zero the gradients\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        # running_loss += loss.item()\n",
        "        # losses.append(loss.item())\n",
        "        total_loss += loss.item() * data.size(0)\n",
        "        total_samples += data.size(0)\n",
        "\n",
        "        if WANDB_CONNECTED or WANDB_LOG:\n",
        "            wandb.log({\"train/batch loss\": loss.item()})\n",
        "\n",
        "    # number_of_batches = batch_idx+1\n",
        "    # mean_loss = np.mean(np.array(losses))\n",
        "    mean_loss = total_loss / total_samples\n",
        "    model.eval()\n",
        "    return mean_loss\n",
        "    # return running_loss/number_of_batches\n",
        "\n",
        "def build_model(model_name, dropout, loss_func):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    if '+' in model_name:\n",
        "        name_parts = model_name.split('+')\n",
        "        encoder = name_parts[-2]\n",
        "        if name_parts[-1] == 'imagenet' or name_parts[-1] == 'ssl':\n",
        "            weights = name_parts[-1]\n",
        "        else:\n",
        "            weights = None\n",
        "    # if loss_func == 'bcelog' or loss_func == 'weighted_bce':\n",
        "    #     out_activation = None\n",
        "    # else:\n",
        "    #     out_activation = 'sigmoid'\n",
        "    out_activation = None\n",
        "    # print(weights)\n",
        "    # print(out_activation)\n",
        "    if model_name == 'plain_unet':\n",
        "        model = UNET(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=out_channels,\n",
        "                device=DEVICE,\n",
        "                dropout_probability=dropout,\n",
        "                activations='ReLU',\n",
        "                out_activation=out_activation).to(DEVICE)\n",
        "    elif 'Unet++' in model_name:\n",
        "        model = smp.UnetPlusPlus(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'Linknet' in model_name:\n",
        "        model = smp.Linknet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'FPN' in model_name:\n",
        "        model = smp.FPN(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    elif 'DeepLabV3' in model_name:\n",
        "        model = smp.DeepLabV3(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    else:\n",
        "        model = smp.Unet(\n",
        "                encoder_name=encoder,\n",
        "                encoder_weights=weights,\n",
        "                in_channels=in_channels,\n",
        "                classes=out_channels,\n",
        "                activation=out_activation,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "def get_loss(pred, target, func_name):\n",
        "    loss_func = None\n",
        "    if func_name == 'dice':\n",
        "        loss_func = smp.losses.DiceLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'bcelog':\n",
        "        loss_func = nn.BCEWithLogitsLoss()\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'jaccard':\n",
        "        loss_func = smp.losses.JaccardLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'weighted_bce':\n",
        "        loss_func = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(4))\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'focal':\n",
        "        loss_func = smp.losses.FocalLoss(mode='binary')\n",
        "        loss = loss_func(pred, target)\n",
        "    elif func_name == 'dice+bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == '40dice+60bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.4*loss1 + 0.6*loss2\n",
        "    elif func_name == '60dice+40bce':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = nn.BCEWithLogitsLoss()\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.6*loss1 + 0.4*loss2\n",
        "    elif func_name == 'dice+focal':\n",
        "        loss_func1 = smp.losses.DiceLoss(mode='binary')\n",
        "        loss1 = loss_func1(pred, target)\n",
        "        loss_func2 = smp.losses.FocalLoss(mode='binary')\n",
        "        loss2 = loss_func2(pred, target)\n",
        "        loss = 0.5*loss1 + 0.5*loss2\n",
        "    elif func_name == 'tversky':\n",
        "        loss_func = smp.losses.TverskyLoss(mode='binary', alpha=0.7, beta=0.3)\n",
        "        loss = loss_func(pred, target)\n",
        "    # elif func_name == 'hausdorff':\n",
        "\n",
        "    return loss\n",
        "\n",
        "def wandb_train(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(config=config):\n",
        "        config = wandb.config\n",
        "\n",
        "        train_loader, val_loader = build_dataloaders(config)\n",
        "        model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "        optimizer = build_optimizer(model, config)\n",
        "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "        # best_dice_score = 0\n",
        "        smallest_val_loss = 1000.0\n",
        "        early_stopper = EarlyStopper(patience=10)\n",
        "        for epoch in range(config.epochs):\n",
        "            print(f'Epoch {epoch}')\n",
        "            avg_loss = train_epoch(model, train_loader, optimizer, config.loss_function)#, loss_fn)\n",
        "            metrics = {\"train/loss\": avg_loss, \"train/epoch\": epoch}\n",
        "            val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            if early_stopper.early_stop(val_loss):\n",
        "                print(f\"early stop on epoch {epoch}\")\n",
        "                with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                    file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "                break\n",
        "\n",
        "            if val_loss < smallest_val_loss:\n",
        "                torch.save(model.state_dict(), os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "            smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "            val_metrics = {\"val/val_loss\": val_loss,\n",
        "                           \"val/dice_score\": dice_score}\n",
        "            wandb.log({**metrics, **val_metrics})\n",
        "\n",
        "class DictObject:\n",
        "    def __init__(self, **entries):\n",
        "        self.__dict__.update(entries)\n",
        "\n",
        "def train(config, model_out_path):\n",
        "    if WANDB_LOG:\n",
        "        wandb.init(\n",
        "            project=\"LSEC_segmentation\",\n",
        "            config=config)\n",
        "        config = wandb.config\n",
        "    else:\n",
        "        config = DictObject(**config)\n",
        "\n",
        "    train_loader, val_loader = build_dataloaders(config)\n",
        "    model = build_model(config.model_type, config.dropout, config.loss_function)\n",
        "    optimizer = build_optimizer(model, config)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "\n",
        "    smallest_val_loss = 1000.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    dice_scores = []\n",
        "\n",
        "    early_stopper = EarlyStopper(patience=10)\n",
        "    for epoch in range(config.num_epochs):\n",
        "        print(f'Epoch {epoch}')\n",
        "        model.train()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, config.loss_function)\n",
        "        train_losses.append(train_loss)\n",
        "        val_loss, dice_score = validate_model(model, val_loader, config.loss_function)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        current_lr = scheduler.get_last_lr()[0]\n",
        "        print(\"Current learning rate:\", current_lr)\n",
        "\n",
        "        if early_stopper.early_stop(val_loss):\n",
        "            print(f\"early stop on epoch {epoch}\")\n",
        "            with open('./gdrive/MyDrive/lsecs/dice_score_test/train_log.txt', \"a+\") as file:\n",
        "                file.write(f'{config.model_type} early stop on epoch {epoch}\\n')\n",
        "            break\n",
        "        if val_loss < smallest_val_loss:\n",
        "            torch.save(model.state_dict(), os.path.join(config.model_path, f'{config.model_type}_{config.loss_function}_{config.image_denoising_methods}.pth'))\n",
        "        smallest_val_loss = min(val_loss, smallest_val_loss)\n",
        "\n",
        "        dice_scores.append(dice_score)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Dice score: {dice_score}')\n",
        "        # view_prediction(val_loader, model, device = DEVICE)\n",
        "        print(train_loss, val_loss)\n",
        "        if WANDB_LOG:\n",
        "            wandb.log({\"train/train_loss\": train_loss,\n",
        "                       \"train/epoch\": epoch,\n",
        "                       \"val/val_loss\": val_loss,\n",
        "                       \"val/dice_score\":dice_score,\n",
        "                       })\n",
        "    if WANDB_LOG:\n",
        "        wandb.finish()\n",
        "\n",
        "    return train_losses, val_losses, dice_scores"
      ],
      "metadata": {
        "id": "c6sxUMdmjwo6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_images = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# train_masks = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# val_images = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "# val_masks = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "\n",
        "# wandb sweep config\n",
        "sweep_config = {\n",
        "    'method': 'grid'#'grid'#\n",
        "    }\n",
        "metric = {\n",
        "    'name': 'val/dice_score',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        # 'values': ['adam', 'sgd']\n",
        "        'value': 'sgd'\n",
        "        },\n",
        "    'learning_rate': {\n",
        "        'value': 0.04,\n",
        "        # a flat distribution between min and max\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.001,\n",
        "        # 'max': 0.01\n",
        "      },\n",
        "    'weight_decay': {\n",
        "        # 'value': 0.0189,\n",
        "        'value': 0.01\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.01,\n",
        "        # 'max' : 0.02,\n",
        "    },\n",
        "    # sgd parameters\n",
        "    'momentum':{\n",
        "        'value': 0.07,\n",
        "        # 'distribution': 'uniform',\n",
        "        # 'min': 0.06,\n",
        "        # 'max' : 0.09,\n",
        "    },\n",
        "\n",
        "    'dropout': {\n",
        "        'value': 0.0,\n",
        "        #   'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "        },\n",
        "    'epochs': {\n",
        "        'value': 200,\n",
        "        },\n",
        "\n",
        "    # Dataloader params\n",
        "    'train_image_patches_path': {\n",
        "        'value': train_img_patches_path\n",
        "        },\n",
        "    'train_mask_patches_path': {\n",
        "        'value': train_mask_patches_path\n",
        "        },\n",
        "    'val_image_patches_path': {\n",
        "        'value': val_img_patches_path\n",
        "        },\n",
        "    'val_mask_patches_path': {\n",
        "        'value': val_mask_patches_path\n",
        "        },\n",
        "    'batch_size': {\n",
        "        'value': 32,\n",
        "        # # integers between min and max\n",
        "        # # with evenly-distributed logarithms\n",
        "        # 'distribution': 'q_log_uniform_values',\n",
        "        # 'q': 2, # the discrete step of the distribution\n",
        "        # 'min': 4,\n",
        "        # 'max': 8,\n",
        "      },\n",
        "    # Adam parameters\n",
        "    # 'beta1': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "    # 'beta2': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'min': 0.95,\n",
        "    #     'max' : 0.999,\n",
        "    # },\n",
        "        # 'fc_layer_size': {\n",
        "    #     'values': [128, 256, 512]\n",
        "    #     },\n",
        "    'image_denoising_methods': {\n",
        "        'value': 'med5',\n",
        "        # 'values': ['no_denoise', 'med5']\n",
        "        # 'values': ['nlm', 'med5']\n",
        "        # 'values': ['clahe+median5', 'med7', 'median5', 'median5+clahe', 'wave1_5+med3', 'wave2_5', 'wave2_5+med5'],#['wavelet', 'wavelet+median', 'advanced median'] # k waveletu jeste pridat ruzne thresholdy\n",
        "    },\n",
        "    'loss_function':{\n",
        "        'value': 'dice+bce',\n",
        "        # 'values': ['dice', 'dice+bce', 'dice+focal', 'tversky'],#['dice', 'bcelog', 'jaccard', 'weighted_bce', 'focal'],#, 'tversky', 'hausdorff']\n",
        "        # 'values': ['dice', 'dice+bce', 'focal','bcelog', 'dice+focal'],\n",
        "        # 'values': ['focal','bcelog', 'dice+focal'],\n",
        "\n",
        "\n",
        "    },\n",
        "    'model_type':{\n",
        "        # 'values': ['plain_unet', 'resnet34+imagenet', 'resnet50+imagenet', 'inceptionv4+imagenet', 'efficientnet-b7+imagenet', 'resnet18+swsl', 'resnet18+imagenet','vgg11+imagenet'], # not great\n",
        "        # 'values': ['vgg11+imagenet', 'vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl','resnet34+imagenet','resnet50+ssl', 'resnext50_32x4d+ssl'], # good\n",
        "        # 'values': ['vgg11+imagenet','vgg13+imagenet', 'vgg16+imagenet', 'vgg19+imagenet',  'resnet18+ssl',  'resnet34+imagenet','resnet50+ssl', 'efficientnet-b7+imagenet'], # the best so far\n",
        "        # 'value': 'vgg11+imagenet',\n",
        "        # 'values':['vgg11+imagenet','vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', 'efficientnet-b7+imagenet'],\n",
        "        # 'values':['vgg13+imagenet', 'resnet18+ssl', 'resnet34+imagenet', [\n",
        "        'values': ['resnet50+imagenet', 'inceptionv4+imagenet']\n",
        "\n",
        "    },\n",
        "    'model_path':{\n",
        "        'value': './gdrive/MyDrive/lsecs',\n",
        "    },\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"LSEC_segmentation\")"
      ],
      "metadata": {
        "id": "Y851Q6gvcd8h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "8d3943eb-77e1-41f5-e095-0ceeb7af5628"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240504_193743-etgung3y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/etgung3y' target=\"_blank\">pious-sweep-1</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/w4m4kkz1' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/w4m4kkz1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/w4m4kkz1' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/w4m4kkz1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/etgung3y' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/etgung3y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: uyetnghj\n",
            "Sweep URL: https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WANDB_CONNECTED = True\n",
        "wandb.agent(sweep_id, wandb_train, count=510)"
      ],
      "metadata": {
        "id": "EXcfFX2wo9P7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e91477dc6d1845d982c8b738bdb50996",
            "f4a4b04d30b74abc80f5671ccc604136",
            "15d74ae8d43e44b68abee90c5c34c33f",
            "78faaa9bc53e4f659c5e46acaa12ca07",
            "e5c359570baa415087038a58287c65df",
            "294d1946c6074bcba974be82f792f399",
            "70e5555926a44635962fcdc28e279a8d",
            "d651390268b4475cabb9c519ac35c474",
            "ecdb778b7b8d42e18714af7663c359f6",
            "817e20e3a60c4975a5f54c542a82c939",
            "4d92ef158f9441dd9dc230b27f6270bf",
            "f3cc425fd231468f990a1bbdf538a7ec",
            "7237da540e73438ebcba8e387a136df0",
            "3a577d6dc92c419fb600e45a818f3ab8",
            "cd8bed1ea499430b83f5428311836c02",
            "f95ce441401e4f5dbe24c336d18e6218"
          ]
        },
        "outputId": "e47b461f-a341-4f5a-c61a-d4970807be29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e91477dc6d1845d982c8b738bdb50996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 68gjaqia with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.04\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: dice+bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: resnet50+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_image_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_image_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_mask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_mask_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_image_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_mask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_mask_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240504_193754-68gjaqia</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/68gjaqia' target=\"_blank\">fluent-sweep-1</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/68gjaqia' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/68gjaqia</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 276MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "1\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "1\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n",
            "Epoch 20\n",
            "1\n",
            "Epoch 21\n",
            "2\n",
            "Epoch 22\n",
            "Epoch 23\n",
            "1\n",
            "Epoch 24\n",
            "Epoch 25\n",
            "Epoch 26\n",
            "1\n",
            "Epoch 27\n",
            "2\n",
            "Epoch 28\n",
            "Epoch 29\n",
            "Epoch 30\n",
            "Epoch 31\n",
            "1\n",
            "Epoch 32\n",
            "2\n",
            "Epoch 33\n",
            "3\n",
            "Epoch 34\n",
            "Epoch 35\n",
            "Epoch 36\n",
            "Epoch 37\n",
            "1\n",
            "Epoch 38\n",
            "2\n",
            "Epoch 39\n",
            "3\n",
            "Epoch 40\n",
            "Epoch 41\n",
            "1\n",
            "Epoch 42\n",
            "2\n",
            "Epoch 43\n",
            "Epoch 44\n",
            "1\n",
            "Epoch 45\n",
            "Epoch 46\n",
            "1\n",
            "Epoch 47\n",
            "2\n",
            "Epoch 48\n",
            "3\n",
            "Epoch 49\n",
            "4\n",
            "Epoch 50\n",
            "5\n",
            "Epoch 51\n",
            "6\n",
            "Epoch 52\n",
            "Epoch 53\n",
            "Epoch 54\n",
            "Epoch 55\n",
            "Epoch 56\n",
            "Epoch 57\n",
            "1\n",
            "Epoch 58\n",
            "Epoch 59\n",
            "Epoch 60\n",
            "1\n",
            "Epoch 61\n",
            "2\n",
            "Epoch 62\n",
            "3\n",
            "Epoch 63\n",
            "Epoch 64\n",
            "1\n",
            "Epoch 65\n",
            "2\n",
            "Epoch 66\n",
            "3\n",
            "Epoch 67\n",
            "4\n",
            "Epoch 68\n",
            "5\n",
            "Epoch 69\n",
            "6\n",
            "Epoch 70\n",
            "7\n",
            "Epoch 71\n",
            "8\n",
            "Epoch 72\n",
            "9\n",
            "Epoch 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-28 (_run_job):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
            "    self._function()\n",
            "  File \"<ipython-input-43-322ae4488cb8>\", line 203, in wandb_train\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 3460, in __exit__\n",
            "    self._finish(exit_code=exit_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 2068, in _finish\n",
            "    hook.call()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 457, in _jupyter_teardown\n",
            "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
            "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/agents/pyagent.py\", line 313, in _run_job\n",
            "    wandb.finish(exit_code=1)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 4189, in finish\n",
            "    wandb.run.finish(exit_code=exit_code, quiet=quiet)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 420, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 361, in wrapper\n",
            "    return func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 2055, in finish\n",
            "    return self._finish(exit_code, quiet)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py\", line 2068, in _finish\n",
            "    hook.call()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\", line 457, in _jupyter_teardown\n",
            "    ipython.display_pub.publish = ipython.display_pub._orig_publish\n",
            "AttributeError: 'ZMQDisplayPublisher' object has no attribute '_orig_publish'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "early stop on epoch 73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w457v4ex with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 200\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_denoising_methods: med5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.04\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_function: dice+bce\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_path: ./gdrive/MyDrive/lsecs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_type: inceptionv4+imagenet\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.07\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_image_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_image_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_mask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/train_mask_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_image_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tval_mask_patches_path: ./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_mask_patches\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.01\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240504_201800-w457v4ex</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dpd/LSEC_segmentation/runs/w457v4ex' target=\"_blank\">sleek-sweep-2</a></strong> to <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/sweeps/uyetnghj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/w457v4ex' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/w457v4ex</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\" to /root/.cache/torch/hub/checkpoints/inceptionv4-8e4777a0.pth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 1348, in do_open\n",
            "    h.request(req.get_method(), req.selector, req.data, headers,\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1283, in request\n",
            "    self._send_request(method, url, body, headers, encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1329, in _send_request\n",
            "    self.endheaders(body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/lib/python3.10/http/client.py\", line 1455, in connect\n",
            "    self.sock = self._context.wrap_socket(self.sock,\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
            "    return self.sslsocket_class._create(\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1100, in _create\n",
            "    self.do_handshake()\n",
            "  File \"/usr/lib/python3.10/ssl.py\", line 1371, in do_handshake\n",
            "    self._sslobj.do_handshake()\n",
            "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-43-322ae4488cb8>\", line 207, in wandb_train\n",
            "    model = build_model(config.model_type, config.dropout, config.loss_function)\n",
            "  File \"<ipython-input-43-322ae4488cb8>\", line 145, in build_model\n",
            "    model = smp.Unet(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/decoders/unet/model.py\", line 71, in __init__\n",
            "    self.encoder = get_encoder(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/__init__.py\", line 85, in get_encoder\n",
            "    encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 766, in load_state_dict_from_url\n",
            "    download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 620, in download_url_to_file\n",
            "    u = urlopen(req)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
            "    response = meth(req, response)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
            "    response = self.parent.error(\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 557, in error\n",
            "    result = self._call_chain(*args)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 749, in http_error_302\n",
            "    return self.parent.open(new, timeout=req.timeout)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 519, in open\n",
            "    response = self._open(req, data)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 536, in _open\n",
            "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 1391, in https_open\n",
            "    return self.do_open(http.client.HTTPSConnection, req,\n",
            "  File \"/usr/lib/python3.10/urllib/request.py\", line 1351, in do_open\n",
            "    raise URLError(err)\n",
            "urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecdb778b7b8d42e18714af7663c359f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sleek-sweep-2</strong> at: <a href='https://wandb.ai/dpd/LSEC_segmentation/runs/w457v4ex' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation/runs/w457v4ex</a><br/> View project at: <a href='https://wandb.ai/dpd/LSEC_segmentation' target=\"_blank\">https://wandb.ai/dpd/LSEC_segmentation</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240504_201800-w457v4ex/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run w457v4ex errored:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 1348, in do_open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     h.request(req.get_method(), req.selector, req.data, headers,\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 1283, in request\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._send_request(method, url, body, headers, encode_chunked)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 1329, in _send_request\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.endheaders(body, encode_chunked=encode_chunked)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 1278, in endheaders\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 1038, in _send_output\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.send(msg)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 976, in send\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.connect()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/http/client.py\", line 1455, in connect\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.sock = self._context.wrap_socket(self.sock,\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/ssl.py\", line 513, in wrap_socket\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.sslsocket_class._create(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/ssl.py\", line 1100, in _create\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.do_handshake()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/ssl.py\", line 1371, in do_handshake\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._sslobj.do_handshake()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m During handling of the above exception, another exception occurred:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-43-322ae4488cb8>\", line 207, in wandb_train\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = build_model(config.model_type, config.dropout, config.loss_function)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"<ipython-input-43-322ae4488cb8>\", line 145, in build_model\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model = smp.Unet(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/decoders/unet/model.py\", line 71, in __init__\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.encoder = get_encoder(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/segmentation_models_pytorch/encoders/__init__.py\", line 85, in get_encoder\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     encoder.load_state_dict(model_zoo.load_url(settings[\"url\"]))\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 766, in load_state_dict_from_url\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/torch/hub.py\", line 620, in download_url_to_file\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     u = urlopen(req)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 216, in urlopen\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return opener.open(url, data, timeout)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 525, in open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     response = meth(req, response)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 634, in http_response\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     response = self.parent.error(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 557, in error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = self._call_chain(*args)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = func(*args)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 749, in http_error_302\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.parent.open(new, timeout=req.timeout)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 519, in open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     response = self._open(req, data)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 536, in _open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = self._call_chain(self.handle_open, protocol, protocol +\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 496, in _call_chain\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = func(*args)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 1391, in https_open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.do_open(http.client.HTTPSConnection, req,\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/lib/python3.10/urllib/request.py\", line 1351, in do_open\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise URLError(err)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Training**"
      ],
      "metadata": {
        "id": "KPwZ2wIG8htJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = False\n",
        "WANDB_CONNECTED = True\n",
        "WANDB_LOG = True\n",
        "# image_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\" # + patches_image_denoising_methods\n",
        "# mask_patches_path = \"./gdrive/MyDrive/lsecs/cropped_selections\"\n",
        "model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg11_dice+bce_no_pre_checkpoint.pth\")\n",
        "config = {\n",
        "    'batch_size' : 32,\n",
        "    'dropout' : 0.0,\n",
        "    'optimizer' : 'adam',\n",
        "    'num_epochs' : 200,\n",
        "    'learning_rate' : 0.02,\n",
        "    'weight_decay' : 0.01,\n",
        "    'momentum' : 0.07,\n",
        "    'train_image_patches_path': train_img_patches_path,\n",
        "    'train_mask_patches_path': train_mask_patches_path,\n",
        "    'val_image_patches_path': val_img_patches_path,\n",
        "    'val_mask_patches_path': val_mask_patches_path,\n",
        "    'image_denoising_methods': 'none',#'nlm'\n",
        "    'loss_function': 'dice+bce',\n",
        "    'model_type': 'vgg11+imagenet',\n",
        "    'model_path':'./gdrive/MyDrive/lsecs',\n",
        "}\n"
      ],
      "metadata": {
        "id": "-0Al6T1fdX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "    model = build_model(config['model_type'], config['dropout'], config['loss_function'])\n",
        "    model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activations = 'ReLU', out_activation=None).to(DEVICE)\n",
        "    load_state_dict(model, model_path)\n",
        "else:\n",
        "    # WANDB_LOG = False\n",
        "    train_losses, val_losses, dice_scores = train(config, model_path)"
      ],
      "metadata": {
        "id": "IaxEOPPRbMjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjg9JWmLAo9"
      },
      "source": [
        "# Training evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ay9PlVUxpq0",
        "outputId": "5bc22a16-0dfb-4d9f-e9b9-e18edafdb528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABxCElEQVR4nO3deZyN5f/H8dc5Z/YVM2NmMIx937JM+IVK2VJKRSlrtKB8tdFiq2hR+YaQijYRCd9sIWQrsoTs69gGg5lhmO2c+/fHcHSaMQZj7lnez8fjPDrnOtd935/bOc2857qv+74thmEYiIiIiBQQVrMLEBEREclJCjciIiJSoCjciIiISIGicCMiIiIFisKNiIiIFCgKNyIiIlKgKNyIiIhIgaJwIyIiIgWKwo2IiIgUKAo3ki9169aNyMjIG1p26NChWCyWnC0ojzl48CAWi4UpU6bk+rYtFgtDhw51vp4yZQoWi4WDBw9ec9nIyEi6deuWo/XczHdF8rfly5djsVhYvny52aVILlO4kRxlsViy9dAPG/M9//zzWCwW9u7de9U+r7/+OhaLhS1btuRiZdfv2LFjDB06lM2bN5tditPlgDlq1CizS8mW6OhonnnmGSIjI/H09KR48eK0b9+e1atXm12ai27dumXrZ0xOh2TJX9zMLkAKlm+++cbl9ddff83ixYsztFetWvWmtjNp0iQcDscNLfvGG28wcODAm9p+QdC5c2fGjBnD1KlTGTx4cKZ9vv/+e2rWrEmtWrVueDtPPvkknTp1wtPT84bXcS3Hjh1j2LBhREZGUqdOHZf3bua7UlisXr2aNm3aAPDUU09RrVo1YmJimDJlCnfccQf//e9/6devn8lVpnv66adp0aKF8/WBAwcYPHgwvXv35o477nC2ly9fnqioKC5evIiHh4cZpYqJFG4kRz3xxBMur3///XcWL16cof3fLly4gI+PT7a34+7ufkP1Abi5ueHmpq9+VFQUFSpU4Pvvv8803Kxdu5YDBw7w7rvv3tR2bDYbNpvtptZxM27mu1IYnD17locffhhvb29Wr15N+fLlne8NGDCAli1b0r9/f+rVq0fjxo1zra6kpCQ8PDywWl0PMDRq1IhGjRo5X//5558MHjyYRo0aZfpzxsvL65bXKnmPDktJrmvevDk1atRgw4YNNG3aFB8fH1577TUA5syZQ9u2bSlRogSenp6UL1+et956C7vd7rKOf8+j+OchgM8++4zy5cvj6elJgwYNWL9+vcuymc25sVgs9O3bl9mzZ1OjRg08PT2pXr06CxcuzFD/8uXLqV+/Pl5eXpQvX56JEydmex7PypUreeSRRyhdujSenp5ERETwn//8h4sXL2bYPz8/P44ePUr79u3x8/MjJCSEl156KcO/RVxcHN26dSMwMJAiRYrQtWtX4uLirlkLpI/e7Ny5k40bN2Z4b+rUqVgsFh577DFSUlIYPHgw9erVIzAwEF9fX+644w6WLVt2zW1kNufGMAzefvttSpUqhY+PD3feeSd///13hmXPnDnDSy+9RM2aNfHz8yMgIIDWrVvz119/OfssX76cBg0aANC9e3fnYYnL840ym3OTmJjIiy++SEREBJ6enlSuXJlRo0ZhGIZLv+v5XtyokydP0rNnT0JDQ/Hy8qJ27dp89dVXGfpNmzaNevXq4e/vT0BAADVr1uS///2v8/3U1FSGDRtGxYoV8fLyIigoiP/7v/9j8eLFWW5/4sSJxMTE8MEHH7gEGwBvb2+++uorLBYLw4cPB9LDhMViybTGRYsWYbFY+Pnnn51tR48epUePHoSGhjr//b788kuX5S7PjZk2bRpvvPEGJUuWxMfHh4SEhGv/A2Yhszk3l3/+bNmyhWbNmuHj40OFChWYOXMmACtWrCAqKgpvb28qV67MkiVLMqw3O/sk5tKfr2KK06dP07p1azp16sQTTzxBaGgokP6L0M/PjwEDBuDn58evv/7K4MGDSUhI4IMPPrjmeqdOncq5c+d4+umnsVgsvP/++zz00EPs37//mn/Br1q1ilmzZvHcc8/h7+/PJ598QocOHYiOjiYoKAiATZs20apVK8LDwxk2bBh2u53hw4cTEhKSrf2eMWMGFy5c4NlnnyUoKIh169YxZswYjhw5wowZM1z62u12WrZsSVRUFKNGjWLJkiV8+OGHlC9fnmeffRZIDwkPPPAAq1at4plnnqFq1ar89NNPdO3aNVv1dO7cmWHDhjF16lRuu+02l23/8MMP3HHHHZQuXZrY2Fg+//xzHnvsMXr16sW5c+f44osvaNmyJevWrctwKOhaBg8ezNtvv02bNm1o06YNGzdu5N577yUlJcWl3/79+5k9ezaPPPIIZcuW5cSJE0ycOJFmzZqxfft2SpQoQdWqVRk+fHiGQxNXG2UwDIP777+fZcuW0bNnT+rUqcOiRYt4+eWXOXr0KB9//LFL/+x8L27UxYsXad68OXv37qVv376ULVuWGTNm0K1bN+Li4njhhRcAWLx4MY899hh333037733HgA7duxg9erVzj5Dhw5l5MiRPPXUUzRs2JCEhAT+/PNPNm7cyD333HPVGv73v//h5eXFo48+mun7ZcuW5f/+7//49ddfuXjxIvXr16dcuXL88MMPGb5n06dPp2jRorRs2RKAEydOcPvttztDYkhICAsWLKBnz54kJCTQv39/l+XfeustPDw8eOmll0hOTr5lh5POnj3LfffdR6dOnXjkkUcYP348nTp14rvvvqN///4888wzPP7443zwwQc8/PDDHD58GH9//xvaJzGJIXIL9enTx/j316xZs2YGYEyYMCFD/wsXLmRoe/rppw0fHx8jKSnJ2da1a1ejTJkyztcHDhwwACMoKMg4c+aMs33OnDkGYPzvf/9ztg0ZMiRDTYDh4eFh7N2719n2119/GYAxZswYZ1u7du0MHx8f4+jRo862PXv2GG5ubhnWmZnM9m/kyJGGxWIxDh065LJ/gDF8+HCXvnXr1jXq1avnfD179mwDMN5//31nW1pamnHHHXcYgDF58uRr1tSgQQOjVKlSht1ud7YtXLjQAIyJEyc615mcnOyy3NmzZ43Q0FCjR48eLu2AMWTIEOfryZMnG4Bx4MABwzAM4+TJk4aHh4fRtm1bw+FwOPu99tprBmB07drV2ZaUlORSl2Gkf9aenp4u/zbr16+/6v7++7ty+d/s7bffdun38MMPGxaLxeU7kN3vRWYufyc/+OCDq/YZPXq0ARjffvutsy0lJcVo1KiR4efnZyQkJBiGYRgvvPCCERAQYKSlpV11XbVr1zbatm2bZU2ZKVKkiFG7du0s+zz//PMGYGzZssUwDMMYNGiQ4e7u7vL/WnJyslGkSBGX70PPnj2N8PBwIzY21mV9nTp1MgIDA53/PyxbtswAjHLlymX6/0hWsvrsL6932bJlzrbLP3+mTp3qbNu5c6cBGFar1fj999+d7YsWLcqw7uzuk5hLh6XEFJ6ennTv3j1Du7e3t/P5uXPniI2N5Y477uDChQvs3Lnzmuvt2LEjRYsWdb6+/Ff8/v37r7lsixYtXIbla9WqRUBAgHNZu93OkiVLaN++PSVKlHD2q1ChAq1bt77m+sF1/xITE4mNjaVx48YYhsGmTZsy9H/mmWdcXt9xxx0u+zJ//nzc3NycIzmQPsfleiZ/PvHEExw5coTffvvN2TZ16lQ8PDx45JFHnOu8/Fe0w+HgzJkzpKWlUb9+/UwPaWVlyZIlpKSk0K9fP5dDeZn9xevp6emcc2G32zl9+jR+fn5Urlz5urd72fz587HZbDz//PMu7S+++CKGYbBgwQKX9mt9L27G/PnzCQsL47HHHnO2ubu78/zzz3P+/HlWrFgBQJEiRUhMTMzyEFORIkX4+++/2bNnz3XVcO7cOeeoxNVcfv/yYaKOHTuSmprKrFmznH1++eUX4uLi6NixI5A+Qvbjjz/Srl07DMMgNjbW+WjZsiXx8fEZPsOuXbu6/D9yq/j5+dGpUyfn68qVK1OkSBGqVq1KVFSUs/3y88uf9Y3sk5hD4UZMUbJkyUyHnP/++28efPBBAgMDCQgIICQkxDlJMD4+/prrLV26tMvry0Hn7Nmz173s5eUvL3vy5EkuXrxIhQoVMvTLrC0z0dHRdOvWjWLFijnn0TRr1gzIuH9eXl4ZDnf9sx6AQ4cOER4ejp+fn0u/ypUrZ6segE6dOmGz2Zg6dSqQPpHzp59+onXr1i5B8auvvqJWrVrO+RwhISHMmzcvW5/LPx06dAiAihUrurSHhIS4bA/Sg9THH39MxYoV8fT0JDg4mJCQELZs2XLd2/3n9kuUKJHhF/rlM/gu13fZtb4XN+PQoUNUrFgxw6TZf9fy3HPPUalSJVq3bk2pUqXo0aNHhnk/w4cPJy4ujkqVKlGzZk1efvnlbJ3C7+/vz7lz57Lsc/n9y/9mtWvXpkqVKkyfPt3ZZ/r06QQHB3PXXXcBcOrUKeLi4vjss88ICQlxeVz+w+bkyZMu2ylbtuw1680JpUqVyjBHLjAwkIiIiAxtcOXnx43sk5hDc27EFJn9dRYXF0ezZs0ICAhg+PDhlC9fHi8vLzZu3Mirr76ardN5r3ZWjvGviaI5vWx22O127rnnHs6cOcOrr75KlSpV8PX15ejRo3Tr1i3D/uXWGUbFixfnnnvu4ccff2TcuHH873//49y5c3Tu3NnZ59tvv6Vbt260b9+el19+meLFi2Oz2Rg5ciT79u27ZbWNGDGCN998kx49evDWW29RrFgxrFYr/fv3z7XTu2/19yI7ihcvzubNm1m0aBELFixgwYIFTJ48mS5dujgn9jZt2pR9+/YxZ84cfvnlFz7//HM+/vhjJkyYwFNPPXXVdVetWpVNmzaRnJx81dP1t2zZgru7u0sg7dixI++88w6xsbH4+/szd+5cHnvsMeeZiJc/nyeeeOKqc8D+fYmB3Bi1gat/ptf6rG9kn8QcCjeSZyxfvpzTp08za9YsmjZt6mw/cOCAiVVdUbx4cby8vDK96F1WF8K7bOvWrezevZuvvvqKLl26ONuvdTZLVsqUKcPSpUs5f/68y+jNrl27rms9nTt3ZuHChSxYsICpU6cSEBBAu3btnO/PnDmTcuXKMWvWLJe/eIcMGXJDNQPs2bOHcuXKOdtPnTqVYTRk5syZ3HnnnXzxxRcu7XFxcQQHBztfX88Vp8uUKcOSJUsyHI65fNjzcn25oUyZMmzZsgWHw+EyepNZLR4eHrRr14527drhcDh47rnnmDhxIm+++aZz5LBYsWJ0796d7t27c/78eZo2bcrQoUOzDDf33Xcfa9euZcaMGZmeSn3w4EFWrlxJixYtXMJHx44dGTZsGD/++COhoaEkJCS4HOoJCQnB398fu93ucl2a/Kwg7lNBpcNSkmdc/qvpn38Rp6Sk8Omnn5pVkgubzUaLFi2YPXs2x44dc7bv3bs3wzyNqy0PrvtnGIbL6bzXq02bNqSlpTF+/Hhnm91uZ8yYMde1nvbt2+Pj48Onn37KggULeOihh1yuD5JZ7X/88Qdr16697ppbtGiBu7s7Y8aMcVnf6NGjM/S12WwZRkhmzJjB0aNHXdp8fX0BsnUKfJs2bbDb7YwdO9al/eOPP8ZisWR7/lROaNOmDTExMS6Hd9LS0hgzZgx+fn7OQ5anT592Wc5qtTpHCJKTkzPt4+fnR4UKFZzvX83TTz9N8eLFefnllzPMI0pKSqJ79+4YhpHhWkhVq1alZs2aTJ8+nenTpxMeHu7yR4nNZqNDhw78+OOPbNu2LcN2T506lWVdeVFB3KeCSiM3kmc0btyYokWL0rVrV+etAb755ptcHf6/lqFDh/LLL7/QpEkTnn32WecvyRo1alzz0v9VqlShfPnyvPTSSxw9epSAgAB+/PHHm5q70a5dO5o0acLAgQM5ePAg1apVY9asWdc9H8XPz4/27ds7593885AUpP91P2vWLB588EHatm3LgQMHmDBhAtWqVeP8+fPXta3L1+sZOXIk9913H23atGHTpk0sWLDAZTTm8naHDx9O9+7dady4MVu3buW7775zGfGB9KvRFilShAkTJuDv74+vry9RUVGZzuFo164dd955J6+//joHDx6kdu3a/PLLL8yZM4f+/ftnuNbLzVq6dClJSUkZ2tu3b0/v3r2ZOHEi3bp1Y8OGDURGRjJz5kxWr17N6NGjnSNLTz31FGfOnOGuu+6iVKlSHDp0iDFjxlCnTh3n/Jxq1arRvHlz6tWrR7Fixfjzzz+ZOXMmffv2zbK+oKAgZs6cSdu2bbntttsyXKF47969/Pe//8301PqOHTsyePBgvLy86NmzZ4a5Q++++y7Lli0jKiqKXr16Ua1aNc6cOcPGjRtZsmQJZ86cudF/VtMUxH0qkHL79CwpXK52Knj16tUz7b969Wrj9ttvN7y9vY0SJUoYr7zyivN0zH+eznm1U8EzO+2Wf52afLVTwfv06ZNh2TJlyricmmwYhrF06VKjbt26hoeHh1G+fHnj888/N1588UXDy8vrKv8KV2zfvt1o0aKF4efnZwQHBxu9evVynlr8z9NNu3btavj6+mZYPrPaT58+bTz55JNGQECAERgYaDz55JPGpk2bsn0q+GXz5s0zACM8PDzD6dcOh8MYMWKEUaZMGcPT09OoW7eu8fPPP2f4HAzj2qeCG4Zh2O12Y9iwYUZ4eLjh7e1tNG/e3Ni2bVuGf++kpCTjxRdfdPZr0qSJsXbtWqNZs2ZGs2bNXLY7Z84co1q1as7T8i/ve2Y1njt3zvjPf/5jlChRwnB3dzcqVqxofPDBBy6npl/el+x+L/7t8nfyao9vvvnGMAzDOHHihNG9e3cjODjY8PDwMGrWrJnhc5s5c6Zx7733GsWLFzc8PDyM0qVLG08//bRx/PhxZ5+3337baNiwoVGkSBHD29vbqFKlivHOO+8YKSkpWdb5z3p79epllC5d2nB3dzeCg4ON+++/31i5cuVVl9mzZ49zf1atWpVpnxMnThh9+vQxIiIiDHd3dyMsLMy4++67jc8++8zZ5/Ip2zNmzMhWrf90I6eCZ/bzp0yZMpmeSp/ZdyA7+yTmshhGHvqzWCSfat++/Q2dhisiIjlPc25ErtO/b5WwZ88e5s+fT/Pmzc0pSEREXGjkRuQ6hYeH061bN8qVK8ehQ4cYP348ycnJbNq0KcO1W0REJPdpQrHIdWrVqhXff/89MTExeHp60qhRI0aMGKFgIyKSR2jkRkRERAoUzbkRERGRAkXhRkRERAqUQjfnxuFwcOzYMfz9/a/rku0iIiJiHsMwOHfuHCVKlMhwwch/K3Th5tixYxnu/CoiIiL5w+HDhylVqlSWfQpduLl8OfPDhw8TEBBgcjUiIiKSHQkJCURERLjc8PZqCl24uXwoKiAgQOFGREQkn8nOlBJNKBYREZECReFGREREChSFGxERESlQCt2cGxERuXl2u53U1FSzy5ACxsPD45qneWeHwo2IiGSbYRjExMQQFxdndilSAFmtVsqWLYuHh8dNrUfhRkREsu1ysClevDg+Pj66GKrkmMsX2T1+/DilS5e+qe+Wwo2IiGSL3W53BpugoCCzy5ECKCQkhGPHjpGWloa7u/sNr0cTikVEJFsuz7Hx8fExuRIpqC4fjrLb7Te1HoUbERG5LjoUJbdKTn23FG5ERESkQFG4ERERuU6RkZGMHj062/2XL1+OxWLRWWa5ROFGREQKLIvFkuVj6NChN7Te9evX07t372z3b9y4McePHycwMPCGtpddClHpdLZUDjoef5H4i6lUCdMNOUVE8oLjx487n0+fPp3Bgweza9cuZ5ufn5/zuWEY2O123Nyu/asxJCTkuurw8PAgLCzsupaRG6eRmxyyYOtxmr6/jEGztmIYhtnliIgIEBYW5nwEBgZisVicr3fu3Im/vz8LFiygXr16eHp6smrVKvbt28cDDzxAaGgofn5+NGjQgCVLlris99+HpSwWC59//jkPPvggPj4+VKxYkblz5zrf//eIypQpUyhSpAiLFi2iatWq+Pn50apVK5cwlpaWxvPPP0+RIkUICgri1VdfpWvXrrRv3/6G/z3Onj1Lly5dKFq0KD4+PrRu3Zo9e/Y43z906BDt2rWjaNGi+Pr6Ur16debPn+9ctnPnzoSEhODt7U3FihWZPHnyDddyKync5JB6kUWxWCxsio5j7f7TZpcjInLLGYbBhZQ0Ux45+UfkwIEDeffdd9mxYwe1atXi/PnztGnThqVLl7Jp0yZatWpFu3btiI6OznI9w4YN49FHH2XLli20adOGzp07c+bMmav2v3DhAqNGjeKbb77ht99+Izo6mpdeesn5/nvvvcd3333H5MmTWb16NQkJCcyePfum9rVbt278+eefzJ07l7Vr12IYBm3atHGe5t+nTx+Sk5P57bff2Lp1K++9955zdOvNN99k+/btLFiwgB07djB+/HiCg4Nvqp5bRYelckhxfy86NYjg67WHGLdsL43L580PXEQkp1xMtVNt8CJTtr19eEt8PHLmV9jw4cO55557nK+LFStG7dq1na/feustfvrpJ+bOnUvfvn2vup5u3brx2GOPATBixAg++eQT1q1bR6tWrTLtn5qayoQJEyhfvjwAffv2Zfjw4c73x4wZw6BBg3jwwQcBGDt2rHMU5Ubs2bOHuXPnsnr1aho3bgzAd999R0REBLNnz+aRRx4hOjqaDh06ULNmTQDKlSvnXD46Opq6detSv359IH30Kq/SyE0O6t20HG5WC6v3nmZT9FmzyxERkWy4/Mv6svPnz/PSSy9RtWpVihQpgp+fHzt27LjmyE2tWrWcz319fQkICODkyZNX7e/j4+MMNgDh4eHO/vHx8Zw4cYKGDRs637fZbNSrV++69u2fduzYgZubG1FRUc62oKAgKleuzI4dOwB4/vnnefvtt2nSpAlDhgxhy5Ytzr7PPvss06ZNo06dOrzyyiusWbPmhmu51TRyk4NKFfWhfd2SzNxwhHHL9vF51/rXXkhEJJ/ydrexfXhL07adU3x9fV1ev/TSSyxevJhRo0ZRoUIFvL29efjhh0lJSclyPf++XYDFYsHhcFxXf7PnbD711FO0bNmSefPm8csvvzBy5Eg+/PBD+vXrR+vWrTl06BDz589n8eLF3H333fTp04dRo0aZWnNmNHKTw55tXh6LBZbsOMHOmASzyxERuWUsFgs+Hm6mPG7lVZJXr15Nt27dePDBB6lZsyZhYWEcPHjwlm0vM4GBgYSGhrJ+/Xpnm91uZ+PGjTe8zqpVq5KWlsYff/zhbDt9+jS7du2iWrVqzraIiAieeeYZZs2axYsvvsikSZOc74WEhNC1a1e+/fZbRo8ezWeffXbD9dxKGrnJYeVD/GhTI5x5W48zbtk+xjxW1+ySRETkOlSsWJFZs2bRrl07LBYLb775ZpYjMLdKv379GDlyJBUqVKBKlSqMGTOGs2fPZivYbd26FX9/f+dri8VC7dq1eeCBB+jVqxcTJ07E39+fgQMHUrJkSR544AEA+vfvT+vWralUqRJnz55l2bJlVK1aFYDBgwdTr149qlevTnJyMj///LPzvbxG4eYWeO7O8szbepx5W44x4J5KlA32vfZCIiKSJ3z00Uf06NGDxo0bExwczKuvvkpCQu6PxL/66qvExMTQpUsXbDYbvXv3pmXLlths1z4k17RpU5fXNpuNtLQ0Jk+ezAsvvMB9991HSkoKTZs2Zf78+c5DZHa7nT59+nDkyBECAgJo1aoVH3/8MZB+rZ5BgwZx8OBBvL29ueOOO5g2bVrO73gOsBhmH+DLZQkJCQQGBhIfH09AwK272F6PKev5dedJOtaP4L2Ha117ARGRPC4pKYkDBw5QtmxZvLy8zC6n0HE4HFStWpVHH32Ut956y+xybomsvmPX8/tbc25ukT53ps+An7XpCMfiLppcjYiI5DeHDh1i0qRJ7N69m61bt/Lss89y4MABHn/8cbNLy/MUbm6RemWKcXu5YqTaDT77bb/Z5YiISD5jtVqZMmUKDRo0oEmTJmzdupUlS5bk2XkueYnm3NxCfe+syO/7/2Da+mj63lWBYD9Ps0sSEZF8IiIigtWrV5tdRr6kkZtbqEmFIGqXCiQp1cEXqw6YXY6IiEihoHBzC1ksFvrcWQGAb9YeIv5iqskViYiIFHwKN7dYi6qhVAr143xyGl+vOWh2OSIiIgVengg348aNIzIyEi8vL6Kioli3bt1V+zZv3hyLxZLh0bZt21ysOPus1iujN1+uPsCFlDSTKxIRESnYTA8306dPZ8CAAQwZMoSNGzdSu3ZtWrZsedWbjc2aNYvjx487H9u2bcNms/HII4/kcuWZOLkTjv+VobltzXDKBPlw9kIqU//I+sZrIiIicnNMDzcfffQRvXr1onv37lSrVo0JEybg4+PDl19+mWn/YsWKERYW5nwsXrwYHx8f88PNtlkwvjHM6QsOu8tbbjYrzzRLv+7NpJX7SU6zZ7YGERERyQGmhpuUlBQ2bNhAixYtnG1Wq5UWLVqwdu3abK3jiy++oFOnThnu6npZcnIyCQkJLo9bomxT8PCDmC2w+bsMbz90W0nCArw4kZDMjxuO3poaRETklmjevDn9+/d3vo6MjGT06NFZLmOxWJg9e/ZNbzun1lOYmBpuYmNjsdvthIaGurSHhoYSExNzzeXXrVvHtm3beOqpp67aZ+TIkQQGBjofERERN113pnyDodkr6c+XDock1xDl6Wajd9NyAExYsY80e+7fhE1EpLBp164drVq1yvS9lStXYrFY2LJly3Wvd/369fTu3ftmy3MxdOhQ6tSpk6H9+PHjtG7dOke39W9TpkyhSJEit3Qbucn0w1I344svvqBmzZo0bNjwqn0GDRpEfHy883H48OFbV1DD3hBUARJPwcpRGd7u1DCCYr4eRJ+5wP+2HLt1dYiICAA9e/Zk8eLFHDlyJMN7kydPpn79+tSqdf33/wsJCcHHxycnSrymsLAwPD11EdjrYWq4CQ4OxmazceLECZf2EydOEBYWluWyiYmJTJs2jZ49e2bZz9PTk4CAAJfHLePmAS1HpD//fTyccb3tgo+HGz3/rywAny7bh8NRqO5ZKiKS6+677z5CQkKYMmWKS/v58+eZMWMGPXv25PTp0zz22GOULFkSHx8fatasyffff5/lev99WGrPnj00bdoULy8vqlWrxuLFizMs8+qrr1KpUiV8fHwoV64cb775Jqmp6dc/mzJlCsOGDeOvv/5yngV8ueZ/H5baunUrd911F97e3gQFBdG7d2/Onz/vfL9bt260b9+eUaNGER4eTlBQEH369HFu60ZER0fzwAMP4OfnR0BAAI8++qjL7+6//vqLO++8E39/fwICAqhXrx5//vknkH6PrHbt2lG0aFF8fX2pXr068+fPv+FassPUcOPh4UG9evVYunSps83hcLB06VIaNWqU5bIzZswgOTmZJ5544laXeX0q3gvl7wZ7CvzyZoa3n2xUBn9PN/acPM8v209ksgIRkXzCMCAl0ZyHkb0/Dt3c3OjSpQtTpkzB+McyM2bMwG6389hjj5GUlES9evWYN28e27Zto3fv3jz55JNZXpbknxwOBw899BAeHh788ccfTJgwgVdffTVDP39/f6ZMmcL27dv573//y6RJk/j4448B6NixIy+++CLVq1d3ng3csWPHDOtITEykZcuWFC1alPXr1zNjxgyWLFlC3759XfotW7aMffv2sWzZMr766iumTJmSIeBll8Ph4IEHHuDMmTOsWLGCxYsXs3//fpf6OnfuTKlSpVi/fj0bNmxg4MCBuLu7A9CnTx+Sk5P57bff2Lp1K++99x5+fn43VEt2mX5vqQEDBtC1a1fq169Pw4YNGT16NImJiXTv3h2ALl26ULJkSUaOHOmy3BdffEH79u0JCgoyo+yrs1jSR2/GN4adP8P+5VCuufPtAC93ujQuw7hl+xi3bC8tq4disVhMK1dE5IalXoARJczZ9mvHwCPzE0n+rUePHnzwwQesWLGC5s2bA+mHpDp06OCcj/nSSy85+/fr149Fixbxww8/ZDnt4bIlS5awc+dOFi1aRIkS6f8eI0aMyDBP5o033nA+j4yM5KWXXmLatGm88soreHt74+fnh5ubW5ZHLqZOnUpSUhJff/2180SasWPH0q5dO9577z3nHNaiRYsyduxYbDYbVapUoW3btixdupRevXpl69/sn5YuXcrWrVs5cOCAc97q119/TfXq1Vm/fj0NGjQgOjqal19+mSpVqgBQsWJF5/LR0dF06NCBmjVrAlCuXLnrruF6mT7npmPHjowaNYrBgwdTp04dNm/ezMKFC50fUHR0NMePH3dZZteuXaxateqah6RMU7wKNLg0yXnhILC7XrivR5OyeLvb2Ho0npV7Yk0oUESk8KhSpQqNGzd2XmJk7969rFy50vk7xG6389Zbb1GzZk2KFSuGn58fixYtIjo6e9cl27FjBxEREc5gA2R69GH69Ok0adKEsLAw/Pz8eOONN7K9jX9uq3bt2i5nCDdp0gSHw8GuXbucbdWrV8dmszlfh4eHX/X6cdnZZkREhMsJOdWqVaNIkSLs2LEDSB+oeOqpp2jRogXvvvsu+/btc/Z9/vnnefvtt2nSpAlDhgy5oQnc18v0kRuAvn37ZhhSu2z58uUZ2ipXruwyvJgnNR8IW3+Ak9th45QrYQcI8vPksYal+XL1AcYu20vTSiHm1SkicqPcfdJHUMza9nXo2bMn/fr1Y9y4cUyePJny5cvTrFkzAD744AP++9//Mnr0aGrWrImvry/9+/cnJSUlx8pdu3YtnTt3ZtiwYbRs2ZLAwECmTZvGhx9+mGPb+KfLh4Qus1gsOBy37izdoUOH8vjjjzNv3jwWLFjAkCFDmDZtGg8++CBPPfUULVu2ZN68efzyyy+MHDmSDz/8kH79+t2yekwfuSmwfIpB89fSn//6Dlw86/J2r6ZlcbdZWHfgDOsPnjGhQBGRm2SxpB8aMuNxnYfzH330UaxWK1OnTuXrr7+mR48ezikBq1ev5oEHHuCJJ56gdu3alCtXjt27d2d73VWrVuXw4cMuRxl+//13lz5r1qyhTJkyvP7669SvX5+KFSty6NAhlz4eHh7Y7Vlf5LVq1ar89ddfJCYmOttWr16N1WqlcuXK2a75elzev3+ebbx9+3bi4uKoVq2as61SpUr85z//4ZdffuGhhx5i8uTJzvciIiJ45plnmDVrFi+++CKTJk26JbVepnBzK9XvASFV4eIZWPG+y1vhgd48XK8UAOOW7TWjOhGRQsPPz4+OHTsyaNAgjh8/Trdu3ZzvVaxYkcWLF7NmzRp27NjB008/neEs3qy0aNGCSpUq0bVrV/766y9WrlzJ66+/7tKnYsWKREdHM23aNPbt28cnn3zCTz/95NInMjKSAwcOsHnzZmJjY0lOTs6wrc6dO+Pl5UXXrl3Ztm0by5Yto1+/fjz55JMZrhl3vex2O5s3b3Z57NixgxYtWlCzZk06d+7Mxo0bWbduHV26dKFZs2bUr1+fixcv0rdvX5YvX86hQ4dYvXo169evp2rVqgD079+fRYsWceDAATZu3MiyZcuc790qCje3ks0NWl06NXzdZ3DK9S+BZ5qVx2qB5btOse1ovAkFiogUHj179uTs2bO0bNnSZX7MG2+8wW233UbLli1p3rw5YWFhtG/fPtvrtVqt/PTTT1y8eJGGDRvy1FNP8c4777j0uf/++/nPf/5D3759qVOnDmvWrOHNN13PqO3QoQOtWrXizjvvJCQkJNPT0X18fFi0aBFnzpyhQYMGPPzww9x9992MHTv2+v4xMnH+/Hnq1q3r8mjXrh0Wi4U5c+ZQtGhRmjZtSosWLShXrhzTp08HwGazcfr0abp06UKlSpV49NFHad26NcOGDQPSQ1OfPn2oWrUqrVq1olKlSnz66ac3XW9WLEaen7ySsxISEggMDCQ+Pv7WXvPmn6Z2gt0LoMI98MRMl7demLaJOZuP0bpGGOOfqJc79YiI3ICkpCQOHDhA2bJl8fLyMrscKYCy+o5dz+9vjdzkhpbvgNUd9i6GPa4XdnqueQUAFv4dw96T58yoTkREpEBRuMkNQeUh6un054teA/uVq0RWDvPn3mqhGAZ8unzfVVYgIiIi2aVwk1uavQI+wRC7G9Z/7vJWnzvTR2/mbD7G4TMXzKhORESkwFC4yS1egXD3pcljy0dC4mnnW7UjinBHxWDsDoOJv2n0RkRE5GYo3OSmuk9CaE1IiodlrjPpL4/e/PDnEU4mJJlRnYhIthSy81AkF+XUd0vhJjdZbdD63fTnGybDib+db0WVLUa9MkVJSXMwaeX+q6xARMQ8l696e+GCDp/LrXH5qtD/vHXEjcgTt18oVCL/D6reDzvmpt93qsscuHR7+753VqD7lPV890c0zzWvQFFfD7OrFRFxstlsFClSxHmPIh8fH934V3KMw+Hg1KlT+Pj44OZ2c/FE4cYM974FuxfBgRWwaz5UaQtA88ohVAsPYPvxBCavOciAeyqZXKiIiKvLd6y+0ZswimTFarVSunTpmw7NuoifWZYOh5UfQtGy0OcPcPMEYN6W4/SZupEALzdWD7wLfy/3a6xIRCT32e12UlNTr91R5Dp4eHhgtWY+Y+Z6fn9r5MYs/zcANn0HZw/A7+Ph//oD0KpGGOVCfNl/KpHv/ojmmWblza1TRCQTNpvtpudFiNwqmlBsFk8/aDEk/flvo+B8+hCvzWrh2UuB5vOVB0hKzfoOsSIiIuJK4cZMtTpBidsg5Vz6YapL2tctScki3sSeT2b6+sNZrEBERET+TeHGTFYrtLp0avimb+H4XwC426w806wcABNX7CMlzWFWhSIiIvmOwo3ZSkdBzUcAAxYMhEvzux+pH0GwnyfH4pOYvfmouTWKiIjkIwo3eUGLoeDmDdFr4O+fAPByt9HrjrIATFi+D7ujUJ3UJiIicsMUbvKCwFLOs6VYPBhSLwLQ+fYyBHq7sz82kQXbjptXn4iISD6icJNXNH4eAkpB/GFYMxYAP083ujWOBGDcsn26n4uIiEg2KNzkFR4+cM+w9OerPoKEYwB0bxKJr4eNHccTWLZLVwQVERG5FoWbvKRGB4i4HVIvwJL0oFPEx4Mnbi8DwNhf92r0RkRE5BoUbvISiwVajUx/vmUaHPkTgJ7/VxYPNysbo+NYu/+0iQWKiIjkfQo3eU3J26BO5/TnC14Fh4PiAV50rB8BwKfL9plYnIiISN6ncJMX3T0YPPzg6J+wdQYATzcrh5vVwqq9sWw+HGdufSIiInmYwk1e5B8GdwxIf75kKKQkUqqoDw/UKQmkz70RERGRzCnc5FW394EiZeDcMVg1GoDn7iyPxQJLdpxgZ0yCufWJiIjkUQo3eZW7F9z7dvrzNZ9AXDTlQ/xoUyMc0NwbERGRq1G4ycuqtoPIOyAtKf3KxcCzzcsD8POWYxyMTTSzOhERkTxJ4SYvu3xquMWafs+pQ2uoUTKQOyuH4DBgwgqN3oiIiPybwk1eF1YTbuuS/nzhQHA46HtXBQB+3HiEY3EXTSxOREQk71G4yQ/ufAM8A+D4X7D5O+qVKUZU2WKk2g0++22/2dWJiIjkKQo3+YFfCDR7Nf350uGQlOAcvZm2PprY88kmFiciIpK3KNzkFw17Q1AFSDwJKz/k/yoEU7tUIEmpDr5cdcDs6kRERPIMhZv8ws0D7n0n/fnvn2I5e4Dn7kwfvflm7SHiL6aaWJyIiEjeoXCTn1RqCeXvAnsK/PIm91QNpVKoH+eS0/hm7UGzqxMREckTFG7yE4sFWo4Eiw12/oz14G/0uTR688WqA1xISTO5QBEREfMp3OQ3xatAg57pzxcOom31EEoX8+HshVSm/hFtbm0iIiJ5gMJNftR8EHgXhZN/47b5G+dViyet3E9ymt3k4kRERMylcJMf+RSD5q+lP//1bR6q6ktYgBcnEpL5ccNRc2sTERExmcJNflW/B4RUgYtn8Fw9il5NywHpt2RIsztMLk5ERMQ8Cjf5lc0NWo5If77uMx4vd5Fivh5En7nAz1uOm1ubiIiIiRRu8rMKd0OlVuBIw3vZEHo0iQRg3LK9OByGubWJiIiYROEmv7v3HbC6w55f6B66D39PN/acPM8v20+YXZmIiIgpTA8348aNIzIyEi8vL6Kioli3bl2W/ePi4ujTpw/h4eF4enpSqVIl5s+fn0vV5kHBFSDqaQB8l71Jt9tLAvDp8r0YhkZvRESk8DE13EyfPp0BAwYwZMgQNm7cSO3atWnZsiUnT57MtH9KSgr33HMPBw8eZObMmezatYtJkyZRsmTJXK48j2n2CvgEQ+xunvFZhpe7lS1H4lm5J9bsykRERHKdqeHmo48+olevXnTv3p1q1aoxYcIEfHx8+PLLLzPt/+WXX3LmzBlmz55NkyZNiIyMpFmzZtSuXTuXK89jvALhrjcA8F37AT1uCwDS596IiIgUNqaFm5SUFDZs2ECLFi2uFGO10qJFC9auXZvpMnPnzqVRo0b06dOH0NBQatSowYgRI7Dbr37huuTkZBISElweBdJtXSC0JiTF04cfcLdZ+OPAGf48eMbsykRERHKVaeEmNjYWu91OaGioS3toaCgxMTGZLrN//35mzpyJ3W5n/vz5vPnmm3z44Ye8/fbbV93OyJEjCQwMdD4iIiJydD/yDKsNWo0EwHfL1zxXLQWAsRq9ERGRQsb0CcXXw+FwULx4cT777DPq1atHx44def3115kwYcJVlxk0aBDx8fHOx+HDh3Ox4lxW9g6oej8YDp65OAmrxWD5rlNsOxpvdmUiIiK5xrRwExwcjM1m48QJ11OWT5w4QVhYWKbLhIeHU6lSJWw2m7OtatWqxMTEkJKSkukynp6eBAQEuDwKtHvfApsn3kdWMajcASD9zCkREZHCwrRw4+HhQb169Vi6dKmzzeFwsHTpUho1apTpMk2aNGHv3r04HFduL7B7927Cw8Px8PC45TXnC0UjoVEfALqem4QHqSzYFsPek+fMrUtERCSXmHpYasCAAUyaNImvvvqKHTt28Oyzz5KYmEj37t0B6NKlC4MGDXL2f/bZZzlz5gwvvPACu3fvZt68eYwYMYI+ffqYtQt50x0DwC8Uj4RDvFNiNYYB45fvN7sqERGRXGFquOnYsSOjRo1i8ODB1KlTh82bN7Nw4ULnJOPo6GiOH79yn6SIiAgWLVrE+vXrqVWrFs8//zwvvPACAwcONGsX8iZPf7h7CAAPnZtKMPHM3nyUw2cumFyYiIjIrWcxCtllbBMSEggMDCQ+Pr5gz79xOODzu+DYJpb5tqb76Sd54vbSvN2+ptmViYiIXLfr+f2dr86WkutgtUKr9wBonriQ6paD/PDnEU4mJJlcmIiIyK2lcFOQlY6CGg9jweADv6mkpNn5fNUBs6sSERG5pRRuCrp7hoGbN9VSt9HG+gff/n6Is4mZnzYvIiJSECjcFHSBpaDJCwAM8ZqGPeUiU9YcNLcmERGRW0jhpjBo8gIElCTUcZKnbPOZsuYg55PTzK5KRETkllC4KQw8fOCe4QD0dZ+L18UTfPv7IZOLEhERuTUUbgqLGh0gIgpvknjFfRqfrzxAUurV76YuIiKSXyncFBYWi/Ou4R1sqyiV+Dc//FmAbyIqIiKFlsJNYVKyHtR+HIDB7l8zcfk+Uu2OaywkIiKSvyjcFDZ3D8Zw9+U2617qn1vC7E1Hza5IREQkRyncFDYB4ViavgjAQPdpTF72N3ZHoboDh4iIFHAKN4XR7X1wFClDuOUMLeOnsWDb8WsvIyIikk8o3BRG7l5Y730LgKdtPzNjyVoK2f1TRUSkAFO4Kayq3k9qRGO8LKk8fPYzlu06aXZFIiIiOULhprCyWHBv8x4GFtrZfmfJwjkavRERkQJB4aYwC69FUs0nAOh05lN+3xdrckEiIiI3T+GmkPNuOYQkqy+1rAfYOm+82eWIiIjcNIWbws4vhKTGLwHw4JnP2bJPVy0WEZH8TeFGKNK8L6c8ShFiiefo3LfNLkdEROSmKNwIuHmQ2iL91PC74mayb9cWkwsSERG5cQo3AkCJBg+y3ac+npY0zv/vNbPLERERuWEKN5LOYsG9zbukGVZqn19JzOZFZlckIiJyQxRuxKlijQYsD2iX/mLBq5CaZG5BIiIiN0DhRlwE3TeUWCOAsOQDnJj1itnliIiIXDeFG3FRt3I5ZpV+A4DQHV+R+NcckysSERG5Pgo3ksHjT/Tke/f26S/m9MGI07VvREQk/1C4kQz8PN2o3vkD/nKUx9dxjtivuoA9zeyyREREskXhRjJVK7I42xp/TILhTcjZjZxZ8JbZJYmIiGSLwo1c1WP3NuWroP8AUOTP/5KyZ7m5BYmIiGSDwo1cldVqoWO3F5hluRsrBkk/9IRE3TlcRETyNoUbyVLxAC+KdfiIPY6SBKTGcvrbHmAYZpclIiJyVQo3ck3Na0SyuPq7JBvuBB1fwbnl/zW7JBERkatSuJFs6fFQWz7z7QWA94q3cBzZZHJFIiIimVO4kWzxcrfRuusgFhkNcSONc989AUkJZpclIiKSgcKNZFuF0ADO3fsRR4xgAi8e4eyMvpp/IyIieY7CjVyXDo1r8H3EENIMK0X3zSHpz2/MLklERMSFwo1cF4vFQu/HH+Nz98fTX89/GU7tNrkqERGRKxRu5LoF+rhTr/MwVjlq4GkkEf/NE5CaZHZZIiIigMKN3KAGZYPZfvsHnDICCEzYxbm5A80uSUREBFC4kZvQo+XtTCz2CgD+WyeT9vf/TK5IRERE4UZugpvNSveuTzGZ+wFInfUcxB02uSoRESnsFG7kppQs4k3Yg2+z2VEeb3sCCd91BXua2WWJiEghpnAjN6117TL8UnUECYY3Aac2cHHJO2aXJCIihZjCjeSIvh1a8F/vPgB4rv0YY/9ycwsSEZFCK0+Em3HjxhEZGYmXlxdRUVGsW7fuqn2nTJmCxWJxeXh5eeVitZIZHw83OnR5gR8cd2HF4OL0pyAx1uyyRESkEDI93EyfPp0BAwYwZMgQNm7cSO3atWnZsiUnT5686jIBAQEcP37c+Th06FAuVixXU61EAEkt3mGPoyQ+yac4P+0pcDjMLktERAoZ08PNRx99RK9evejevTvVqlVjwoQJ+Pj48OWXX151GYvFQlhYmPMRGhqaixVLVp68oypflxpCkuGO3+FlpK4ea3ZJIiJSyJgablJSUtiwYQMtWrRwtlmtVlq0aMHatWuvutz58+cpU6YMERERPPDAA/z9999X7ZucnExCQoLLQ24di8VC/8fb85GtR/rrX4fB0Q0mVyUiIoWJqeEmNjYWu92eYeQlNDSUmJiYTJepXLkyX375JXPmzOHbb7/F4XDQuHFjjhw5kmn/kSNHEhgY6HxERETk+H6IqyA/T5p2epkF9oa4GWlcmNoVkhQqRUQkd5h+WOp6NWrUiC5dulCnTh2aNWvGrFmzCAkJYeLEiZn2HzRoEPHx8c7H4cO6yFxu+L9KIexs+DZHjGB8Eg9z4afnwTDMLktERAoBU8NNcHAwNpuNEydOuLSfOHGCsLCwbK3D3d2dunXrsnfv3kzf9/T0JCAgwOUhuaNP6wZ8UmQgaYYVn10/4dj0rdkliYhIIWBquPHw8KBevXosXbrU2eZwOFi6dCmNGjXK1jrsdjtbt24lPDz8VpUpN8jDzUqfLo8zxugIgP3nl+DULpOrEhGRgs70w1IDBgxg0qRJfPXVV+zYsYNnn32WxMREunfvDkCXLl0YNGiQs//w4cP55Zdf2L9/Pxs3buSJJ57g0KFDPPXUU2btgmShTJAvZR54jZX2Grg7krgwtQukXjS7LBERKcDczC6gY8eOnDp1isGDBxMTE0OdOnVYuHChc5JxdHQ0VuuVDHb27Fl69epFTEwMRYsWpV69eqxZs4Zq1aqZtQtyDQ/VK83gncOpsrsbIWd3kjz/NTwf+NjsskREpICyGEbhmuWZkJBAYGAg8fHxmn+Ti84lpTL04zF8mDwMAOPRb7BUu9/kqkREJL+4nt/fph+WksLB38udJ5/owUR7OwBSZ/WBuGiTqxIRkYJI4UZyTZ2IInDnG2xyVMAjLYGL07qDPc3sskREpIBRuJFc1at5Zb4q8SYJhjfeMX+S9usIs0sSEZECRuFGcpXVamHQ46142/oMALbVH8H+FSZXJSIiBYnCjeS60AAvWj76LFPT7sSCQfIPPeH8KbPLEhGRAkLhRkxxd9VQDtR/k92OkngmnSL5x6fB4TC7LBERKQAUbsQ0L7atw4eBg0gy3PE8sBTH2rFmlyQiIgWAwo2YxsvdxktPtGek0TW9YckwOLrB3KJERCTfU7gRU1UM9adym+eZZ2+I1UgjeVo3SEowuywREcnHFG7EdI9FlWZJ+Tc4YgTjeS6a1DkvQOG6cLaIiOQghRsxncViYcijjRjm8SJphhX3HbNg07dmlyUiIvmUwo3kCUV8POj1eCc+tD8KQNq8l+DULpOrEhGR/EjhRvKMhmWL4d70P/xmr4mbPYmUaV0h9aLZZYmISD6jcCN5yvN3V+Lr0IGcMgLwOL0D+8LXzC5JRETyGYUbyVPcbFaGdr6L1y39ALBt+BK2zzG5KhERyU8UbiTPKVXUhwceepLxae0ASJvdF+KiTa5KRETyC4UbyZPa1grncJ0BbHJUwC0lgdQfeoA91eyyREQkH1C4kTzrjftrMcr/FRIMb9yPrcdYNtLskkREJB9QuJE8y8fDjdc6t+IN+9PpDas+gn3LzC1KRETyPIUbydOqlwikTqtuTE27CwsGaT/2hvOnzC5LRETysBsKN4cPH+bIkSPO1+vWraN///589tlnOVaYyGXdm0Syoux/2OUohduFk9hnPQ0Oh9lliYhIHnVD4ebxxx9n2bL0wwMxMTHcc889rFu3jtdff53hw4fnaIEiFouFdzpG8ab7iyQZ7tj2L4W1Y80uS0RE8qgbCjfbtm2jYcOGAPzwww/UqFGDNWvW8N133zFlypScrE8EgGA/T/p1asewtC4AOJYMgyMbTK5KRETyohsKN6mpqXh6egKwZMkS7r//fgCqVKnC8ePHc646kX+4o2IIAY2f4md7FFYjjbQfukFSvNlliYhIHnND4aZ69epMmDCBlStXsnjxYlq1agXAsWPHCAoKytECRf7pxZZV+C5kAIcdIbglROP4X38wDLPLEhGRPOSGws17773HxIkTad68OY899hi1a9cGYO7cuc7DVSK3goeblRGP38ErPE+qYcP69yzY9I3ZZYmISB5iMYwb+7PXbreTkJBA0aJFnW0HDx7Ex8eH4sWL51iBOS0hIYHAwEDi4+MJCAgwuxy5QT9uOMKeWW8x0H0aDpsX1qdXQPEqZpclIiK3yPX8/r6hkZuLFy+SnJzsDDaHDh1i9OjR7Nq1K08HGyk4HrqtJDE1evObvSZWexL2H7pB6kWzyxIRkTzghsLNAw88wNdffw1AXFwcUVFRfPjhh7Rv357x48fnaIEimbFYLLz1YC1G+Q3glBGILXYHxsLXzC5LRETygBsKNxs3buSOO+4AYObMmYSGhnLo0CG+/vprPvnkkxwtUORq/L3cGfbYnbyU9hwAlg1fwvY5JlclIiJmu6Fwc+HCBfz9/QH45ZdfeOihh7Bardx+++0cOnQoRwsUyUrd0kVp2OJhxqe1A8A+uy+c1XdQRKQwu6FwU6FCBWbPns3hw4dZtGgR9957LwAnT57UJF3Jdc80K8+a0s+wyVEBW0oCjh97gj3V7LJERMQkNxRuBg8ezEsvvURkZCQNGzakUaNGQPooTt26dXO0QJFrsVktjOpUnzds/UkwfLAeWQ/LRphdloiImOSGTwWPiYnh+PHj1K5dG6s1PSOtW7eOgIAAqlTJu6fk6lTwgmvJ9hPM+nYsn3p8goEFy5OzoPxdZpclIiI54JafCg4QFhZG3bp1OXbsmPMO4Q0bNszTwUYKthbVQgmJ6sh3aXdjwcDxY284f9LsskREJJfdULhxOBwMHz6cwMBAypQpQ5kyZShSpAhvvfUWDocjp2sUybZBbaoyPehZdjlKYb1wCmNWb7CnmV2WiIjkohsKN6+//jpjx47l3XffZdOmTWzatIkRI0YwZswY3nzzzZyuUSTbvNxtfPj47QxwvMBFwwPL/mXwv+d1/ykRkULkhubclChRggkTJjjvBn7ZnDlzeO655zh69GiOFZjTNOemcPjuj0MsnzOF8e6jcbM4oFFfuPdtsFjMLk1ERG7ALZ9zc+bMmUzn1lSpUoUzZ87cyCpFctTjDUsTUKc9r6b2Tm9YOxZWfWRuUSIikituKNzUrl2bsWPHZmgfO3YstWrVuumiRG6WxWLhvQ41SarRkbdSO6c3Lh0Of35pbmEiInLL3dBhqRUrVtC2bVtKly7tvMbN2rVrOXz4MPPnz3femiEv0mGpwiXV7qDv1I3U3PUJfd3mpJ8i/vCXUOMhs0sTEZHrcMsPSzVr1ozdu3fz4IMPEhcXR1xcHA899BB///0333zzzQ0VLXIruNusjHnsNjaV73vlFPFZvWHvUrNLExGRW+SGL+KXmb/++ovbbrsNu92eU6vMcRq5KZySUu30/uoPHjk0jHa237G7eWPr+j+IaGB2aSIikg25chE/kfzEy93GZ12j+CHiDX6z18SWdpG0bzvAie1mlyYiIjlM4UYKDS93GxO7NWJS+FtsdFTALTme1K8egLMHzS5NRERyUJ4IN+PGjSMyMhIvLy+ioqJYt25dtpabNm0aFouF9u3b39oCpcDw8XBjfM87GF38HXY6InC/cJKUyffDuRNmlyYiIjnE7Xo6P/RQ1meYxMXFXXcB06dPZ8CAAUyYMIGoqChGjx5Ny5Yt2bVrF8WLF7/qcgcPHuSll17K02dmSd7k5+nG2Kfu5vmJ7zD89IuUTjhE8pT2eD61ALyLmF2eiIjcpOsauQkMDMzyUaZMGbp06XJdBXz00Uf06tWL7t27U61aNSZMmICPjw9ffnn165HY7XY6d+7MsGHDKFeu3HVtTwQgwMud0b1aM6zIO5wyAvE8vZ2krx+BlAtmlyYiIjfpukZuJk+enKMbT0lJYcOGDQwaNMjZZrVaadGiBWvXrr3qcsOHD6d48eL07NmTlStXZrmN5ORkkpOTna8TEhJuvnApEIr4ePDB0w/y+vgk3js3kIDj67g49Um8n5wGNnezyxMRkRtk6pyb2NhY7HY7oaGhLu2hoaHExMRkusyqVav44osvmDRpUra2MXLkSJfRpYiIiJuuWwqOYr4eDH+6E2/6vMlFwwPvg0u48ENv0N3tRUTyrTwxoTi7zp07x5NPPsmkSZMIDg7O1jKDBg0iPj7e+Th8+PAtrlLymxB/T157tgdDvV4h1bDhs2sWiXNf1J3ERUTyqes6LJXTgoODsdlsnDjheqbKiRMnCAsLy9B/3759HDx4kHbt2jnbHJf+wnZzc2PXrl2UL1/eZRlPT088PT1vQfVSkIQGePHCs30Z8WkibyaPxnfzl5z3DsKv5RtmlyYiItfJ1JEbDw8P6tWrx9KlVy6F73A4WLp0qfOeVf9UpUoVtm7dyubNm52P+++/nzvvvJPNmzfrkJPclBJFvOnxzCt85P4UAH5rPyDxt3EmVyUiItfL1JEbgAEDBtC1a1fq169Pw4YNGT16NImJiXTv3h2ALl26ULJkSUaOHImXlxc1atRwWb5IkSIAGdpFbkREMR8eeXYYn32aQG/7dHx/fY1E76L4Nnjc7NJERCSbTA83HTt25NSpUwwePJiYmBjq1KnDwoULnZOMo6OjsVrz1dQgyefKBPly99MfMm38OToZ8/Ga14dE70B8a7Q1uzQREcmGHL1xZn6gG2dKdu2OiWfPxCdoa/xGMh7YO/+IT8WmZpclIlIo6caZIjmgUlggZXtOYQX18CQFpnbkYvQms8sSEZFrULgRyUK1UkEEdfueDVTFx7hA8pQHSYrZY3ZZIiKSBYUbkWuoERmK2xPT2WmUoYjjLOcmtSXptK6XJCKSVynciGRD7QpluNhpBoeMMELsJ4idcB/J52LNLktERDKhcCOSTXWrVub0Qz9wwihKqdSDHB17H6kXda8yEZG8RuFG5DrcVrs2R+77jrOGH+WSd7BnzIOkJV80uywREfkHhRuR61SvQRP23zuZRMOTahf+ZOvYTtjT0swuS0RELlG4EbkB9Zrcy45m40kxbNQ9t5x147rhsOtO4iIieYHCjcgNqn9XB7bd/iEOw0Kjs/9jxYTnKWTXxBQRyZMUbkRuwm2tu7Ol7lAA7jz1Db9MekMBR0TEZAo3IjepTvv+bK3SH4CWx8by81cfKOCIiJhI4UYkB9TsOJQdZbsB0ObACH76/jMFHBERkyjciOQEi4WqXUazp2R7bBaDtrte48cfvze7KhGRQknhRiSnWCxU7PEFB0PuxNOSRqut/+GHuXPNrkpEpNBRuBHJSTY3IntP40iRBvhZkrh7w3PMWLjU7KpERAoVhRuRnObuRalnf+KEXzWCLOdovLYXM5euNbsqEZFCQ+FG5Fbw9Kf4s//jtHckJS2nqbuiOz/+ttnsqkRECgWFG5FbxOIbTLGnfybeI5Ty1uNUXNKNWWt3mF2WiEiBp3AjcgtZikQQ0OtnEt2KUMt6gLAFPZj7536zyxIRKdAUbkRuMUtIJXy6zybJ6kNj63Y85/Ri/ubDZpclIlJgKdyI5AJLybp4PDGdVIsHLW1/kvhjHxZtO252WSIiBZLCjUgusZZrivXRydix8YhtBYenv8ivO2LMLktEpMBRuBHJRbaq98H9YwB4yjaPTVOH8NvuUyZXJSJSsCjciOQy222dsd/zNgAv2qax5Jt3WbMv1uSqREQKDoUbERPYmvTD3mQAAEOtX/DDlDGsO3DG5KpERAoGhRsRk9haDCatbjesFoP3rWOYNHkSG6PPml2WiEi+p3AjYhaLBbd2H2Gv9iAeFjv/tXzIh19+y9Yj8WZXJiKSrynciJjJasP20GfYy96JjyWZccZIhn4+g+3HEsyuTEQk31K4ETGbmwe2x77DXqI+RSyJfGq8xSufz2VXzDmzKxMRyZcUbkTyAg9fbE/MwB5clVBLHGPThtNv0iL2njxvdmUiIvmOwo1IXuFTDFuXn7AHliHSeoLRqcPp/dlSDsYmml2ZiEi+onAjkpcEhGPr8hMOnxCqWQ/xbso7dP9sBYfPXDC7MhGRfEPhRiSvCSqPtctPODwDaGjdxRsX36fzxFX8dTjO7MpERPIFhRuRvCisJtbHf8Bw8+Zu2yZevPAxncb/xvjl+3A4DLOrExHJ0xRuRPKqMo2wPPo1htWNB2xrmO42mOmLlvHEF38QE59kdnUiInmWwo1IXlbpXiydpmJ4F6WW9QDzPF4j7MBPtPrvbyz6W3cUFxHJjMKNSF5XqSWWZ9dA5B34WpL5yGMCQ1JH8+I3K3ntp61cTLGbXaGISJ6icCOSHwSUgC5z4K43MCw2HrStZp7Ha/y97lfuG7OSv4/plg0iIpcp3IjkF1YbNH0ZS/cFEFiaMtaT/Og5jBZnpvHQuFV8vnK/JhuLiKBwI5L/lI6CZ1ZCtfa4YWeQ+/dMso5k4rw1dJuynpPnNNlYRAo3hRuR/Mi7CDwyBe4fg+HmTVPbVhZ6DsK6dzGtR69k2c6TZlcoImIahRuR/Mpigdu6YHl6BYTWJMiSwBSP93k2+QuenrKGoXP/JilVk41FpPBRuBHJ70Iqw1NLIOoZAJ5yW8AsjyGsWLuW9uNWs/uE7i4uIoWLwo1IQeDuBa3fg8emgXcxalgPMs/zdWqc+pl2Y1byzdqDGIYmG4tI4ZAnws24ceOIjIzEy8uLqKgo1q1bd9W+s2bNon79+hQpUgRfX1/q1KnDN998k4vViuRhlVvDs6sh8g58SGKU+0Tet4zhvTnr6fX1Bs4kpphdoYjILWd6uJk+fToDBgxgyJAhbNy4kdq1a9OyZUtOnsx8QmSxYsV4/fXXWbt2LVu2bKF79+50796dRYsW5XLlInnU5Wvi3D0Yw2LjAdsaFni+RuzO1bQa/Rur9sSaXaGIyC1lMUweq46KiqJBgwaMHTsWAIfDQUREBP369WPgwIHZWsdtt91G27Zteeutt67ZNyEhgcDAQOLj4wkICLip2kXyvMPr4cceEBdNGjY+TH2ECfb76N20Ai/eWxkPN9P/vhERyZbr+f1t6k+2lJQUNmzYQIsWLZxtVquVFi1asHbt2msubxgGS5cuZdeuXTRt2jTTPsnJySQkJLg8RAqNiAbwzCqo/hBu2HnVfRrfuI9k1m8beGj8avafOm92hSIiOc7UcBMbG4vdbic0NNSlPTQ0lJiYq98UMD4+Hj8/Pzw8PGjbti1jxozhnnvuybTvyJEjCQwMdD4iIiJydB9E8jyvQHj4S7h/LLj78H+2v1nkNZCQ4yto+8kqpq+P1mRjESlQ8uWYtL+/P5s3b2b9+vW88847DBgwgOXLl2fad9CgQcTHxzsfhw8fzt1iRfICiwVuexKe/g3CalKMc0z2+ICXjcm8+eNG+kzdSPyFVLOrFBHJEW5mbjw4OBibzcaJEydc2k+cOEFYWNhVl7NarVSoUAGAOnXqsGPHDkaOHEnz5s0z9PX09MTT0zNH6xbJt4IrwlNLYfEQ+GM8PdwWcrt1B3239aN1dBwfd6xDVLkgs6sUEbkppo7ceHh4UK9ePZYuXepsczgcLF26lEaNGmV7PQ6Hg+Tk5FtRokjB4+YJrd+Fx38AnyCqWQ8xz/N1mpxfSKdJa/nwl12k2h1mVykicsNMPyw1YMAAJk2axFdffcWOHTt49tlnSUxMpHv37gB06dKFQYMGOfuPHDmSxYsXs3//fnbs2MGHH37IN998wxNPPGHWLojkT5VawrNroGwzvEnmA/fP+MRtDFN+3cIjE9YSffqC2RWKiNwQUw9LAXTs2JFTp04xePBgYmJiqFOnDgsXLnROMo6OjsZqvZLBEhMTee655zhy5Aje3t5UqVKFb7/9lo4dO5q1CyL5l38YPDkbVo+GX9+mne136tr20e9IX9p8cp632lfnwbqlzK5SROS6mH6dm9ym69yIXMWRP2FmD4g7hB0rH6Y+zAT7/dxfpxTD29cgwMvd7ApFpBDLN9e5EZE8pFR9eGYl1HgYGw5ecf+B7zxGsHbzNtp+spINh86aXaGISLYo3IjIFV6B0OFzeOBTcPelkXU7i7wGUTFuNY9OXMsnS/dgdxSqwV4RyYcUbkTElcUCdTtfuiZOLYpwji89RvGGdQrjFm/jsc9+52jcRbOrFBG5KoUbEclccAV4agk06gtAd7dFzPUczOlDW2k9+jd+3nLM5AJFRDKncCMiV+fmCS3fgcdngE8wlS3RzPN8g1api+k7dSOvzPyLxOQ0s6sUEXGhcCMi11bpXnh2NZRrjhfJvO8+iXHun7Dwz13cN2YVW47EmV2hiIiTwo2IZI9/GDzxE9wzHKxutLX9wSKv1yh6ehMPfbqGCSv24dBkYxHJAxRuRCT7rFZo8gL0+AWKRhLOKWZ4Dudpy0+8v2A7T375BzHxSWZXKSKFnMKNiFy/UvXg6ZVQ8xFsOHjZ/Qe+9xzBvr27af3f3/jl7xizKxSRQkzhRkRujFcAPDQJ2k8Ad1+iLNv5xfs16ietpfc3G3j9p61cTLGbXaWIFEIKNyJy4ywWqPNY+pWNw2sTYJxjksdHDHWbwsw/9tJu7Cq2H0swu0oRKWQUbkTk5gWVh55XronTze0XfvYaDKd20n7car5cdYBCdhs7ETGRwo2I5Aw3j/Rr4nT+EXxDqEg087zepAOLGf7z33Sfsp5T55LNrlJECgGFGxHJWRVbwDOrofxdeBrJjHT/ggken7Bx1wFajf6N6eujdX8qEbmlFG5EJOf5h6aP4NzzFljdaGX9g8Xer1P2whZe/XEr7casYs2+WLOrFJECymIUsgPhCQkJBAYGEh8fT0BAgNnliBR8RzfAzJ5w9gAGFhYTxejk+9luRHJPtVBea1OVssG+ZlcpInnc9fz+VrgRkVsv+RwseBU2f+dsWuK4jTGp7dlurUjXRpH0u7sigd7uJhYpInmZwk0WFG5ETHRiO6wcBdtmAek/en6z1+STtAfZ512TAfdU4rGGpXGz6Yi5iLhSuMmCwo1IHhC7B1Z+BFumg5F+ob/fHVX5JO1BTgVF8fp91WheubjJRYpIXqJwkwWFG5E85MwBWD0aY9N3WBypAGxwVGRMWnuocA9v3FeNCsX9za1RRPIEhZssKNyI5EHxR2D1Jxgbv8KSln7jza2OSD61P0TxBg/ywj1VKObrYXKRImImhZssKNyI5GHnTsDaMTjWfY417SIAOx0RfGHtQJW7nuDJxuXxcNN8HJHCSOEmCwo3IvlA4mn4fRxpv0/ELfU8APsc4fzg/Sj17+tNixolsVgsJhcpIrlJ4SYLCjci+cjFszh+n0jqmnF4pqbfgDPaEcKiYo/TpMPzVIsINrlAEcktCjdZULgRyYeSz5G89jPSVo3BN+0sAMeMYqwv0YVGj/SneLGiJhcoIreawk0WFG5E8rGURM6unIR1zScE2k8DcMoowu7y3ajX4UW8fPX/tEhBpXCTBYUbkQIgNYlDv36G9x+fUNxxCoCzBBBTvSdV2v0Hi1egyQWKSE5TuMmCwo1IweFITWbzvAmEbP6UCGIAOG/x43ydpwi75wXwKWZyhSKSUxRusqBwI1LwXExKZsWsCVTcNYHylmMAJFl9SKvXE79mL4BfiMkVisjNUrjJgsKNSMEVczaRRT9+RsPoL6lqjQYg1eoJ9Xvgfkd/8A8zt0ARuWEKN1lQuBEp+P6KPsPCHyfT6uy31LbuB8Bu9cB6Wxcs/9cfikSYW6CIXDeFmywo3IgUDoZhMH/LcX6dN5XHkqZT37obAIfFDWudx+COAVCsnMlVikh2KdxkQeFGpHBJSrXz5ar9rF8+l16OmTS2bQfAsNiw1HwE7ngRQiqZXKWIXIvCTRYUbkQKp1Pnkvlo8S72/LmEvrbZNLf9BYCBBUv19nDHSxBWw9wiReSqFG6yoHAjUrhtP5bA2/O2c27/evq5/cS9tg1X3qzcFpq+BCVvM69AEcmUwk0WFG5ExDAMluw4yYj5O/A8vYO+brNpY/sDK5d+HFZoAU1fgdJR5hYqIk4KN1lQuBGRy1LSHHzz+yH+u2Q3IcmHeM5tDu1ta7DhSO8QeQc0eyX9v7oLuYipFG6yoHAjIv92NjGF0Ut28+0f0ZQ0YujrPpcOtt+wGfb0DhG3Q7OXofzdCjkiJlG4yYLCjYhczd6T53hn3g6W7TpFCWJ5wWseD1uXYXOkpHcocRs0fRkqt1bIEcllCjdZULgRkWtZsfsUb/+8nT0nz1Ocs7wS8AsP2hdhsyeldyheHWo9ClXbQVB5c4sVKSQUbrKgcCMi2ZFmd/D9+sN89Msuzl5IJYh4hhVfQZuL/8OamnilY2gNqHp/etApXlUjOiK3iMJNFhRuROR6xF9MZeyve5iy5iCpdoNi1kSGltvJ3cY6fI+tgcvzcgCCKqSHnKr3Q4m6CjoiOUjhJgsKNyJyIw7GJjJi/g5+2X7C2daguMFzJfbQKHk1XtErwJ5yZYHAiCtBJ6IhWG0mVC1ScCjcZEHhRkRuxtp9p/ly9QGW7zpJqj39x6fFAs1Ke9IrfB8NL67Cff8SSL1wZSG/UKjSNj3oRP4f2NxNql4k/1K4yYLCjYjkhLgLKczfGsPszUdZd+CMs93DZuWeigH0CNtPncSV2HYvhOT4Kwt6F4XKbdJHdcrdCe5eJlQvkv/ku3Azbtw4PvjgA2JiYqhduzZjxoyhYcOGmfadNGkSX3/9Ndu2bQOgXr16jBgx4qr9/03hRkRy2tG4i8zdfIw5m4+yM+acs93f04221YJ4MjyaqnHLse6cBxdiryzo4QeVWqYHnQr3gKefCdWL5A/5KtxMnz6dLl26MGHCBKKiohg9ejQzZsxg165dFC9ePEP/zp0706RJExo3boyXlxfvvfceP/30E3///TclS5a85vYUbkTkVtoZk8DsTceYu/kox+KTnO3F/T25v1Yoj4cdo+ypJVh2/Aznjl1Z0M0r/SKB1e5PDzzeRU2oXiTvylfhJioqigYNGjB27FgAHA4HERER9OvXj4EDB15zebvdTtGiRRk7dixdunS5Zn+FGxHJDQ6HwfqDZ5i9+Rjztx4n/mKq871yIb60rx3OI+EnCT+2GLbPhbMHrixsdYOyzdKDTuW24Bdiwh6I5C35JtykpKTg4+PDzJkzad++vbO9a9euxMXFMWfOnGuu49y5cxQvXpwZM2Zw3333ZXg/OTmZ5ORk5+uEhAQiIiIUbkQk16SkOVix+xSzNx9lyfYTJKc5nO/VLV2E9rVL8ED4GYocWpQedE7tuLKwxQqlG6cHnSr3QeC1R6hFCqLrCTduuVRTpmJjY7Hb7YSGhrq0h4aGsnPnzmyt49VXX6VEiRK0aNEi0/dHjhzJsGHDbrpWEZEb5eFm5Z5qodxTLZRzSaks+vsEczYfZfXeWDZFx7EpOo7hVgt3VGxB+8ZduTc0AZ9989ODzvHNcGhV+mPBK1CyfvocnWr3Q7FyZu+aSJ5k6sjNsWPHKFmyJGvWrKFRo0bO9ldeeYUVK1bwxx9/ZLn8u+++y/vvv8/y5cupVatWpn00ciMiedXJhCT+t+U4czYfZcuRK2dUebvbuLd6KO3rlOT/Qi7gvnse7PgfRP8O/ONHdmjNK0EnpIouGigFWqE4LDVq1CjefvttlixZQv369bO9Tc25EZG8aP+p88y+dMbVodNXrpFTzNeDtjXDaV+3BLcVTcayaz7smAsHVmZydeRLt4HQ1ZGlAMo34QbSJxQ3bNiQMWPGAOkTikuXLk3fvn2vOqH4/fff55133mHRokXcfvvt17U9hRsRycsMw2Dz4TjmbD7Gz1uOEXv+ylWPI4p580DtkrSvW4IKfqmwa0F60Nn367+ujlz60tWR20FEFFitJuyJSM7KV+Fm+vTpdO3alYkTJ9KwYUNGjx7NDz/8wM6dOwkNDaVLly6ULFmSkSNHAvDee+8xePBgpk6dSpMmTZzr8fPzw8/v2teIULgRkfwize5g9b7TzNl0lEV/x5CYcmWkpnqJANrXKUm72iUI80yBPb+kH7rasxj+eWNPXR1ZCoh8FW4Axo4d67yIX506dfjkk0+IiooCoHnz5kRGRjJlyhQAIiMjOXToUIZ1DBkyhKFDh15zWwo3IpIfXUyxs3jHCeZsOsqK3adIc1y59UOjckG0r1OSVjXDCLClwd6l6UFn14KrXB35fijXXFdHlnwl34Wb3KRwIyL53ZnEFOZtPc6cTUf589BZZ7uHm5W7qxTngTolubNKCJ7Y4eBv6WddZbg6sj9UvAfKNYMyTdLn7GiejuRhCjdZULgRkYLk8JkLzP3rGLM3HWXPyfPO9gAvN9rUDOf+OiW4vWwQVhwQvTY96Oz4n+vVkQF8i0OZxulBp0xjKF5Nc3UkT1G4yYLCjYgURIZhsP14wqV7XB0jJuHKrR/CAry4v04JHqhTgmrhAVgMA45thN2L4NAaOLIe7MmuK/QKTL944OXAE15L83XEVAo3WVC4EZGCzuEw+OPAGeZsPsr8rcdJSEpzvlexuB/t65bk/toliCjmk96YlgxHN8Kh1elh5/AfkHLedaXuvhDR8MrITsl6mrMjuUrhJgsKNyJSmCSn2Vm28xRzNh9l6c6TpPzj1g/1yhSleaUQGlcIpnapQNxslw5D2dMgZkt60Dm0BqLXwMWzriu2eaRfLbnMpdGdiIbg6Z+LeyaFjcJNFhRuRKSwir+YyqJtMczefJS1+0/zz5/+fp5uRJUtRuMKwTSpEETlUH8slycYOxxwaueVkZ1Dq+H8CdeVW2wQXvvKYazSt4NPsdzbOSnwFG6yoHAjIgIx8Uks3nGCNXtjWbv/NHEXUl3eD/bzoFH5YJqUD6JJheArh7AADAPO7L8ysnNoNcRlvEQHxatfGdkp0xj8w27xXklBpnCTBYUbERFXDkf6ZOTVe2NZve806w+c4WKq3aVPRDFvmpQPpnGFYBqXDyLYz9N1JfFH4NDaSzf5XAOxuzNuqFh51zOyipTW6eeSbQo3WVC4ERHJWnKanc3Rcazed5o1e2PZfDjOedHAy6qE+dO4fHrQiSpXDH+vf51Jdf5U+lydyyM7MdtwueknQECpf4zsNIHgigo7clUKN1lQuBERuT7nk9NYf+CMc2Rnx/EEl/dtVgu1SgVeGtkJ4rbSRfFyt7mu5GJc+llYl+ftHNsEjjTXPj7BriM7odXB+q/1SKGlcJMFhRsRkZtz+nwya/efZvXe06zZF+tyF3MATzcrDSKL0bhCEE3KB1OjZCA2679GZFIS06+vc3nezpH1kJbk2sczMH1i8uXAU6KOrrVTiCncZEHhRkQkZx05e4E1l4LO6n2nOXXO9YKAAV5u3F4ufWJykwpBlA/xu3Im1mVpyemjOZdHdqL/gJRzrn3cfaBUgysjO6Xqg7v3Ld47ySsUbrKgcCMicusYhsHek+edh7B+33+ac0muh5+K+3vS5NLE5CYVgilRJJOAYk+DE1tdz8j697V2rO7pFxMs0whK3JZ+FeUiZTRvp4BSuMmCwo2ISO5JszvYdiz9TKw1+2L58+BZkv9xIUGAyCCf9OvrlA+mUfkgivl6ZFyRwwGxu66M7BxcDedjMvbzDISwmulBJ6xW+vOQyjqcVQAo3GRB4UZExDxJqXY2HjrL6n2xrN57mi1H4vjniVgWC1QNC6BJhSAaVwimYWQxfD3dMq7IMODsgUuHsNbC8S1wcgc4UjP2tXlC8ar/CDy10icre/rduh2VHKdwkwWFGxGRvCMhKZU/9p9xjuzsPuF6Tys3q4W6pYvQuHwwTSoEUyeiCB5uV7lbeVpK+ujO8S3pt484vgVitmacuwOABYIquI7yhNcG3+Cc30nJEQo3WVC4ERHJu06dS2bNvljW7D3N6n2xHDl70eV9b3cbDcsWc87XqRYegPXfZ2L9k8MBcQevBJ6YrenPMzukBeAffino/OOwVtFIzePJAxRusqBwIyKSf0SfvnDpEFYsa/ed5nRiisv7RXzcaVQuiLqli1CjZCA1SgYS8O8LCmbm/Ml/BJ5Lozxn9mXe12UeT8300KN5PLlO4SYLCjciIvmTw2Gw68S5S4ewTvPH/tMkptgz9IsM8qFGyUBqXnpULxlIoHc2gkjyOTjx96XQ81f6KM/JHWBPydj38jyesJrph7M0j+eWU7jJgsKNiEjBkGp3sOVIHL/vP8O2o/FsORLP0biLmfYtE+TjDDvXFXhc5vFsvXJoKzkhk84WCCr/r8NatcAv5OZ2VACFmywp3IiIFFxnElPYdjSerUfjnf/997ydy8r8a4SnRolAAn2yEXguz+O5PH/ncuA5dzzz/i7zeC4d1tI8nuumcJMFhRsRkcLlbGIK2465Bp7DZzIPPKWL+VCz1A0EHkifx/PPs7RitsDpfWS4YShcmcfzz7O1NI8nSwo3WVC4ERGR6w48lyYrXw492Q48yefT5/HEbIHjf6X/96rzeDygaFkoVvbSf8tdeV6kNLhlcnHDQkThJgsKNyIikpm4CylsO5rgEniiz1zItG9EMe9LQafIpeATQBGfbIYPeyqc2vWvUZ6tkBx/9WUsVggslTH0XH7u4XsDe5y/KNxkQeFGRESyK/5CqnOEZ+uR7AWef47wZDvwGAbERaefjn7mQPrVl88cuPI8NfNtOvkWzzz0FC0LPsUKxPwehZssKNyIiMjNcAk8l0Z5Dp3OPHyUKnpphKfUlTk8RTO7d1ZWDAPOn/hH6Nnv+vzfNxT9N89AKBaZ+aiPfzhYr3LF5zxG4SYLCjciIpLT4i+m8vdR18Bz8BqB558jPNcdeP7pYlwmoefS49yxrJe1eaafueUSei4Fn8CIPDXPR+EmCwo3IiKSG64n8JQskh54ygT5EB7oRVigN+GBXoQHehHk54ktq1tMZCX1Ipw9mHHU58x+iD8MjrSrL3t5nk+xcq6h5/LzXJ7no3CTBYUbERExS/zFVP4+dnnCcgLbjsZzIDYxy2XcrBZCA7wIC0x/lPhH+Am7FIBC/Dxxs13n4SV7WnrAObM/4xyfMwcgLfOzx5z8QjMPPcXKgXfRHJ/no3CTBYUbERHJSxKSUvn7aALbjydwLO4ix+Mvcjw+iZj4JE4kJOHIxm9pqwWK+18KP0W8CAtwDT9hgV6EBnjhnt0AZBhwLuYfoWe/6/OkuKyXD68NT/+WvW1l0/X8/nbL0S2LiIjIdQnwcqdR+SAalQ/K8F6a3UHs+RSOx18kJj6JY/FJxPwj/By/FIDSHAYxCUnEJCSx+XDm27FYINjP89LIjxfhgd5Xwk9A+uvQQE883WzpnQPC0x9lGmdc2cWz/wo9B688P3ccfM295YRGbkRERPIxh8MgNjGZ43FJl0LPRY4nXAk/MZceKXZHttYX7OeRfggsk9GfEpcCkZe77eorSLmQfhNS/9Ac2sN0GrkREREpJKxWC8X9vSju70XtiMz7OBwGZy6k/CPwuI7+XD4UlpyWPlIUez79goZXU9TH3XXeT8CV0aDwIl6EBQRh5mUFFW5EREQKOKvVQrCfJ8F+ntQoGZhpH8MwiLuQmh56Eq6En2NxV14fj0viYqqdsxdSOXshlR3HMw9AlUL9+OU/zW7lLmVJ4UZERESwWCwU9fWgqK8H1UpkftjHMAwSktIujfhcCjz/GgmKiU8iPNA7l6t3pXAjIiIi2WKxWAj0difQ253KYf5X7ZeSlr35PbdK/rjmsoiIiOQbHm7mxguFGxERESlQFG5ERESkQFG4ERERkQJF4UZEREQKFIUbERERKVAUbkRERKRAUbgRERGRAkXhRkRERAoU08PNuHHjiIyMxMvLi6ioKNatW3fVvn///TcdOnQgMjISi8XC6NGjc69QERERyRdMDTfTp09nwIABDBkyhI0bN1K7dm1atmzJyZMnM+1/4cIFypUrx7vvvktYWFguVysiIiL5ganh5qOPPqJXr150796datWqMWHCBHx8fPjyyy8z7d+gQQM++OADOnXqhKenZy5XKyIiIvmBaeEmJSWFDRs20KJFiyvFWK20aNGCtWvXmlWWiIiI5HOm3RU8NjYWu91OaGioS3toaCg7d+7Mse0kJyeTnJzsfJ2QkJBj6xYREZG8x7Rwk1tGjhzJsGHDMrQr5IiIiOQfl39vG4Zxzb6mhZvg4GBsNhsnTpxwaT9x4kSOThYeNGgQAwYMcL4+evQo1apVIyIiIse2ISIiIrnj3LlzBAYGZtnHtHDj4eFBvXr1WLp0Ke3btwfA4XCwdOlS+vbtm2Pb8fT0dJl87Ofnx+HDh/H398diseTYdiA9VUZERHD48GECAgJydN1y/fR55C36PPIWfR55jz6TrBmGwblz5yhRosQ1+5p6WGrAgAF07dqV+vXr07BhQ0aPHk1iYiLdu3cHoEuXLpQsWZKRI0cC6ZOQt2/f7nx+9OhRNm/ejJ+fHxUqVMjWNq1WK6VKlbo1O3RJQECAvph5iD6PvEWfR96izyPv0WdyddcasbnM1HDTsWNHTp06xeDBg4mJiaFOnTosXLjQOck4Ojoaq/XKCV3Hjh2jbt26ztejRo1i1KhRNGvWjOXLl+d2+SIiIpIHWYzszMyRbElISCAwMJD4+Hil7jxAn0feos8jb9HnkffoM8k5pt9+oSDx9PRkyJAhusBgHqHPI2/R55G36PPIe/SZ5ByN3IiIiEiBopEbERERKVAUbkRERKRAUbgRERGRAkXhRkRERAoUhZscMm7cOCIjI/Hy8iIqKop169aZXVKhNXLkSBo0aIC/vz/Fixenffv27Nq1y+yy5JJ3330Xi8VC//79zS6l0Dp69ChPPPEEQUFBeHt7U7NmTf7880+zyyqU7HY7b775JmXLlsXb25vy5cvz1ltvZev+SXJ1Cjc5YPr06QwYMIAhQ4awceNGateuTcuWLTl58qTZpRVKK1asoE+fPvz+++8sXryY1NRU7r33XhITE80urdBbv349EydOpFatWmaXUmidPXuWJk2a4O7uzoIFC9i+fTsffvghRYsWNbu0Qum9995j/PjxjB07lh07dvDee+/x/vvvM2bMGLNLy9d0KngOiIqKokGDBowdOxZIv0dWREQE/fr1Y+DAgSZXJ6dOnaJ48eKsWLGCpk2bml1OoXX+/Hluu+02Pv30U95++23q1KnD6NGjzS6r0Bk4cCCrV69m5cqVZpciwH333UdoaChffPGFs61Dhw54e3vz7bffmlhZ/qaRm5uUkpLChg0baNGihbPNarXSokUL1q5da2Jlcll8fDwAxYoVM7mSwq1Pnz60bdvW5f8VyX1z586lfv36PPLIIxQvXpy6desyadIks8sqtBo3bszSpUvZvXs3AH/99RerVq2idevWJleWv5l6b6mCIDY2Frvd7rwf1mWhoaHs3LnTpKrkMofDQf/+/WnSpAk1atQwu5xCa9q0aWzcuJH169ebXUqht3//fsaPH8+AAQN47bXXWL9+Pc8//zweHh507drV7PIKnYEDB5KQkECVKlWw2WzY7XbeeecdOnfubHZp+ZrCjRRoffr0Ydu2baxatcrsUgqtw4cP88ILL7B48WK8vLzMLqfQczgc1K9fnxEjRgBQt25dtm3bxoQJExRuTPDDDz/w3XffMXXqVKpXr87mzZvp378/JUqU0OdxExRublJwcDA2m40TJ064tJ84cYKwsDCTqhKAvn378vPPP/Pbb79RqlQps8sptDZs2MDJkye57bbbnG12u53ffvuNsWPHkpycjM1mM7HCwiU8PJxq1aq5tFWtWpUff/zRpIoKt5dffpmBAwfSqVMnAGrWrMmhQ4cYOXKkws1N0Jybm+Th4UG9evVYunSps83hcLB06VIaNWpkYmWFl2EY9O3bl59++olff/2VsmXLml1SoXb33XezdetWNm/e7HzUr1+fzp07s3nzZgWbXNakSZMMl0bYvXs3ZcqUMamiwu3ChQtYra6/im02Gw6Hw6SKCgaN3OSAAQMG0LVrV+rXr0/Dhg0ZPXo0iYmJdO/e3ezSCqU+ffowdepU5syZg7+/PzExMQAEBgbi7e1tcnWFj7+/f4b5Tr6+vgQFBWkelAn+85//0LhxY0aMGMGjjz7KunXr+Oyzz/jss8/MLq1QateuHe+88w6lS5emevXqbNq0iY8++ogePXqYXVq+plPBc8jYsWP54IMPiImJoU6dOnzyySdERUWZXVahZLFYMm2fPHky3bp1y91iJFPNmzfXqeAm+vnnnxk0aBB79uyhbNmyDBgwgF69epldVqF07tw53nzzTX766SdOnjxJiRIleOyxxxg8eDAeHh5ml5dvKdyIiIhIgaI5NyIiIlKgKNyIiIhIgaJwIyIiIgWKwo2IiIgUKAo3IiIiUqAo3IiIiEiBonAjIiIiBYrCjYgUehaLhdmzZ5tdhojkEIUbETFVt27dsFgsGR6tWrUyuzQRyad0bykRMV2rVq2YPHmyS5unp6dJ1YhIfqeRGxExnaenJ2FhYS6PokWLAumHjMaPH0/r1q3x9vamXLlyzJw502X5rVu3ctddd+Ht7U1QUBC9e/fm/PnzLn2+/PJLqlevjqenJ+Hh4fTt29fl/djYWB588EF8fHyoWLEic+fOvbU7LSK3jMKNiOR5b775Jh06dOCvv/6ic+fOdOrUiR07dgCQmJhIy5YtKVq0KOvXr2fGjBksWbLEJbyMHz+ePn360Lt3b7Zu3crcuXOpUKGCyzaGDRvGo48+ypYtW2jTpg2dO3fmzJkzubqfIpJDDBERE3Xt2tWw2WyGr6+vy+Odd94xDMMwAOOZZ55xWSYqKsp49tlnDcMwjM8++8woWrSocf78eef78+bNM6xWqxETE2MYhmGUKFHCeP31169aA2C88cYbztfnz583AGPBggU5tp8ikns050ZETHfnnXcyfvx4l7ZixYo5nzdq1MjlvUaNGrF582YAduzYQe3atfH19XW+36RJExwOB7t27cJisXDs2DHuvvvuLGuoVauW87mvry8BAQGcPHnyRndJREykcCMipvP19c1wmCineHt7Z6ufu7u7y2uLxYLD4bgVJYnILaY5NyKS5/3+++8ZXletWhWAqlWr8tdff5GYmOh8f/Xq1VitVipXroy/vz+RkZEsXbo0V2sWEfNo5EZETJecnExMTIxLm5ubG8HBwQDMmDGD+vXr83//93989913rFu3ji+++AKAzp07M2TIELp27crQoUM5deoU/fr148knnyQ0NBSAoUOH8swzz1C8eHFat27NuXPnWL16Nf369cvdHRWRXKFwIyKmW7hwIeHh4S5tlStXZufOnUD6mUzTpk3jueeeIzw8nO+//55q1aoB4OPjw6JFi3jhhRdo0KABPj4+dOjQgY8++si5rq5du5KUlMTHH3/MSy+9RHBwMA8//HDu7aCI5CqLYRiG2UWIiFyNxWLhp59+on379maXIiL5hObciIiISIGicCMiIiIFiubciEiepiPnInK9NHIjIiIiBYrCjYiIiBQoCjciIiJSoCjciIiISIGicCMiIiIFisKNiIiIFCgKNyIiIlKgKNyIiIhIgaJwIyIiIgXK/wPfnr35FLJgAgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLh2DOF_z7En",
        "outputId": "3fb04fe1-3ade-4fca-9175-0f92c4efe341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(dice_scores, label='Dice score')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dice score')\n",
        "plt.title('Dice Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLmWmnwWbCaZ"
      },
      "source": [
        "# Inference debug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtZyUqY3bByz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0c88f9-a6f2-4042-9de2-32a81584d3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg13-c768596a.pth\" to /root/.cache/torch/hub/checkpoints/vgg13-c768596a.pth\n",
            "100%|██████████| 508M/508M [00:27<00:00, 19.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Inference on full images\n",
        "test_image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05.tif\"\n",
        "test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "\n",
        "output_folder = \"./gdrive/MyDrive/lsec_test\"\n",
        "# model = UNET(in_channels=1, out_channels=1, device=DEVICE, dropout_probability=config['dropout'], activation=None).to(DEVICE)\n",
        "model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "out_mask_path = inference_on_image_with_overlap(model, test_image_path, output_folder)\n",
        "merge_original_mask(test_image_path, test_mask_path, output_folder)\n",
        "merge_masks(out_mask_path, test_mask_path, output_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Inference loop**"
      ],
      "metadata": {
        "id": "ZiGwUlrqBx1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = preprocess_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "# class SigmoidWrapper(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super(SigmoidWrapper, self).__init__()\n",
        "#         self.model = model\n",
        "#         self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.model(x)\n",
        "#         x = self.sigmoid(x)\n",
        "#         return x\n",
        "\n",
        "# model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_checkpoint.pth\")\n",
        "# device = torch.device('cuda')\n",
        "# model = build_model('vgg13+none', 0.0, 'dice+bce')\n",
        "# if torch.cuda.is_available():\n",
        "#     model.load_state_dict(torch.load(model_path))\n",
        "# model_with_sigmoid = SigmoidWrapper(model)\n",
        "# model_with_sigmoid.to(device=device)\n",
        "# wrapper = FunctionWrapper(model_with_sigmoid)\n",
        "# wrapper.to(device=device)\n",
        "# full_model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13_dice+bce_nlm_sigmoid_checkpoint.pth\")\n",
        "# torch.save(wrapper, full_model_path)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)"
      ],
      "metadata": {
        "id": "XK2mzMWMK02R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert Google Drive paths:**\n",
        "\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "input_images_folder = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "output_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "model_path = './gdrive/MyDrive/lsecs/resnet50+imagenet_dice+bce_med5.pth' #@param {type:\"string\"}\n",
        "\n",
        "input_images_folder = input_images_folder.strip()\n",
        "output_mask_folder = output_mask_folder.strip()\n",
        "model_path = model_path.strip()\n",
        "\n",
        "if not os.path.exists(output_mask_folder):\n",
        "    os.makedirs(output_mask_folder)\n",
        "if not os.path.exists(input_images_folder):\n",
        "    print(f'{input_images_folder} does not exist)')"
      ],
      "metadata": {
        "id": "9JALgAucLM_7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # model_path = os.path.join(\"./gdrive/MyDrive/lsecs\", f\"vgg13+imagenet_dice+bce_nlm.pth\")\n",
        "# # Inference on full images\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/sem_images\"\n",
        "# # # test_mask_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_05_original_mask.tif\"\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/new_masks\"\n",
        "\n",
        "\n",
        "# # images_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/sem_images\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/zapotoczny_test/edited_masks\"\n",
        "# masks_path = \"./gdrive/MyDrive/lsecs/fenestration_seg/whole_cells/fen_mask\"\n",
        "\n",
        "# # output_folder = \"./gdrive/MyDrive/lsecs/fenestration_seg/patches/new_masks\"\n",
        "# # if not os.path.exists(output_folder):\n",
        "# #     os.makedirs(output_folder)\n",
        "\n",
        "# image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "# mask_names = [f for f in sorted(os.listdir(masks_path)) if os.path.isfile(os.path.join(masks_path, f))]\n",
        "\n",
        "# if len(image_names) != len(mask_names):\n",
        "#     print(f'There are {len(image_names)} images, but {len(mask_names)} masks.')\n",
        "# # print(image_names, mask_names)\n",
        "# # model = build_model('vgg13+imagenet', 0.0, 'dice+bce')\n",
        "# # if torch.cuda.is_available():\n",
        "# #     model.load_state_dict(torch.load(model_path))\n",
        "# # else:\n",
        "# #     model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "# model = torch.load(model_path)"
      ],
      "metadata": {
        "id": "5YvVBtOmB2aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for image_name, mask_name in zip(image_names, mask_names):\n",
        "image_names = [f for f in sorted(os.listdir(input_images_folder)) if os.path.isfile(os.path.join(input_images_folder, f))]\n",
        "# model = torch.load_state_dict(model_path)\n",
        "basename = os.path.basename(model_path).split('+')[0]\n",
        "encoder = 'vgg11'\n",
        "if 'vgg11' in basename:\n",
        "    encoder='vgg11'\n",
        "encoder = basename\n",
        "model = build_model(encoder+'+none', 0,'dice+bce')\n",
        "# build_model('vgg11+none', 0.0, 'dice+bce')\n",
        "loaded_state_dict = torch.load(model_path)\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "\n",
        "for image_name in image_names:\n",
        "    print(image_name)\n",
        "    # print(f'{image_name} - {mask_name}')\n",
        "    image_path = os.path.join(input_images_folder, image_name)\n",
        "    # mask_path = os.path.join(masks_path, mask_name)\n",
        "    filter = None\n",
        "    output_mask = inference_on_image_with_overlap(model, image_path, filter)\n",
        "\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    out_mask_path = os.path.join(output_mask_folder, filename+\"_new_mask\"+ext)\n",
        "    # Save created mask\n",
        "    cv.imwrite(out_mask_path, output_mask)\n",
        "    # # Merge image with created mask\n",
        "    # merge = merge_images(cv.imread(image_path, cv.IMREAD_GRAYSCALE), output_mask)\n",
        "    # cv.imwrite(os.path.join(output_mask_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "    # # print(out_mask_path)\n",
        "    # # print(image_path, mask_path)\n",
        "    # merge_original_mask(image_path, mask_path, output_mask_folder)\n",
        "    # merge_masks(out_mask_path, mask_path, output_mask_folder)\n",
        "    # # break"
      ],
      "metadata": {
        "id": "OyROv0wvZRUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6861431d-f9fa-4e36-d309-8ee5eaa09657"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "II_Bleb20_07.tif\n",
            "II_Y10_03.tif\n",
            "IV_Cal10_CB_05.tif\n",
            "IV_Cal10_CB_14.tif\n",
            "IV_Cal10_CB_16.tif\n",
            "IV_Cal10_CB_18.tif\n",
            "IV_K_03.tif\n",
            "IV_K_14.tif\n",
            "IV_K_16.tif\n",
            "IV_K_17.tif\n",
            "I_K_04.tif\n",
            "I_K_06.tif\n",
            "I_K_10.tif\n",
            "I_K_11.tif\n",
            "I_K_12.tif\n",
            "V_ML_04.tif\n",
            "V_ML_05.tif\n",
            "V_ML_12.tif\n",
            "V_ML_16.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Apply cell mask**"
      ],
      "metadata": {
        "id": "vDkdVH2o-xRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask_path = './gdrive/MyDrive/lsecs/dice_score_test/my_masks' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "\n",
        "images = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "if len(images) != len(cells):\n",
        "    print('The number of ground truths and created masks differs.')\n",
        "\n",
        "for image_name, cell_name in zip(images, cells):\n",
        "    print(f'Image: {image_name} - cell: {cell_name}')\n",
        "    im_path = os.path.join(mask_path, image_name)\n",
        "    cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "    image = cv.imread(im_path, cv.IMREAD_GRAYSCALE)\n",
        "    cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "    image[cell == 0] = 0\n",
        "    if rewrite_images:\n",
        "        new_name = image_name\n",
        "        cv.imwrite(os.path.join(mask_path, new_name), image)\n",
        "    else:\n",
        "        new_name = image_name\n",
        "        if not os.path.exists(os.path.join(mask_path, 'single_cell')):\n",
        "            os.makedirs(os.path.join(mask_path, 'single_cell'))\n",
        "        print(os.path.join(mask_path, 'single_cell', new_name))\n",
        "        cv.imwrite(os.path.join(mask_path, 'single_cell', new_name), image)\n",
        "\n"
      ],
      "metadata": {
        "id": "gSHpG-sO-urM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208c72ea-faf4-4a9d-a30a-3d40c669bcad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: IV_Cal10_CB_16_new_mask.tif - cell: IV_Cal10_CB_16_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/IV_Cal10_CB_16_new_mask.tif\n",
            "Image: IV_Cal10_CB_18_new_mask.tif - cell: IV_Cal10_CB_18_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/IV_Cal10_CB_18_new_mask.tif\n",
            "Image: I_K_12_new_mask.tif - cell: I_K_12_cell.tif\n",
            "./gdrive/MyDrive/lsecs/dice_score_test/my_masks/single_cell/I_K_12_new_mask.tif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Exclusion of fenestrations based on diameter and roundness**"
      ],
      "metadata": {
        "id": "StzWoL-FyWjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "def remove_fenestrations(mask, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    # contours = find_fenestration_contours(mask_path)\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None:\n",
        "            center, axes, angle = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            # print(axes)\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length:\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                # rotation_angle = angle\n",
        "                diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "                # print(contour)\n",
        "                # print(diameter)\n",
        "                if (diameter < min_d or diameter > max_d) or  (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    mask = remove_contour_from_mask(contour, mask)\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "            else:\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # print(equivalent_diameters)\n",
        "    if len(equivalent_diameters) > 0:\n",
        "        mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "        std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "        print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    return mask"
      ],
      "metadata": {
        "id": "EKJrxqjN6WqR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert the pixel size, and min and max fenestration diameters in nanometers:**\n",
        "\n",
        "#@markdown All fenestration with a smaller or larger diameter than the chosen range will be removed from the crated masks.\n",
        "#@markdown (Use dot '.' as the decimal separator, not comma ',').\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "pixel_size_nm = 9.259 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "min_diameter_nm = 105 #@param {type:\"number\"}\n",
        "max_diameter_nm = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "filter_by_roundness = False # @param {type:\"boolean\"}\n",
        "min_roundness = 0.2 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "# #@markdown ---\n",
        "# filter_by_fenestration_area = False # @param {type:\"boolean\"}\n",
        "# min_area_nm2 = 105 #@param {type:\"number\"}\n",
        "# max_area_nm2 = 500 #@param {type:\"number\"}\n",
        "#@markdown ---\n",
        "mask_path = './gdrive/MyDrive/lsecs/mask_edit_test' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "\n",
        "mask_path = mask_path.strip()\n",
        "mask_names = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "\n",
        "# def remove_contour_from_mask(contour, mask):\n",
        "#     # Fill the contour with black pixels\n",
        "#     cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "#     return mask\n",
        "\n",
        "# def remove_fenestrations(image, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "#     # contours = find_fenestration_contours(mask_path)\n",
        "#     contours, _ = cv.findContours(image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "#     fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "#     contour_centers = find_contour_centers(contours)\n",
        "#     ellipses = fit_ellipses(contours, contour_centers)\n",
        "#     roundness_of_ellipses = []\n",
        "#     equivalent_diameters = []\n",
        "#     fenestration_areas_from_ellipses = []\n",
        "#     mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "#     # Remove all contours that do not fit the chosen conditions\n",
        "#     # Also remove all contours that were too small to fit an ellipse\n",
        "#     for contour, ellipse in zip(contours, ellipses):\n",
        "#         if ellipse is not None:\n",
        "#             center, axes, angle = ellipse\n",
        "#             # center_x, center_y = center\n",
        "#             minor_axis_length, major_axis_length = axes\n",
        "#             # print(axes)\n",
        "#             if major_axis_length != 0 or major_axis_length > 20*minor_axis_length:\n",
        "#                 roundness = minor_axis_length/major_axis_length\n",
        "#                 if roundness >= min_roundness:\n",
        "#                     roundness_of_ellipses.append(roundness)\n",
        "#                 # rotation_angle = angle\n",
        "#                 diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "#                 # print(contour)\n",
        "#                 # print(diameter)\n",
        "#                 if filter_by_diameter and (diameter < min_d or diameter > max_d) or filter_by_roundness and (roundness < min_roundness) or np.isnan(diameter):\n",
        "#                     mask = remove_contour_from_mask(contour, mask)\n",
        "#                 else:\n",
        "#                     equivalent_diameters.append(diameter)\n",
        "#                     fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "#             else:\n",
        "#                 mask = remove_contour_from_mask(contour, mask)\n",
        "#         else:\n",
        "#             mask = remove_contour_from_mask(contour, mask)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "#     equivalent_diameters = np.array(equivalent_diameters)\n",
        "#     # print(equivalent_diameters)\n",
        "#     if len(equivalent_diameters) > 0:\n",
        "#         mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "#         std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "#         print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "#     return mask\n",
        "\n",
        "# def remove_fenestrations(mask_path, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "#     contours = find_fenestration_contours(mask_path)\n",
        "#     fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "#     contour_centers = find_contour_centers(contours)\n",
        "#     ellipses = fit_ellipses(contours, contour_centers)\n",
        "#     roundness_of_ellipses = []\n",
        "#     equivalent_diameters = []\n",
        "#     fenestration_areas_from_ellipses = []\n",
        "#     mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "#     # Remove all contours that do not fit the chosen conditions\n",
        "#     # Also remove all contours that were too small to fit an ellipse\n",
        "#     for contour, ellipse in zip(contours, ellipses):\n",
        "#         if ellipse is not None:\n",
        "#             center, axes, angle = ellipse\n",
        "#             # center_x, center_y = center\n",
        "#             minor_axis_length, major_axis_length = axes\n",
        "#             # print(axes)\n",
        "#             if major_axis_length != 0 or major_axis_length > 20*minor_axis_length:\n",
        "#                 roundness = minor_axis_length/major_axis_length\n",
        "#                 if roundness >= min_roundness:\n",
        "#                     roundness_of_ellipses.append(roundness)\n",
        "#                 # rotation_angle = angle\n",
        "#                 diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "#                 # print(contour)\n",
        "#                 # print(diameter)\n",
        "#                 if filter_by_diameter and (diameter < min_d or diameter > max_d) or filter_by_roundness and (roundness < min_roundness) or np.isnan(diameter):\n",
        "#                     mask = remove_contour_from_mask(contour, mask)\n",
        "#                 else:\n",
        "#                     equivalent_diameters.append(diameter)\n",
        "#                     fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "#             else:\n",
        "#                 mask = remove_contour_from_mask(contour, mask)\n",
        "#         else:\n",
        "#             mask = remove_contour_from_mask(contour, mask)\n",
        "#     # cv2_imshow(mask)\n",
        "#     # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "#     equivalent_diameters = np.array(equivalent_diameters)\n",
        "#     # print(equivalent_diameters)\n",
        "#     if len(equivalent_diameters) > 0:\n",
        "#         mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "#         std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "#         print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "#     return mask\n",
        "\n",
        "\n",
        "#TODO: ukazat statistiky pro celou slozku obrazku\n",
        "\n",
        "if not rewrite_images:\n",
        "    new_mask_path = os.path.join(mask_path, 'edited_masks')\n",
        "    os.makedirs(new_mask_path, exist_ok=True)\n",
        "else:\n",
        "    new_mask_path = mask_path\n",
        "# print(new_mask_path)\n",
        "for mask_name in mask_names:\n",
        "    # print(mask_name)\n",
        "    mask_path_full = os.path.join(mask_path, mask_name)\n",
        "    # print(mask_path)\n",
        "    mask = cv.imread(mask_path_full, cv.IMREAD_GRAYSCALE)\n",
        "    edited_mask = remove_fenestrations(mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    # print(os.path.join(new_mask_path, mask_name))\n",
        "    cv.imwrite(os.path.join(new_mask_path, mask_name), edited_mask)\n",
        "    # cv.imwrite(os.path.join(new_mask_path, mask_name), mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Display the number of circles and their fitted ellipses\n",
        "# print(\"Number of fenestrations:\", len(contours))\n",
        "# print(\"Number of fitted ellipses:\", len(ellipses))\n"
      ],
      "metadata": {
        "id": "tRq0VTv6yrgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52e1749b-72a0-4a92-c1bd-bd1b2427b25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean equavalent diameter: 171 nm, std: 45 nm \n",
            "Mean equavalent diameter: 193 nm, std: 47 nm \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Inference evaluation (dice score)**"
      ],
      "metadata": {
        "id": "RxiOKWQb6aFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert folders with cell images and their ground truth masks for comparison:\n",
        "images_path = './gdrive/MyDrive/lsecs/dice_score_test/images' #@param {type:\"string\"}\n",
        "ground_truth_mask_folder = './gdrive/MyDrive/lsecs/dice_score_test/ground_truth_masks' #@param {type:\"string\"}\n",
        "models_path = './gdrive/MyDrive/lsecs' #@param {type:\"string\"}\n",
        "cell_mask_path = './gdrive/MyDrive/lsecs/dice_score_test/cell_masks' #@param {type:\"string\"}\n",
        "\n",
        "# filter_by_diameter = False # @param {type:\"boolean\"}\n",
        "\n",
        "remove_false_fenestrations = True # @param {type:\"boolean\"}\n",
        "pixel_size_nm = 9.259 #@param {type:\"number\"}\n",
        "min_diameter_nm = 50 #@param {type:\"number\"}\n",
        "max_diameter_nm = 400 #@param {type:\"number\"}\n",
        "min_roundness = 0.4 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "log_file_path = './gdrive/MyDrive/lsecs/dice_score_test/log.txt'\n",
        "\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "\n",
        "\n",
        "\n",
        "ground_truth_mask_folder = ground_truth_mask_folder.strip()\n",
        "images_path = images_path.strip()\n",
        "models_path = models_path.strip()\n",
        "cell_mask_path = cell_mask_path.strip()\n",
        "\n",
        "\n",
        "model_names = sorted([f for f in os.listdir(models_path) if os.path.isfile(os.path.join(models_path, f)) and 'pt' in f])\n",
        "cells = sorted([f for f in os.listdir(cell_mask_path) if os.path.isfile(os.path.join(cell_mask_path, f))])\n",
        "\n",
        "print(model_names)\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.exists(images_path):\n",
        "    print(\"Images folder does not exist\")\n",
        "    # exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    print(\"Folder with ground truth masks does not exist\")\n",
        "    # exit()\n",
        "\n",
        "ground_truth_images = sorted([f for f in os.listdir(ground_truth_mask_folder) if os.path.isfile(os.path.join(ground_truth_mask_folder, f))])\n",
        "images = sorted([f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))])\n",
        "\n",
        "if len(ground_truth_images) != len(images):\n",
        "    print('The number of ground truths and images differs.')\n",
        "    # exit()\n",
        "\n",
        "def compute_dice_score(image1, image2):\n",
        "    eps = 1e-8\n",
        "    image1[image1 == 255] = 1\n",
        "    image2[image2 == 255] = 1\n",
        "    intersection_sum = np.logical_and(image1, image2).sum()\n",
        "    dice_score = (2*intersection_sum+eps)/(image1.sum() + image2.sum() + eps)\n",
        "    return dice_score\n",
        "\n",
        "# images_path = './gdrive/MyDrive/lsecs/dice_score_test/semiautomatic_masks'\n",
        "# dice_scores = []\n",
        "# with open(log_file_path, \"a+\") as file:\n",
        "#     file.write(f'{len(images)} images\\n')\n",
        "#     for ground_truth_mask_name, image_name, cell_name  in zip(ground_truth_images, images, cells):\n",
        "#         print(f'Compare: {ground_truth_mask_name} - {image_name} - {cell_name}')\n",
        "#         file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "#         ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "#         image_path = os.path.join(images_path, image_name)\n",
        "#         cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "#         cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "#         ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#         image_mask[cell == 0] = 0\n",
        "#         current_dice_score = compute_dice_score(ground_truth_mask, image_mask)\n",
        "#         print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "#         file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "#         dice_scores.append(current_dice_score)\n",
        "\n",
        "#     dice_scores = np.array(dice_scores)\n",
        "#     mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "#     std_dice = round(np.std(dice_scores)*100, 1)\n",
        "\n",
        "#     print(f'Semiautomatic Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "#     file.write(f'Semiautomatic Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "with open(log_file_path, \"a+\") as file:\n",
        "    file.write(f'{len(images)} images\\n')\n",
        "    for model_name in model_names:\n",
        "        file.write(f'{model_name}\\n')\n",
        "        print(model_name)\n",
        "        model = build_model(model_name.split('+')[0]+'+none', 0,'dice+bce')\n",
        "        loaded_state_dict = torch.load(os.path.join(models_path, model_name)) # TODO:these models do not include sigmoid and preprocessing yet\n",
        "        model.load_state_dict(loaded_state_dict)\n",
        "        model.eval()\n",
        "        dice_scores = []\n",
        "        dice_scores_filt = []\n",
        "        if 'nlm' in model_name:\n",
        "            filter_type = 'nlm'\n",
        "        elif 'med5' in model_name:\n",
        "            filter_type = 'med5'\n",
        "        else:\n",
        "            filter_type = None\n",
        "        print(filter_type)\n",
        "        for ground_truth_mask_name, image_name, cell_name  in zip(ground_truth_images, images, cells):\n",
        "            print(f'Compare: {ground_truth_mask_name} - {image_name} - {cell_name}')\n",
        "            file.write(f'Compare: {ground_truth_mask_name} - {image_name}\\n')\n",
        "            ground_truth_mask_path = os.path.join(ground_truth_mask_folder, ground_truth_mask_name)\n",
        "            image_path = os.path.join(images_path, image_name)\n",
        "            cell_path = os.path.join(cell_mask_path, cell_name)\n",
        "            cell = cv.imread(cell_path, cv.IMREAD_GRAYSCALE)\n",
        "            ground_truth_mask = cv.imread(ground_truth_mask_path, cv.IMREAD_GRAYSCALE)\n",
        "            # image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "            # output_folder = None\n",
        "            new_mask = inference_on_image_with_overlap(model, image_path, filter_type)\n",
        "            new_mask[cell == 0] = 0\n",
        "            current_dice_score = compute_dice_score(ground_truth_mask, new_mask)\n",
        "            dice_scores.append(current_dice_score)\n",
        "\n",
        "            if remove_false_fenestrations:\n",
        "                new_mask_filt = remove_fenestrations(new_mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "                current_dice_score_filt = compute_dice_score(ground_truth_mask, new_mask_filt)\n",
        "                dice_scores_filt.append(current_dice_score_filt)\n",
        "                print(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})')\n",
        "                file.write(f'Image Dice score: {round(current_dice_score*100, 1)}, ({round(current_dice_score_filt*100, 1)})\\n')\n",
        "            else:\n",
        "                print(f'Image Dice score: {round(current_dice_score*100, 1)}')\n",
        "                file.write(f'Image Dice score: {round(current_dice_score*100, 1)}\\n')\n",
        "\n",
        "        dice_scores = np.array(dice_scores)\n",
        "        mean_dice = round(np.mean(dice_scores)*100, 1)\n",
        "        std_dice = round(np.std(dice_scores)*100, 1)\n",
        "        if remove_false_fenestrations:\n",
        "            dice_scores_filt = np.array(dice_scores_filt)\n",
        "            mean_dice_filt = round(np.mean(dice_scores_filt)*100, 1)\n",
        "            std_dice_filt = round(np.std(dice_scores_filt)*100, 1)\n",
        "            print(f'{model_name} Mean dice: {mean_dice} +- {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n')\n",
        "            file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice} ({mean_dice_filt} +- {std_dice_filt})\\n\\n')\n",
        "        else:\n",
        "            print(f'{model_name} Mean dice: {mean_dice} +- {std_dice}\\n')\n",
        "            file.write(f'{model_name} Mean dice: {mean_dice} += {std_dice}\\n\\n')\n",
        "\n",
        "# print(f'Mean dice score is {mean_dice}')\n",
        "\n"
      ],
      "metadata": {
        "id": "U1JDFWEQ6hbm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "e8c9e2b5-6f3d-496b-b513-7d7018ef0e49"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['resnet50+imagenet_dice+bce_med5 (1).pth']\n",
            "resnet50+imagenet_dice+bce_med5 (1).pth\n",
            "med5\n",
            "Compare: II_Bleb20_07_mask.tif - II_Bleb20_07.tif - mask_II_Bleb20_07.tif\n",
            "Mean equavalent diameter: 106 nm, std: 38 nm \n",
            "Image Dice score: 92.6, (92.1)\n",
            "Compare: II_Y10_03.tif - II_Y10_03.tif - mask_II_Y10_03.tif\n",
            "Mean equavalent diameter: 156 nm, std: 51 nm \n",
            "Image Dice score: 94.2, (94.1)\n",
            "Compare: IV_Cal10_CB_05.tif - IV_Cal10_CB_05.tif - mask_IV_Cal10_CB_05.tif\n",
            "Mean equavalent diameter: 159 nm, std: 46 nm \n",
            "Image Dice score: 92.9, (93.1)\n",
            "Compare: IV_Cal10_CB_14_mask.tif - IV_Cal10_CB_14.tif - mask_IV_Cal10_CB_14.tif\n",
            "Mean equavalent diameter: 185 nm, std: 65 nm \n",
            "Image Dice score: 85.0, (88.2)\n",
            "Compare: IV_Cal10_CB_16_mask.tif - IV_Cal10_CB_16.tif - mask_IV_Cal10_CB_16.tif\n",
            "Mean equavalent diameter: 153 nm, std: 43 nm \n",
            "Image Dice score: 94.0, (94.0)\n",
            "Compare: IV_Cal10_CB_18_mask.tif - IV_Cal10_CB_18.tif - mask_IV_Cal10_CB_18.tif\n",
            "Mean equavalent diameter: 180 nm, std: 50 nm \n",
            "Image Dice score: 94.6, (95.0)\n",
            "Compare: IV_K_03.tif - IV_K_03.tif - mask_IV_K_03.tif\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5a523a803af5>\u001b[0m in \u001b[0;36m<cell line: 84>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# image_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;31m# output_folder = None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mnew_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_on_image_with_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mnew_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mcurrent_dice_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_dice_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-ad5ce0583c94>\u001b[0m in \u001b[0;36minference_on_image_with_overlap\u001b[0;34m(model, image_path, filter_type)\u001b[0m\n\u001b[1;32m    307\u001b[0m                 \u001b[0msquare_section\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlm_filt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare_section\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mfilter_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'med5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0msquare_section\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedianBlur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msquare_section\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: prehodit tohle, at se to dela jednou pro celej obrazek, ne pro patche?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0msquare_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquare_section\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch and channel dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model('vgg11+none', 0.0, 'dice+bce')\n",
        "loaded_state_dict = torch.load('./gdrive/MyDrive/lsecs/vgg11+imagenet_dice+bce_med5.pth')\n",
        "model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "square_section = cv.imread('./gdrive/MyDrive/lsecs/cropped_selections/patches_med5/val_image_patches/II_Y10_04_3_patch_1_0.tif', cv.IMREAD_GRAYSCALE)\n",
        "square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch  dimension\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = torch.sigmoid(model(square_tensor))\n",
        "    # output = (output > 0.5).float()\n",
        "    output = output.cpu().data.numpy().squeeze(0).squeeze()\n",
        "    output = output*255\n",
        "    cv2_imshow(square_section)\n",
        "    cv2_imshow(output)\n",
        "\n",
        "    # # # Forward pass through the model\n",
        "    # # with torch.no_grad():\n",
        "    # #     output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "    # # Scale the probablity to 0-255\n",
        "\n",
        "    # # output = output.to(torch.uint8)\n",
        "    # # output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "    # # cv2_imshow(output_pil)\n",
        "    # output_probs[x:x+window_size, y:y+window_size] += output*weighting_window\n",
        "    # # Crop\n",
        "    # # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    # output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    # weights *= 255\n",
        "    # # weights = weights[:original_height, :original_width]*255\n",
        "    # # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # # Apply weights\n",
        "    # # output_probs /= weights\n",
        "\n",
        "    # # Create image from mask\n",
        "    # # output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    # output_mask = output_mask.astype(np.uint8)\n",
        "    # return output_mask"
      ],
      "metadata": {
        "id": "du6wjpaN760E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyXUfWY3KtHa"
      },
      "source": [
        "# Bioimageio stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0UWu17Y2fM4"
      },
      "outputs": [],
      "source": [
        "# !pip install \"bioimageio.core>=0.5,<0.6\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7SgQQqm3K8q"
      },
      "outputs": [],
      "source": [
        "# @torch.jit.ignore\n",
        "# def call_np(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class MyModule(nn.Module):\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         done = call_np(tensor)\n",
        "#         print (done)\n",
        "\n",
        "# scripted_module = torch.jit.script(MyModule())\n",
        "# print(scripted_module.forward.graph)\n",
        "# empty_tensor = torch.empty(3, 4)\n",
        "# scripted_module.forward(empty_tensor)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2FcX34DwhgX"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms as transforms\n",
        "# import numpy as np\n",
        "\n",
        "# @torch.jit.ignore\n",
        "# def denoise_image(tensor) -> torch.Tensor:\n",
        "#   na = tensor.numpy()\n",
        "#   # Interesting stuff here\n",
        "#   tt = torch.tensor(na)\n",
        "#   return tt\n",
        "\n",
        "# class FunctionWrapper(nn.Module):\n",
        "#   def __init__(self, model):\n",
        "#     super(FunctionWrapper, self).__init__()\n",
        "#     self.model = model\n",
        "\n",
        "#     @torch.jit.export\n",
        "#     def forward(self, tensor):\n",
        "#         denoised = denoise_image(tensor)\n",
        "#         return self.model(denoised)\n",
        "\n",
        "\n",
        "\n",
        "# device = torch.device('cpu')\n",
        "# model = UNET(in_channels=1, out_channels=1, device='cpu')\n",
        "# model.load_state_dict(torch.load(biomodel_path, map_location=device))\n",
        "# # model.to(device=device)\n",
        "# model = torch.jit.script(model)\n",
        "# # wrapper = FunctionWrapper(model)\n",
        "# wrapper.to(device=device)\n",
        "# # wrapper = PreprocessingWrapper(denoise, model)\n",
        "# # model = torch.jit.script(wrapper)\n",
        "# #\n",
        "# model.eval()\n",
        "# torchscript_weights_path = os.path.join(biomodel_folder, 'torchscript_weights.pt')\n",
        "# torch.jit.save(model, torchscript_weights_path)\n",
        "\n",
        "# preprocessing=[[{\"name\": \"scale_range\",\n",
        "#                  \"kwargs\": {\"axes\": \"xy\",\n",
        "#                           #  \"min_percentile\": min_percentile,\n",
        "#                             # \"max_percentile\": max_percentile,\n",
        "#                             \"mode\": \"per_sample\"\n",
        "#                             }}]]\n",
        "\n",
        "# threshold = 0.5\n",
        "# postprocessing = [[{\"name\": \"binarize\", \"kwargs\": {\"threshold\": threshold}}]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DU4m7qIy7rt"
      },
      "outputs": [],
      "source": [
        "# input = np.random.rand(1, 1, 512, 512).astype(\"float32\")  # an example input\n",
        "# test_inputs = os.path.join(biomodel_folder, \"test-input.npy\")\n",
        "# test_outputs = os.path.join(biomodel_folder, \"test-output.npy\")\n",
        "# np.save(test_inputs, input)\n",
        "# with torch.no_grad():\n",
        "#   output = model(torch.from_numpy(input)).cpu().numpy() # copy to cpu(is on gpu because of jit.script)\n",
        "#   output = output > threshold\n",
        "# np.save(test_outputs, output)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaqoBNRJiNKg"
      },
      "outputs": [],
      "source": [
        "# # create markdown documentation for your model\n",
        "# # this should describe how the model was trained, (and on which data)\n",
        "# # and also what to take into consideration when running the model, especially how to validate the model\n",
        "# # here, we just create a stub documentation\n",
        "# doc_path = os.path.join(biomodel_folder, \"doc.md\")\n",
        "# with open(doc_path, \"w\") as f:\n",
        "#     f.write(\"# My First Model\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfMXWAziiNGI"
      },
      "outputs": [],
      "source": [
        "# from bioimageio.core.build_spec import build_model\n",
        "# import torch\n",
        "# # now we can use the build_model function to create the zipped package.\n",
        "# # it takes the path to the weights and data we have just created, as well as additional information\n",
        "# # that will be used to add metadata to the rdf.yaml file in the model zip\n",
        "# # we only use a subset of the available options here, please refer to the advanced examples and to the\n",
        "# # function signature of build_model in order to get an overview of the full functionality\n",
        "# build_model(\n",
        "#     # the weight file and the type of the weights\n",
        "#     weight_uri= torchscript_weights_path,\n",
        "#     weight_type=\"torchscript\",\n",
        "#     # the test input and output data as well as the description of the tensors\n",
        "#     # these are passed as list because we support multiple inputs / outputs per model\n",
        "#     test_inputs=[test_inputs],\n",
        "#     test_outputs=[test_outputs],\n",
        "#     input_axes=[\"bcyx\"],\n",
        "#     output_axes=[\"bcyx\"],\n",
        "#     # where to save the model zip, how to call the model and a short description of it\n",
        "#     output_path=os.path.join(biomodel_folder,\"model.zip\"),\n",
        "#     name=\"MyFirstModel\",\n",
        "#     description=\"a fancy new model\",\n",
        "#     # additional metadata about authors, licenses, citation etc.\n",
        "#     authors=[{\"name\": \"Gizmo\"}],\n",
        "#     license=\"CC-BY-4.0\",\n",
        "#     documentation=doc_path,\n",
        "#     tags=[\"nucleus-segmentation\"],  # the tags are used to make models more findable on the website\n",
        "#     cite=[{\"text\": \"Gizmo et al.\", \"doi\": \"10.1002/xyzacab123\"}],\n",
        "#     pytorch_version=torch.__version__,\n",
        "#     preprocessing=preprocessing,\n",
        "#     postprocessing=postprocessing\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2RJJ5WriND4"
      },
      "outputs": [],
      "source": [
        "# # finally, we test that the expected outptus are reproduced when running the model.\n",
        "# # the 'test_model' function runs this test.\n",
        "# # it will output a list of dictionaries. each dict gives the status of a different test that is being run\n",
        "# # if all of them contain \"status\": \"passed\" then all tests were successful\n",
        "# from bioimageio.core.resource_tests import test_model\n",
        "# import bioimageio.core\n",
        "# my_model = bioimageio.core.load_resource_description(os.path.join(biomodel_folder,\"model.zip\"))\n",
        "# test_model(my_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oyXUfWY3KtHa"
      ],
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e91477dc6d1845d982c8b738bdb50996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4a4b04d30b74abc80f5671ccc604136",
              "IPY_MODEL_15d74ae8d43e44b68abee90c5c34c33f"
            ],
            "layout": "IPY_MODEL_78faaa9bc53e4f659c5e46acaa12ca07"
          }
        },
        "f4a4b04d30b74abc80f5671ccc604136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5c359570baa415087038a58287c65df",
            "placeholder": "​",
            "style": "IPY_MODEL_294d1946c6074bcba974be82f792f399",
            "value": "0.001 MB of 0.001 MB uploaded\r"
          }
        },
        "15d74ae8d43e44b68abee90c5c34c33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e5555926a44635962fcdc28e279a8d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d651390268b4475cabb9c519ac35c474",
            "value": 1
          }
        },
        "78faaa9bc53e4f659c5e46acaa12ca07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c359570baa415087038a58287c65df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "294d1946c6074bcba974be82f792f399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70e5555926a44635962fcdc28e279a8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d651390268b4475cabb9c519ac35c474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecdb778b7b8d42e18714af7663c359f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_817e20e3a60c4975a5f54c542a82c939",
              "IPY_MODEL_4d92ef158f9441dd9dc230b27f6270bf"
            ],
            "layout": "IPY_MODEL_f3cc425fd231468f990a1bbdf538a7ec"
          }
        },
        "817e20e3a60c4975a5f54c542a82c939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7237da540e73438ebcba8e387a136df0",
            "placeholder": "​",
            "style": "IPY_MODEL_3a577d6dc92c419fb600e45a818f3ab8",
            "value": "0.016 MB of 0.016 MB uploaded\r"
          }
        },
        "4d92ef158f9441dd9dc230b27f6270bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd8bed1ea499430b83f5428311836c02",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f95ce441401e4f5dbe24c336d18e6218",
            "value": 1
          }
        },
        "f3cc425fd231468f990a1bbdf538a7ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7237da540e73438ebcba8e387a136df0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a577d6dc92c419fb600e45a818f3ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd8bed1ea499430b83f5428311836c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f95ce441401e4f5dbe24c336d18e6218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}