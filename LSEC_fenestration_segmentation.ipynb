{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOKhfNGqXiIdMK/JIqLph5h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/LSEC_fenestration_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Automatic segmentation of electron microscope images**"
      ],
      "metadata": {
        "id": "Lyx5iPnyA82x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for using a neural network for the task of binary segmentation of fenestrations of Liver sinusoidal entdothelial cells (LSECs)."
      ],
      "metadata": {
        "id": "tnKtiKypBF_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to use this notebook"
      ],
      "metadata": {
        "id": "w_L_dMb6BQ--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook works with data saved on your Google Drive. Network training requires pairs of images and their corresponding masks saved in two diferent folders. The image-mask pairs don't need to be named exactly the same, but they should correspond when sorted alphabetically."
      ],
      "metadata": {
        "id": "lNAUKzhLBUXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to connect to Google Drive**\n",
        "#@markdown A new window will open where you will be able to connect.\n",
        "\n",
        "#@markdown When you are connected, you can see your Drive content in the left sidebar under **Files**.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "Lgeow57BBU4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "hgpgKViFC1lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Import necessary libraries**\n",
        "!python --version\n",
        "!pip install wandb\n",
        "!pip install torchmetrics\n",
        "!pip install segmentation-models-pytorch\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchmetrics.classification import Dice, BinaryJaccardIndex\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "import shutil\n",
        "import cv2 as cv\n",
        "from numpy.lib.stride_tricks import as_strided\n",
        "import pywt\n",
        "from scipy.stats import norm\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc\n",
        "import wandb\n",
        "from numba import njit\n",
        "from scipy.signal import convolve2d\n",
        "import math\n",
        "\n",
        "# gc.collect()\n",
        "drive.mount('/content/gdrive')\n",
        "model_folder = \"./gdrive/MyDrive/ROI_patches/my_model\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # TODO: do not even try this, if the gpu is not connected\n",
        "print(DEVICE)\n",
        "biomodel_folder = os.path.join(model_folder, \"bioimageio_model\")\n",
        "biomodel_path = os.path.join(biomodel_folder, \"weights.pt\")\n",
        "os.makedirs(biomodel_folder, exist_ok=True)\n",
        "LOAD_TRAINED_MODEL = False\n",
        "model_path = os.path.join(model_folder,\"my_checkpoint.pth.tar\")"
      ],
      "metadata": {
        "id": "APb3jJ72BTfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "6MS6VgDyCDfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transofrm=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transofrm\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask[mask == 255.0] = 1\n",
        "        mask /= 255\n",
        "        return image, mask\n",
        "\n",
        "class TransformDataset(Dataset):\n",
        "    def __init__(self, dataset, transform):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, mask = self.dataset[index]\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "        return image, mask\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "\n",
        "def get_loaders(img_dir, mask_dir, split, batch_size, num_workers=4, pin_memory=True): # TODO: check these parameters\n",
        "    data = MyDataset(\n",
        "        image_dir=img_dir,\n",
        "        mask_dir=mask_dir,\n",
        "        transofrm=None\n",
        "    )\n",
        "\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    train_indices, test_indices = train_test_split(\n",
        "        range(len(data)),\n",
        "        test_size=split,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_data = TransformDataset(Subset(data, train_indices), train_transform)\n",
        "    val_data = TransformDataset(Subset(data, test_indices), val_transform)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, train_indices\n",
        "\n",
        "def get_transforms():\n",
        "    train_transform = A.Compose( # TODO: background(preprocessing?), intensity\n",
        "        [\n",
        "            A.Rotate(limit=35, p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            # A.Affine(shear=(0.5,1)),\n",
        "            # A.Affine(scale=(-10, 10)),\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0, # normalization to [0, 1]\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_transform = A.Compose(\n",
        "        [\n",
        "            A.Normalize(\n",
        "                mean = 0.0,\n",
        "                std = 1.0,\n",
        "                max_pixel_value=255.0,\n",
        "            ),\n",
        "            ToTensorV2()\n",
        "        ]\n",
        "    )\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# test_transform = A.Compose(\n",
        "#     [\n",
        "#     A.Normalize(\n",
        "#       mean = 0.0,\n",
        "#       std = 1.0,\n",
        "#       max_pixel_value=255.0,\n",
        "#     ),\n",
        "#         ToTensorV2()\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add more transformations if needed\n",
        "])\n",
        "\n",
        "\n",
        "def merge_images(image, mask):\n",
        "    merge = np.zeros((mask.shape[0], mask.shape[1], 3))\n",
        "    merge[:, :, 0] = image # B channel (0, 1, 2) = (B, G, R)\n",
        "    merge[:, :, 2] = image # R channel\n",
        "    merge[:, :, 1] = mask # G channel\n",
        "    merge[:, :, 2][mask == 255.0] = 255 # R channel\n",
        "    merge = merge.astype('uint8')\n",
        "    return merge\n",
        "\n",
        "\n",
        "def merge_original_mask(image_path, mask_path, output_folder):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    merge = merge_images(image, mask)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_original_mask_merge\"+ext), merge)\n",
        "\n",
        "\n",
        "def merge_masks(mask1_path, mask2_path, output_folder):\n",
        "    mask1 = cv.imread(mask1_path, cv.IMREAD_GRAYSCALE)\n",
        "    mask2 = cv.imread(mask2_path, cv.IMREAD_GRAYSCALE)\n",
        "    # merge = merge_images(image, mask)\n",
        "    merge = np.zeros((mask1.shape[0], mask1.shape[1], 3))\n",
        "\n",
        "    merge[:, :, 1][mask1 == 255.0] = 255\n",
        "    merge[:, :, 2][mask2 == 255.0] = 255\n",
        "\n",
        "    filename_ext = os.path.basename(mask1_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_mask_compare\"+ext), merge)\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((middle*255).astype(np.uint8))\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((left*255).astype(np.uint8))\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((right*255).astype(np.uint8))\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top*255).astype(np.uint8))\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom*255).astype(np.uint8))\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "    # cv2_imshow((top_right*255).astype(np.uint8))\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_left*255).astype(np.uint8))\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "    # cv2_imshow((bottom_right*255).astype(np.uint8))\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - border_size:height, :])\n",
        "    bottom_zeros = np.zeros((bottom_edge-border_size, width), dtype = image.dtype)\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border, bottom_zeros))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - border_size:width])\n",
        "    right_zeros = np.zeros((top_bottom_mirrored.shape[0], right_edge-border_size), dtype = image.dtype)\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border, right_zeros))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "    window_size = 512\n",
        "    oh, ow = 50, 50\n",
        "    # out_crop =\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "    # bottom_edge = (image_height + oh) % (window_size - oh)\n",
        "    # right_edge = (image_height + ow) % (window_size - ow)\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    # print(mirrored_image.shape)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    # tryout = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "            # print(x, y)\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    # if original_height != window_size:\n",
        "                    weighting_window = top_left\n",
        "                    # print('top left')\n",
        "                elif y == image_width - window_size:\n",
        "                    # print('top right')\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "                    # print('top ')\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                    # print('bottom left')\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                    # print('bottom right')\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "                    # print('bottom')\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "                # print('left')\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "                # print('right')\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "                # print('middle')\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "            square_section = preprocess_image(square_section) # TODO: prehodit tohle, at se to dela jednou pro celej obrazek, ne pro patche?\n",
        "            square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            # output = output.to(torch.uint8)\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            # cv2_imshow(output_pil)\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "    # Crop\n",
        "    # cv.imwrite(os.path.join(output_folder, \"probs\"+\".png\"), output_probs)\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "    # weights = weights[:original_height, :original_width]*255\n",
        "    # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "    # Apply weights\n",
        "    # output_probs /= weights\n",
        "\n",
        "    # Create image from mask\n",
        "    output_mask = np.where(output_probs > 127, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    filename_ext = os.path.basename(image_path)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_mirrored\"+ext), mirrored_image)\n",
        "\n",
        "    # Merge image with created mask\n",
        "    out_mask_path = os.path.join(output_folder, filename+\"_new_mask\"+ext)\n",
        "    merge = merge_images(input_image, output_mask)\n",
        "    cv.imwrite(os.path.join(output_folder, filename+\"_new_mask_merge\"+ext), merge)\n",
        "\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "    cv.imwrite(out_mask_path, output_mask)\n",
        "    # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "    return out_mask_path\n",
        "\n",
        "# def inference_on_image_with_overlap(model, image_path, output_folder):\n",
        "#     window_size = 512\n",
        "#     oh, ow = 124, 124\n",
        "#     input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "#     image_height, image_width = input_image.shape\n",
        "#     original_height, original_width = image_height, image_width\n",
        "#     bottom_edge = image_height % (window_size - oh)\n",
        "#     right_edge = image_width % (window_size - ow)\n",
        "#     mirrored_image = np.zeros((image_height+bottom_edge, image_width+right_edge)).astype(np.uint8)\n",
        "#     mirrored_image[:image_height, :image_width] = input_image\n",
        "#     mirrored_image[image_height:, :image_width] = np.flipud(input_image[image_height-bottom_edge:, :])\n",
        "#     mirrored_image[:, image_width:] = np.fliplr(mirrored_image[:, image_width-right_edge:image_width])\n",
        "#     image_height += bottom_edge\n",
        "#     image_width += right_edge\n",
        "#     weights = np.zeros((image_height, image_width))\n",
        "#     # tryout = np.zeros((image_height, image_width))\n",
        "#     output_probs = np.zeros((image_height, image_width))\n",
        "#     output_mask = np.zeros((image_height, image_width))\n",
        "#     middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "#     for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "#         for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "#             # Choose weighting window\n",
        "#             if x == 0:\n",
        "#                 if y == 0:\n",
        "#                     if original_height != window_size:\n",
        "#                         weighting_window = top_left\n",
        "#                     else:\n",
        "#                         weighting_window = np.ones((window_size, window_size))\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = top_right\n",
        "#                 else:\n",
        "#                     weighting_window = top\n",
        "#             elif x == window_size - oh - 1:\n",
        "#                 if y == 0:\n",
        "#                     weighting_window = bottom_left\n",
        "#                 elif y == window_size - ow - 1:\n",
        "#                     weighting_window = bottom_right\n",
        "#                 else:\n",
        "#                     weighting_window = bottom\n",
        "#             elif y == 0:\n",
        "#                 weighting_window = left\n",
        "#             elif y == window_size - ow - 1:\n",
        "#                 weighting_window = right\n",
        "#             else:\n",
        "#                 weighting_window = middle\n",
        "#             square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "#             weights[x:x + window_size, y:y + window_size] = weighting_window\n",
        "#             # tryout[x:x + window_size, y:y + window_size] += np.ones((window_size, window_size))*weighting_window\n",
        "#             square_section = preprocess_image(square_section)\n",
        "#             square_tensor = test_transform(square_section).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
        "\n",
        "#             # Forward pass through the model\n",
        "#             with torch.no_grad():\n",
        "#                 output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "#             # Scale the probablity to 0-255\n",
        "#             output = output*255\n",
        "#             output = output.to(torch.uint8)\n",
        "#             output_pil = output.squeeze(0).cpu().numpy()\n",
        "#             output_probs[x:x+window_size, y:y+window_size] += output_pil.squeeze()*weighting_window\n",
        "#     # Crop\n",
        "#     output_probs = output_probs[:original_height, :original_width]\n",
        "#     # weights = weights[:original_height, :original_width]*255\n",
        "#     # tryout = tryout[:original_height, :original_width]*255\n",
        "\n",
        "#     # Apply weights\n",
        "#     # output_probs /= weights\n",
        "\n",
        "#     # Create image from mask\n",
        "#     output_mask = np.where(output_probs > 127, 255, 0)\n",
        "#     output_mask = output_mask.astype(np.uint8)\n",
        "#     filename_ext = os.path.basename(image_path)\n",
        "#     filename, ext = os.path.splitext(filename_ext)\n",
        "\n",
        "#     # Merge image with created mask\n",
        "#     out_mask_path = os.path.join(output_folder, filename+\"_mask\"+ext)\n",
        "#     merge = merge_images(input_image, output_mask)\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_merge\"+ext), merge)\n",
        "\n",
        "#     cv.imwrite(os.path.join(output_folder, filename+\"_probs\"+ext), output_probs)\n",
        "#     cv.imwrite(out_mask_path, output_mask)\n",
        "#     # cv.imwrite(os.path.join(output_folder, filename+\"_weights\"+ext), weights)\n",
        "#     return out_mask_path\n",
        "\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = nlm_filt(image)\n",
        "    # image = wavelet_denoise(image, threshold=1.5)\n",
        "    # image = apply_clahe(image)\n",
        "    # image = cv.medianBlur(image, 7)\n",
        "    return image\n",
        "\n",
        "\n",
        "def apply_clahe(image):\n",
        "    clahe = cv.createCLAHE(clipLimit=0.8, tileGridSize=(8, 8))\n",
        "    clahe_image = clahe.apply(image)\n",
        "    return clahe_image\n",
        "\n",
        "\n",
        "def create_image_patches(image_folder, mask_folder, output_folder, patch_size):\n",
        "    image_patches_path = os.path.join(output_folder,'image_patches')\n",
        "    mask_patches_path = os.path.join(output_folder,'mask_patches')\n",
        "    # rejected_path = os.path.join(output_folder,'rejected')\n",
        "    # print(image_path)\n",
        "\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    if os.path.exists(image_patches_path):\n",
        "        shutil.rmtree(image_patches_path)\n",
        "    os.mkdir(image_patches_path)\n",
        "    if os.path.exists(mask_patches_path):\n",
        "        shutil.rmtree(mask_patches_path)\n",
        "    os.mkdir(mask_patches_path)\n",
        "    # if os.path.exists(rejected_path):\n",
        "    #     shutil.rmtree(rejected_path)\n",
        "    # os.mkdir(rejected_path)\n",
        "\n",
        "    patch_area = patch_size**2\n",
        "    fenestration_area_thresh = 0.0 #0.01\n",
        "    image_filenames = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
        "    image_filenames = sorted(image_filenames)\n",
        "    mask_filenames = [f for f in os.listdir(mask_folder) if os.path.isfile(os.path.join(mask_folder, f))]\n",
        "    mask_filenames = sorted(mask_filenames)\n",
        "\n",
        "    for image_name, mask_name in zip(image_filenames, mask_filenames):\n",
        "        # if image_name.endswith(\".tif\"): # TODO: tohle mozna odstranit\n",
        "        input_path = os.path.join(image_folder, image_name)\n",
        "        mask_path = os.path.join(mask_folder, mask_name)\n",
        "\n",
        "        img = cv.imread(input_path, cv.IMREAD_GRAYSCALE)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "        height, width = img.shape\n",
        "\n",
        "        shape = (height // patch_size, width // patch_size, patch_size, patch_size)\n",
        "        strides = (patch_size * width , patch_size , width, 1)\n",
        "        # strides = (patch_size * width , patch_size)\n",
        "\n",
        "        # img_strided = as_strided(img, shape=(width//patch_size, height//patch_size, patch_size, patch_size),\n",
        "        #              strides=img.strides + img.strides, writeable=False)\n",
        "        img_strided = as_strided(img, shape=shape,\n",
        "                        strides=strides, writeable=False) #TODO: check if the patches do not overlap\n",
        "        mask_strided = as_strided(mask, shape=shape,\n",
        "                        strides=strides, writeable=False)\n",
        "\n",
        "        for i in range(img_strided.shape[0]):\n",
        "            for j in range(img_strided.shape[1]):\n",
        "                img_patch = img_strided[i, j]\n",
        "                mask_patch = mask_strided[i, j]\n",
        "                # Compute the percentage of white pixels\n",
        "                fenestration_area = np.sum(mask_patch == 255)\n",
        "                # print(fenestration_area)\n",
        "                # fenestration_percentage = fenestration_area/patch_area\n",
        "                if fenestration_area >= fenestration_area_thresh:\n",
        "                    patch_filename = f\"{os.path.splitext(os.path.basename(image_name))[0]}_patch_{i}_{j}.tif\"\n",
        "                    # preprocess image\n",
        "                    img_patch = preprocess_image(img_patch)\n",
        "                    cv.imwrite(os.path.join(image_patches_path, patch_filename), img_patch)\n",
        "                    cv.imwrite(os.path.join(mask_patches_path, patch_filename), mask_patch)\n",
        "                    # print(\"written patch \", patch_filename)\n",
        "                else:\n",
        "                    print(\"not writing patch\")\n",
        "    return image_patches_path, mask_patches_path\n",
        "\n",
        "\n",
        "# Denoising\n",
        "#   References for non-local means filtering and noise variance estimation:\n",
        "#\n",
        "#   [1] Antoni Buades, Bartomeu Coll, and Jean-Michel Morel, A Non-Local\n",
        "#       Algorithm for Image Denoising, Computer Vision and Pattern\n",
        "#       Recognition 2005. CVPR 2005, Volume 2, (2005), pp. 60-65.\n",
        "#   [2] John Immerkaer, Fast Noise Variance Estimation, Computer Vision and\n",
        "#       Image Understanding, Volume 64, Issue 2, (1996), pp. 300-302\n",
        "\n",
        "def estimate_degree_of_smoothing(I): # This is how the estimation is done in Matlab (see imnlmfilt in Matlab)\n",
        "    H, W = I.shape\n",
        "    I = I.astype(np.float32)\n",
        "    kernel = np.array([[1, -2, 1], [-2, 4, -2], [1, -2, 1]])\n",
        "    conv_result = np.abs(convolve2d(I[:, :], kernel, mode='valid'))\n",
        "    res = np.sum(conv_result)\n",
        "    degree_of_smoothing = (res * np.sqrt(0.5 * np.pi) / (6 * (W - 2) * (H - 2)))\n",
        "    if degree_of_smoothing == 0:\n",
        "        degree_of_smoothing = np.finfo(np.float32).eps\n",
        "    return degree_of_smoothing\n",
        "\n",
        "\n",
        "def nlm_filt(image):\n",
        "    window_size = 5\n",
        "    search_window_size = 21\n",
        "    degree_of_smoothing = estimate_degree_of_smoothing(image)\n",
        "    image = cv.fastNlMeansDenoising(image, None, h = degree_of_smoothing, templateWindowSize = 5, searchWindowSize = 21)\n",
        "    return image\n",
        "\n",
        "\n",
        "def anscombe_transform(data):\n",
        "    return 2.0 * np.sqrt(data + 3.0/8.0)\n",
        "\n",
        "\n",
        "def inverse_anscombe_transform(data):\n",
        "    # Reference\n",
        "    # https://github.com/broxtronix/pymultiscale/blob/master/pymultiscale/anscombe.py\n",
        "    return (1.0/4.0 * np.power(data, 2) +\n",
        "        1.0/4.0 * np.sqrt(3.0/2.0) * np.power(data, -1.0) -\n",
        "        11.0/8.0 * np.power(data, -2.0) +\n",
        "        5.0/8.0 * np.sqrt(3.0/2.0) * np.power(data, -3.0) - 1.0 / 8.0)\n",
        "\n",
        "\n",
        "def wavelet_denoising(data, threshold=1.5, wavelet='coif4', threshold_type='soft'):\n",
        "    coeffs = pywt.wavedec2(data, wavelet = wavelet, level=3)\n",
        "    coeffs[-1] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-1])\n",
        "    coeffs[-2] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-2])\n",
        "    coeffs[-3] = tuple(pywt.threshold(c, threshold, threshold_type) for c in coeffs[-3])\n",
        "    return pywt.waverec2(coeffs, wavelet)\n",
        "\n",
        "\n",
        "def wavelet_denoise(image, threshold):\n",
        "    image = anscombe_transform(image)\n",
        "    image = wavelet_denoising(image, threshold)\n",
        "    image = inverse_anscombe_transform(image)\n",
        "    # TODO: not sure this is the correct way how to do this\n",
        "    image = image/np.max(image)*255\n",
        "    return image.astype(np.uint8)"
      ],
      "metadata": {
        "id": "y8BfGPazDVzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data utils"
      ],
      "metadata": {
        "id": "1O_7FnBDCMDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training utils"
      ],
      "metadata": {
        "id": "uC_5EJ4EDN_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the official implementation of BoundaryDOULoss https://arxiv.org/pdf/2308.00220.pdf\n",
        "# Taken from: https://github.com/sunfan-bvb/BoundaryDoULoss/tree/main\n",
        "class BoundaryDoULoss(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(BoundaryDoULoss, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def _one_hot_encoder(self, input_tensor):\n",
        "        tensor_list = []\n",
        "        for i in range(self.n_classes):\n",
        "            temp_prob = input_tensor == i\n",
        "            tensor_list.append(temp_prob.unsqueeze(1))\n",
        "        output_tensor = torch.cat(tensor_list, dim=1)\n",
        "        return output_tensor.float()\n",
        "\n",
        "    def _adaptive_size(self, score, target):\n",
        "        kernel = torch.Tensor([[0,1,0], [1,1,1], [0,1,0]])\n",
        "        padding_out = torch.zeros((target.shape[0], target.shape[-2]+2, target.shape[-1]+2))\n",
        "        padding_out[:, 1:-1, 1:-1] = target\n",
        "        h, w = 3, 3\n",
        "\n",
        "        Y = torch.zeros((padding_out.shape[0], padding_out.shape[1] - h + 1, padding_out.shape[2] - w + 1)).cuda()\n",
        "        for i in range(Y.shape[0]):\n",
        "            Y[i, :, :] = torch.conv2d(target[i].unsqueeze(0).unsqueeze(0), kernel.unsqueeze(0).unsqueeze(0).cuda(), padding=1)\n",
        "        Y = Y * target\n",
        "        Y[Y == 5] = 0\n",
        "        C = torch.count_nonzero(Y)\n",
        "        S = torch.count_nonzero(target)\n",
        "        smooth = 1e-5\n",
        "        alpha = 1 - (C + smooth) / (S + smooth)\n",
        "        alpha = 2 * alpha - 1\n",
        "\n",
        "        intersect = torch.sum(score * target)\n",
        "        y_sum = torch.sum(target * target)\n",
        "        z_sum = torch.sum(score * score)\n",
        "        alpha = min(alpha, 0.8)  ## We recommend using a truncated alpha of 0.8, as using truncation gives better results on some datasets and has rarely effect on others.\n",
        "        loss = (z_sum + y_sum - 2 * intersect + smooth) / (z_sum + y_sum - (1 + alpha) * intersect + smooth)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def forward(self, inputs, target):\n",
        "        inputs = torch.softmax(inputs, dim=1)\n",
        "        target = self._one_hot_encoder(target)\n",
        "\n",
        "        assert inputs.size() == target.size(), 'predict {} & target {} shape do not match'.format(inputs.size(), target.size())\n",
        "\n",
        "        loss = 0.0\n",
        "        for i in range(0, self.n_classes):\n",
        "            loss += self._adaptive_size(inputs[:, i], target[:, i])\n",
        "        return loss / self.n_classes\n",
        "\n",
        "\n",
        "def save_checkpoint(model, model_path):#, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    model.save(model_path)\n",
        "    # torch.save(state, filename)\n",
        "\n",
        "def save_state_dict(model, model_path):\n",
        "    print(\"=> Saving checkpoint\")\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "\n",
        "def load_state_dict(model, model_path):\n",
        "    print(\"=> Loading checkpoint\")\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "def validate_model(model, loader, loss_fn):\n",
        "    num_correct = 0\n",
        "    num_pixels = 0\n",
        "    dice_score = 0\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(loader):\n",
        "            x = x.to(DEVICE)\n",
        "            y = y.to(DEVICE).unsqueeze(1)\n",
        "            # Forward\n",
        "            preds = model(x)\n",
        "            # loss_fn = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(16))\n",
        "            # loss = loss_fn(preds, y)\n",
        "            loss = get_loss(preds, y, loss_fn)\n",
        "            running_loss += loss.cpu()\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "            # num_correct += (preds == y).sum()\n",
        "            # num_pixels += torch.numel(preds)\n",
        "            dice_score += (2*(preds*y).sum()) / (preds+y).sum() + 1e-8 # this is a better predictor\n",
        "    # print(\n",
        "    #     f\"Got {num_correct}/{num_pixels} with acc {num_correct/num_pixels*100:.2f} ()\"\n",
        "    # )\n",
        "    dice_score = dice_score/(idx+1)\n",
        "    val_loss = running_loss/(idx+1)\n",
        "    # dice_score = dice_score/len(loader)\n",
        "    # val_loss = running_loss/len(loader) #TODO: not sure this is correct(dividing by batch size?)\n",
        "    # print(f\"Dice score is {dice_score}\")\n",
        "    # val_losses.append(running_loss/len(loader))\n",
        "    # dice_scores.append(dice_score.cpu())\n",
        "    model.train()\n",
        "    return val_loss, dice_score.cpu()\n",
        "\n",
        "\n",
        "\n",
        "# def save_predictions_as_imgs(\n",
        "#         loader, model, folder=\"saved_images\", device=\"cpu\"\n",
        "# ):\n",
        "#     model.eval()\n",
        "#     for idx, (x, y) in enumerate(loader):\n",
        "#         x = x.to(device=device)\n",
        "#         with torch.no_grad():\n",
        "#             preds = torch.sigmoid(model(x))\n",
        "#             preds = (preds > 0.5).float()\n",
        "#         # print(f\"preds max{preds.max()}\")\n",
        "#         # print(f\"y max {y.max()}\")\n",
        "#         # torchvision.utils.save_image(preds, os.path.join(folder, f\"pred{idx}.png\"))\n",
        "#         # torchvision.utils.save_image(y.unsqueeze(1), os.path.join(folder, f\"pred{idx}_correct.png\"))\n",
        "#             imshow(preds)\n",
        "#             imshow(y.unsqueeze(1))\n",
        "#         break # TODO: change this so it does not loop\n",
        "#     model.train()\n",
        "#     print(\"Saving prediction as images.\")\n",
        "\n",
        "def view_prediction(loader, model, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    for idx, (x, y) in enumerate(loader):\n",
        "        x = x.to(device=device)\n",
        "        with torch.no_grad():\n",
        "            # output = torch.softmax(model(x), dim=1)\n",
        "            output = torch.sigmoid(model(x))\n",
        "            preds = (output > 0.5).float()\n",
        "            preds = preds.cpu().data.numpy()\n",
        "            output = output.cpu().data.numpy()\n",
        "            for i in range(preds.shape[0]):\n",
        "                f=plt.figure(figsize=(128,32))\n",
        "                # Original image\n",
        "                plt.subplot(1,5*preds.shape[0],i+1)\n",
        "                x = x.cpu()\n",
        "                plt.imshow(x[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Validation image')\n",
        "                # NN output(probability)\n",
        "                plt.subplot(1,5*preds.shape[0],i+2)\n",
        "                plt.imshow(output[i, 0, :, :], interpolation='nearest', cmap='magma') # preds is a batch\n",
        "                plt.title('NN output')\n",
        "                # Segmentation\n",
        "                plt.subplot(1,5*preds.shape[0],i+3)\n",
        "                plt.imshow(preds[i, 0, :, :], cmap='gray') # preds is a batch\n",
        "                plt.title('Prediction')\n",
        "                # True mask\n",
        "                plt.subplot(1,5*preds.shape[0],i+4)\n",
        "                plt.imshow(y.unsqueeze(1)[i, 0, :, :], cmap='gray')\n",
        "                plt.title('Ground truth')\n",
        "                # IoU\n",
        "                plt.subplot(1,5*preds.shape[0],i+5)\n",
        "                im1 = y.unsqueeze(1)[i, 0, :, :]\n",
        "                im2 = preds[i, 0, :, :]\n",
        "                plt.imshow(im1, alpha=0.8, cmap='Blues')\n",
        "                plt.imshow(im2, alpha=0.6,cmap='Oranges')\n",
        "                plt.title('IoU')\n",
        "\n",
        "            plt.show()\n",
        "            break # TODO: change this so it does not loop\n",
        "    model.train()\n",
        "\n",
        "\n",
        "# def getClassWeights(mask_path, train_indices):\n",
        "#     mask_dir_list = sorted(os.listdir(mask_path))\n",
        "#     class_count = np.zeros(2, dtype=int)\n",
        "#     for i in train_indices:\n",
        "#         mask = cv.imread(os.path.join(mask_path, mask_dir_list[i]), cv.IMREAD_GRAYSCALE) #np.array(Image.open(os.path.join(mask_path, mask_dir_list[i])).convert('L'), dtype=np.float32)\n",
        "#         mask[mask == 255.0] = 1\n",
        "#         class_count[0] += mask.shape[0]*mask.shape[1] - mask.sum()\n",
        "#         class_count[1] += mask.sum()\n",
        "\n",
        "#     n_samples = class_count.sum()\n",
        "#     n_classes = 2\n",
        "\n",
        "#     class_weights = n_samples / (n_classes * class_count)\n",
        "#     return torch.from_numpy(class_weights)\n"
      ],
      "metadata": {
        "id": "waQz3TZFCKcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug"
      ],
      "metadata": {
        "id": "FnaUJmICDfbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz7EFi-iA3j7"
      },
      "outputs": [],
      "source": [
        "from matplotlib.ticker import PercentFormatter\n",
        "\n",
        "def show_fitted_ellipses(image_path, ellipses):\n",
        "    image = cv.imread(image_path)\n",
        "    for ellipse in ellipses:\n",
        "        if ellipse is not None:\n",
        "            cv.ellipse(image, ellipse, (0, 0, 255), 1)\n",
        "            center, axes, angle = ellipse\n",
        "            center_x, center_y = center\n",
        "            major_axis_length, minor_axis_length = axes\n",
        "            rotation_angle = angle\n",
        "            # print(center_x, center_y)\n",
        "            cv.circle(image, (int(center_x), int(center_y)),radius=1, color=(0, 0, 255), thickness=-1)\n",
        "\n",
        "        # print(\"Center:\", center)\n",
        "        # print(\"Major Axis Length:\", major_axis_length)\n",
        "        # print(\"Minor Axis Length:\", minor_axis_length)\n",
        "        # print(\"Rotation Angle:\", rotation_angle)\n",
        "\n",
        "    cv2_imshow(image)\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            # ellipse = cv.minAreaRect(cnt) # the fitEllipse functions fails sometimes(when the fenestration is on the edge and only a part of it is visible)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            # print(dist)\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "            else:\n",
        "                ellipses.append(None)\n",
        "        else:\n",
        "            ellipses.append(None)\n",
        "    return ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "    # image = cv.cvtColor(seg_mask, cv.COLOR_GRAY2RGB)\n",
        "    # image_el = image.copy()\n",
        "    # cv.drawContours(image, contours, -1, (0, 0, 255), 1)\n",
        "    # cv2_imshow(image)\n",
        "\n",
        "    # Remove noise and small artifacts\n",
        "    # min_contour_area = 10\n",
        "    # filtered_contours = [cnt for cnt in contours if cv.contourArea(cnt) > min_contour_area]\n",
        "    # return filtered_contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    # print(contour_centers)\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n",
        "\n",
        "import seaborn as sns\n",
        "import itertools\n",
        "\n",
        "\n",
        "\n",
        "def show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness=0, min_d=None, max_d=None):\n",
        "    palette = itertools.cycle(sns.color_palette())\n",
        "    plt.figure(figsize=(21, 5))\n",
        "\n",
        "    # Plot histogram of fenestration areas\n",
        "    plt.subplot(1, 4, 1)\n",
        "    sns.histplot(fenestration_areas, stat='probability')\n",
        "    # plt.hist(fenestration_areas, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of areas of fitted elipses\n",
        "    plt.subplot(1, 4, 2)\n",
        "    sns.histplot(fenestration_areas_from_ellipses, stat='probability', color=next(palette)) # this will be the first color (blue)\n",
        "    # plt.hist(fenestration_areas_from_ellipses, bins=20, color='red', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Fenestration Areas (fitted ellipses)')\n",
        "    plt.xlabel('Area ($\\mathrm{nm}^2$)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Plot histogram of roundness\n",
        "    plt.subplot(1, 4, 3)\n",
        "    r = sns.histplot(roundness_of_ellipses, stat='probability', color=next(palette), binwidth=0.025)\n",
        "    r.set(xlim=(min_roundness, None))\n",
        "    # plt.hist(roundness_of_ellipses, bins=10, color='blue', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Roundness')\n",
        "    plt.xlabel('Roundness (-)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "    # print(np.array(roundness_of_ellipses).max())\n",
        "\n",
        "    # Plot histogram of equivalent circle diameters\n",
        "    plt.subplot(1, 4, 4)\n",
        "    d = sns.histplot(equivalent_diameters, stat='probability', color=next(palette), binwidth=10)\n",
        "    d.set(xlim=(0, max_d))\n",
        "    # plt.hist(equivalent_diameters, bins=20, color='green', edgecolor='black', density=density)\n",
        "    plt.title('Histogram of Equivalent Circle Diameters')\n",
        "    plt.xlabel('Diameter (nm)')\n",
        "    # plt.ylabel('Frequency')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    # plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
        "\n",
        "\n",
        "\n",
        "# Mask statistics debug\n",
        "# One pixel corresponds to 10.62 nm\n",
        "image_path = \"./gdrive/MyDrive/ROIs_manually_corrected/augment_mask/_0_379.tif\"\n",
        "image_path = \"./gdrive/MyDrive/lsec_test/old11_CA150_NE_01_original_mask.tif\" # Image from semiautomatic labeling\n",
        "\n",
        "\n",
        "pixel_size_nm = 10.62\n",
        "contours = find_fenestration_contours(image_path)\n",
        "fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "contour_centers = find_contour_centers(contours)\n",
        "ellipses = fit_ellipses(contours, contour_centers)\n",
        "\n",
        "# Show image of fitted ellipses\n",
        "# show_fitted_ellipses(image_path, ellipses)\n",
        "\n",
        "roundness_of_ellipses = []\n",
        "equivalent_diameters = []\n",
        "fenestration_areas_from_ellipses = []\n",
        "\n",
        "for ellipse in ellipses:\n",
        "    center, axes, angle = ellipse\n",
        "    # center_x, center_y = center\n",
        "    major_axis_length, minor_axis_length = axes\n",
        "    roundness = minor_axis_length/major_axis_length\n",
        "    roundness_of_ellipses.append(roundness)\n",
        "    # rotation_angle = angle\n",
        "    diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "    equivalent_diameters.append(diameter)\n",
        "    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "\n",
        "# show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters)\n",
        "\n",
        "\n",
        "# Display the number of circles and their fitted ellipses\n",
        "print(\"Number of fenestrations:\", len(contours))\n",
        "print(\"Number of fitted ellipses:\", len(ellipses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insert input and output folders"
      ],
      "metadata": {
        "id": "CYjQKLjXBRrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "#@markdown Insert folder containing LSEC images:\n",
        "input_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "#@markdown Insert where to save the output masks (the folder will be created if it does not exist yet):\n",
        "output_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "\n",
        "input_folder = input_folder.strip()\n",
        "output_folder = output_folder.strip()\n",
        "\n",
        "if not os.path.exists(input_folder):\n",
        "    print(\"Input folder does not exist\")\n",
        "    exit()\n",
        "if not os.path.exists(ground_truth_mask_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f'Created {output_folder}')"
      ],
      "metadata": {
        "id": "sTqRswq5BJwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get model weights"
      ],
      "metadata": {
        "id": "nJBuaigTF8jZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Insert model weights path:\n",
        "model_path = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "model_path = model_path.strip()"
      ],
      "metadata": {
        "id": "fwq57J3LF7oE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segment images"
      ],
      "metadata": {
        "id": "K3QMfzlNFa9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown Run segmentation:\n",
        "image_names = [f for f in sorted(os.listdir(input_folder)) if os.path.isfile(os.path.join(input_folder, f))]\n",
        "\n",
        "model = build_model('vgg19+imagenet', 0.0, 'dice')\n",
        "if torch.cuda.is_available():\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "for image_name in image_names:\n",
        "    print(image_name)\n",
        "    image_path = os.path.join(input_folder, image_name)\n",
        "    out_mask_path = inference_on_image_with_overlap(model, image_path, output_folder)\n",
        "    print(f'Saving {out_mask_path}')"
      ],
      "metadata": {
        "id": "OSv_W-oXFaKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Exclude fenestrations based on diameter and roundness**"
      ],
      "metadata": {
        "id": "07-2MApvGm_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert the pixel size, and min and max fenestration diameters in nanometers:**\n",
        "\n",
        "#@markdown All fenestration with a smaller or larger diameter than the chosen range will be removed from the crated masks.\n",
        "#@markdown (Use dot '.' as the decimal separator, not comma ',').\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "pixel_size_nm = 10.62 #@param {type:\"number\"}\n",
        "min_diameter_nm = 105 #@param {type:\"number\"}\n",
        "max_diameter_nm = 500 #@param {type:\"number\"}\n",
        "min_roundness = 0 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "mask_path = './gdrive/MyDrive/lsecs/mask_edit_test' #@param {type:\"string\"}\n",
        "#@markdown If this is checked, the old masks will be deleted.\n",
        "rewrite_images = False # @param {type:\"boolean\"}\n",
        "mask_path = mask_path.strip()\n",
        "mask_names = sorted([f for f in os.listdir(mask_path) if os.path.isfile(os.path.join(mask_path, f))])\n",
        "\n",
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "\n",
        "def remove_fenestrations(mask_path, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    contours = find_fenestration_contours(mask_path)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "    mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_fitted_ellipses(mask_path, ellipses)\n",
        "\n",
        "    # Remove all contours that do not fit the chosen conditions\n",
        "    # Also remove all contours that were too small to fit an ellipse\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse is not None:\n",
        "            center, axes, angle = ellipse\n",
        "            # center_x, center_y = center\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            # print(axes)\n",
        "            roundness = minor_axis_length/major_axis_length\n",
        "            if roundness >= min_roundness:\n",
        "                roundness_of_ellipses.append(roundness)\n",
        "            # rotation_angle = angle\n",
        "            diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "            # print(contour)\n",
        "            # print(diameter)\n",
        "            if diameter < min_d or diameter > max_d or roundness < min_roundness or np.isnan(diameter):\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "            else:\n",
        "                equivalent_diameters.append(diameter)\n",
        "                fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    # cv2_imshow(mask)\n",
        "    # show_statistics(fenestration_areas, fenestration_areas_from_ellipses, roundness_of_ellipses, equivalent_diameters, min_roundness, min_diameter_nm, max_diameter_nm)\n",
        "    equivalent_diameters = np.array(equivalent_diameters)\n",
        "    # print(equivalent_diameters)\n",
        "    if len(equivalent_diameters) > 0:\n",
        "        mean = int(np.nanmean(equivalent_diameters) + 0.5) # This is how to round numbers in python...\n",
        "        std = int(np.nanstd(equivalent_diameters) + 0.5)\n",
        "        print(f\"Mean equavalent diameter: {mean} nm, std: {std} nm \")\n",
        "    return mask\n",
        "\n",
        "\n",
        "#TODO: ukazat statistiky pro celou slozku obrazku\n",
        "\n",
        "if not rewrite_images:\n",
        "    new_mask_path = os.path.join(mask_path, 'edited_masks')\n",
        "    os.makedirs(new_mask_path, exist_ok=True)\n",
        "else:\n",
        "    new_mask_path = mask_path\n",
        "# print(new_mask_path)\n",
        "for mask_name in mask_names:\n",
        "    # print(mask_name)\n",
        "    mask_path_full = os.path.join(mask_path, mask_name)\n",
        "    # print(mask_path)\n",
        "    # mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    edited_mask = remove_fenestrations(mask_path_full, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    # print(os.path.join(new_mask_path, mask_name))\n",
        "    cv.imwrite(os.path.join(new_mask_path, mask_name), edited_mask)\n",
        "    # cv.imwrite(os.path.join(new_mask_path, mask_name), mask)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Display the number of circles and their fitted ellipses\n",
        "# print(\"Number of fenestrations:\", len(contours))\n",
        "# print(\"Number of fitted ellipses:\", len(ellipses))\n"
      ],
      "metadata": {
        "id": "OqohEo7UGmuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show mask statistics"
      ],
      "metadata": {
        "id": "9uiSBYduHBsI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTp5_8AlHBW4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}