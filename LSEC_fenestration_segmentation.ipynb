{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marketakvasova/LSEC_segmentation/blob/main/LSEC_fenestration_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Automatic segmentation of electron microscope images**\n",
        "---"
      ],
      "metadata": {
        "id": "Lyx5iPnyA82x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is intended for segmenting fenestrations in SEM images of Liver sinusoidal entdothelial cells (LSECs).\n",
        "You can run it either in Colab, or download it and run it on your PC (in VS Code for example).\n",
        "If you want to run this on your pc, follow the steps described here: https://github.com/marketakvasova/LSEC_segmentation\n",
        "\n",
        "Download the model weights from here: https://drive.google.com/drive/folders/18O8pFbqFLx34X1dliWbPf9EkqeFO0ASK and save them on your Google Drive or on your pc.\n",
        "\n",
        "I you are using Colab, you can connect to a GPU in Runtime > Change runtime type > Hardware accelerator\n",
        "(If connecting to a GPU is not possible, you can use a CPU, it is just ~10x slower.)"
      ],
      "metadata": {
        "id": "tnKtiKypBF_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Run sections 1 and 2 to load the necessary functions and then edit the parameters in the following sections and run them.\n",
        "\n",
        "(The cell runs when you click on the arrow on the left side of the cell.)"
      ],
      "metadata": {
        "id": "PGT9r__uaNhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Import necessary libraries and connect to Drive if you are in Colab**"
      ],
      "metadata": {
        "id": "msPAIMAaZwg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Import necessary libraries and connect to Drive if you are in Colab**\n",
        "#@markdown In Colab a popup window will appear to connect to Google Drive.\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    %pip install segmentation-models-pytorch\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "from segmentation_models_pytorch import Unet\n",
        "import os\n",
        "import torch.cuda\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2 as cv\n",
        "import math\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f'Running on {DEVICE}.')"
      ],
      "metadata": {
        "id": "APb3jJ72BTfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Load necessary functions**\n",
        "---"
      ],
      "metadata": {
        "id": "6MS6VgDyCDfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Load necessary functions**\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = sorted([f for f in os.listdir(self.image_dir) if os.path.isfile(os.path.join(self.image_dir, f))])\n",
        "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if os.path.isfile(os.path.join(self.mask_dir, f))])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = os.path.join(self.image_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[index]) # mask and image need to be called the same\n",
        "        image = cv.imread(img_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE).astype(np.float32)\n",
        "        # mask /= 255\n",
        "        mask[mask == 255.0] = 1\n",
        "\n",
        "        augmentations = self.transform(image=image, mask=mask)\n",
        "        image = augmentations[\"image\"]\n",
        "        mask = augmentations[\"mask\"]\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "def normalize_hist(img):\n",
        "    clahe = cv.createCLAHE(10, tileGridSize=(11, 11))\n",
        "    img = clahe.apply(img)\n",
        "    img = cv.medianBlur(img, 3)\n",
        "    return img\n",
        "\n",
        "test_transform = A.Compose(\n",
        "    [\n",
        "        A.Normalize(\n",
        "        mean = 0.5,\n",
        "        std = 0.5,\n",
        "        max_pixel_value=255.0,\n",
        "        ),\n",
        "            ToTensorV2()\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def create_weighting_patches(patch_size, edge_size):\n",
        "    patch = np.ones((patch_size, patch_size), dtype=float)\n",
        "\n",
        "    # Calculate the linear decrease values\n",
        "    decrease_values = np.linspace(1, 0, num=edge_size)\n",
        "    decrease_values = np.tile(decrease_values, (patch_size, 1))\n",
        "    increase_values = np.linspace(0, 1, num=edge_size)\n",
        "    increase_values = np.tile(increase_values, (patch_size, 1))\n",
        "\n",
        "    # Middle patch\n",
        "    # Apply linear decrease to all four edges\n",
        "    middle = patch.copy()\n",
        "    middle[:, 0:edge_size] *= increase_values\n",
        "    middle[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    middle[0:edge_size, :] *= increase_values.T\n",
        "    middle[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Left\n",
        "    left = patch.copy()\n",
        "    left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    left[0:edge_size, :] *= increase_values.T\n",
        "    left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Right\n",
        "    right = patch.copy()\n",
        "    right[:, 0:edge_size] *= increase_values\n",
        "    right[0:edge_size, :] *= increase_values.T\n",
        "    right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Top\n",
        "    top = patch.copy()\n",
        "    top[:, 0:edge_size] *= increase_values\n",
        "    top[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Bottom\n",
        "    bottom = patch.copy()\n",
        "    bottom[:, 0:edge_size] *= increase_values\n",
        "    bottom[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom[0:edge_size, :] *= increase_values.T\n",
        "\n",
        "    # Left Top edge\n",
        "    top_left = patch.copy()\n",
        "    top_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    top_left[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Right top edge\n",
        "    top_right = patch.copy()\n",
        "    top_right[:, 0:edge_size] *= increase_values\n",
        "    top_right[patch_size-edge_size:patch_size, :] *= decrease_values.T\n",
        "\n",
        "    # Left bottom edge\n",
        "    bottom_left = patch.copy()\n",
        "    bottom_left[:, patch_size-edge_size:patch_size] *= decrease_values\n",
        "    bottom_left[0:edge_size, :] *= increase_values.T\n",
        "\n",
        "    # Right Bottom edge\n",
        "    bottom_right = patch.copy()\n",
        "    bottom_right[:, 0:edge_size] *= increase_values\n",
        "    bottom_right[0:edge_size, :] *= increase_values.T\n",
        "\n",
        "    return middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left\n",
        "\n",
        "\n",
        "def add_mirrored_border(image, border_size, window_size):\n",
        "    height, width = image.shape\n",
        "\n",
        "    bottom_edge = window_size - ((height + border_size) % (window_size - border_size))\n",
        "    right_edge = window_size - ((width + border_size) % (window_size - border_size))\n",
        "\n",
        "    top_border = np.flipud(image[0:border_size, :])\n",
        "    bottom_border = np.flipud(image[height - (border_size+bottom_edge):height, :])\n",
        "    top_bottom_mirrored = np.vstack((top_border, image, bottom_border))\n",
        "\n",
        "    left_border = np.fliplr(top_bottom_mirrored[:, 0:border_size])\n",
        "    right_border = np.fliplr(top_bottom_mirrored[:, width - (border_size+right_edge):width])\n",
        "    mirrored_image = np.hstack((left_border, top_bottom_mirrored, right_border))\n",
        "    return mirrored_image\n",
        "\n",
        "def inference_on_image_with_overlap(model, image_path):\n",
        "    window_size = 224\n",
        "    oh, ow = 20, 20\n",
        "\n",
        "    input_image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    image_height, image_width = input_image.shape\n",
        "    original_height, original_width = image_height, image_width\n",
        "\n",
        "\n",
        "    mirrored_image = add_mirrored_border(input_image, oh, window_size)\n",
        "    image_height, image_width = mirrored_image.shape\n",
        "\n",
        "\n",
        "    weights = np.zeros((image_height, image_width))\n",
        "    output_probs = np.zeros((image_height, image_width))\n",
        "    output_mask = np.zeros((image_height, image_width))\n",
        "    middle, top_left, top, top_right, right, bottom_right, bottom, bottom_left, left = create_weighting_patches(window_size, oh)\n",
        "\n",
        "    for x in range(0, image_height-window_size+1, window_size - oh):\n",
        "        for y in range(0, image_width-window_size+1, window_size - ow):\n",
        "            # Choose weighting window\n",
        "\n",
        "            if x == 0:\n",
        "                if y == 0:\n",
        "                    weighting_window = top_left\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = top_right\n",
        "                else:\n",
        "                    weighting_window = top\n",
        "            elif x == image_height - window_size:\n",
        "                if y == 0:\n",
        "                    weighting_window = bottom_left\n",
        "                elif y == image_width - window_size:\n",
        "                    weighting_window = bottom_right\n",
        "                else:\n",
        "                    weighting_window = bottom\n",
        "            elif y == 0:\n",
        "                weighting_window = left\n",
        "            elif y == image_width - window_size:\n",
        "                weighting_window = right\n",
        "            else:\n",
        "                weighting_window = middle\n",
        "            square_section = mirrored_image[x:x + window_size, y:y + window_size]\n",
        "            weights[x:x + window_size, y:y + window_size] += weighting_window\n",
        "            square_section = normalize_hist(square_section)\n",
        "            square_tensor = test_transform(image=square_section)['image'].unsqueeze(0).to(DEVICE)  # Add batch and channel dimension\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = torch.sigmoid(model(square_tensor)).float()\n",
        "\n",
        "            # Scale the probablity to 0-255\n",
        "            output = output*255\n",
        "            output_pil = output.squeeze(0).cpu().numpy().squeeze()\n",
        "            output_probs[x:x+window_size, y:y+window_size] += output_pil*weighting_window\n",
        "\n",
        "    output_probs = output_probs[oh:original_height+oh, ow:original_width+ow]\n",
        "    weights *= 255\n",
        "\n",
        "    threshold = int(255*0.4)\n",
        "    output_mask = np.where(output_probs > threshold, 255, 0)\n",
        "    output_mask = output_mask.astype(np.uint8)\n",
        "    return output_mask\n",
        "\n",
        "def build_model(model_name):\n",
        "    in_channels = 1\n",
        "    out_channels = 1\n",
        "    model = Unet(\n",
        "            encoder_name=model_name,\n",
        "            encoder_weights=None,\n",
        "            in_channels=in_channels,\n",
        "            classes=out_channels,\n",
        "            activation=None,).to(DEVICE)\n",
        "    return model\n",
        "\n",
        "\n",
        "def remove_contour_from_mask(contour, mask):\n",
        "    # Fill the contour with black pixels\n",
        "    cv.drawContours(mask, [contour], -1, 0, thickness=cv.FILLED)\n",
        "    return mask\n",
        "\n",
        "def remove_fenestrations(mask, min_d, max_d, min_roundness, pixel_size_nm):\n",
        "    contours, _ = cv.findContours(mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    fenestration_areas = [cv.contourArea(cnt) * (pixel_size_nm**2) for cnt in contours]\n",
        "    contour_centers = find_contour_centers(contours)\n",
        "    ellipses, num_ellipses = fit_ellipses(contours, contour_centers)\n",
        "    roundness_of_ellipses = []\n",
        "    equivalent_diameters = []\n",
        "    fenestration_areas_from_ellipses = []\n",
        "\n",
        "    for contour, ellipse in zip(contours, ellipses):\n",
        "        if ellipse != (None, None, None) and ellipse is not None:\n",
        "            center, axes, _ = ellipse\n",
        "            minor_axis_length, major_axis_length = axes\n",
        "            if major_axis_length != 0 and major_axis_length < 20*minor_axis_length:\n",
        "                roundness = minor_axis_length/major_axis_length\n",
        "                if roundness >= min_roundness:\n",
        "                    roundness_of_ellipses.append(roundness)\n",
        "                diameter = pixel_size_nm * equivalent_circle_diameter(major_axis_length, minor_axis_length)\n",
        "\n",
        "                if (diameter < min_d or diameter > max_d) or  (roundness < min_roundness) or np.isnan(diameter):\n",
        "                    mask = remove_contour_from_mask(contour, mask)\n",
        "                else:\n",
        "                    equivalent_diameters.append(diameter)\n",
        "                    fenestration_areas_from_ellipses.append((diameter**2)/4*math.pi)\n",
        "            else:\n",
        "                mask = remove_contour_from_mask(contour, mask)\n",
        "        else:\n",
        "            mask = remove_contour_from_mask(contour, mask)\n",
        "    return mask\n",
        "\n",
        "\n",
        "\n",
        "def fit_ellipses(filtered_contours, centers):\n",
        "    ellipses = []\n",
        "    num_ellipses = 0\n",
        "    for contour, cnt_center in zip(filtered_contours, centers):\n",
        "        if len(contour) >= 5:  # Ellipse fitting requires at least 5 points\n",
        "            ellipse = cv.fitEllipse(contour) # TODO: maybe try a different computation, if this does not work well on edges (probably ok)\n",
        "            dist = cv.norm(cnt_center, ellipse[0])\n",
        "            if dist < 20:\n",
        "                ellipses.append(ellipse)\n",
        "                num_ellipses += 1\n",
        "            else:\n",
        "                ellipses.append((None, None, None))\n",
        "        else:\n",
        "            ellipses.append((None, None, None))\n",
        "    # print(len(filtered_contours), len(ellipses))\n",
        "    return ellipses, num_ellipses\n",
        "\n",
        "def find_fenestration_contours(image_path):\n",
        "    seg_mask = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    contours, _ = cv.findContours(seg_mask, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "    return contours\n",
        "\n",
        "def find_contour_centers(contours):\n",
        "    contour_centers = []\n",
        "    for cnt in contours:\n",
        "        M = cv.moments(cnt)\n",
        "        center_x = int(M['m10'] / (M['m00'] + 1e-10))\n",
        "        center_y = int(M['m01'] / (M['m00'] + 1e-10))\n",
        "        contour_centers.append((center_x, center_y))\n",
        "    return contour_centers\n",
        "\n",
        "def equivalent_circle_diameter(major_axis_length, minor_axis_length):\n",
        "    return math.sqrt(major_axis_length * minor_axis_length)\n",
        "\n"
      ],
      "metadata": {
        "id": "y8BfGPazDVzo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Insert input and output folders and the path of model weights**\n",
        "---"
      ],
      "metadata": {
        "id": "CYjQKLjXBRrH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown All Google Drive paths should start with ./gdrive/MyDrive/ (Check the folder structure in the left sidebar under **Files**).\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# |                       CHANGE THESE PARAMETERS                             |#\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "#!!! If running locally on Windows, use / or \\\\ as folder separator !!! (not \\)\n",
        "\n",
        "#@markdown Insert folder containing LSEC images:\n",
        "input_folder = './gdrive/MyDrive/lsecs/images' #@param {type:\"string\"}\n",
        "#@markdown Insert where to save the output masks\n",
        "#@markdown (the folder will be created if it does not exist yet)\n",
        "#@markdown If the folder contains images, they may be overwritten:\n",
        "output_folder = './gdrive/MyDrive/lsecs/my_masks' #@param {type:\"string\"}\n",
        "#@markdown Insert model weights path:\n",
        "model_path = './gdrive/MyDrive/lsecs/model_weights.pth' #@param {type:\"string\"}\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "model_path = model_path.strip()\n",
        "input_folder = input_folder.strip()\n",
        "output_folder = output_folder.strip()\n",
        "\n",
        "\n",
        "model = build_model('resnet34')\n",
        "if torch.cuda.is_available():\n",
        "    loaded_state_dict = torch.load(model_path) # TODO this is without sigmoid, it is applied in the inference loop\n",
        "    model.load_state_dict(loaded_state_dict)\n",
        "\n",
        "else:\n",
        "    loaded_state_dict = torch.load(model_path, map_location=torch.device('cpu')) # TODO this is without sigmoid, it is applied in the inference loop\n",
        "    model.load_state_dict(loaded_state_dict)\n",
        "model.eval()\n",
        "\n",
        "\n",
        "if not os.path.exists(input_folder):\n",
        "    print(\"Input folder does not exist\")\n",
        "    # exit()\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f'Created folder {output_folder}')\n",
        "\n"
      ],
      "metadata": {
        "id": "sTqRswq5BJwb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Run image segmentation**\n",
        "---"
      ],
      "metadata": {
        "id": "K3QMfzlNFa9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Run this cell to segment images:**\n",
        "#@markdown You can choose, if you want to remove objects from the masks based on their parameters.\n",
        "\n",
        "\n",
        "#@markdown If the remove_fenestrations_based_on_params box is not checked, no objects will be removed from the segmented masks.\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# |                       CHANGE THESE PARAMETERS                             |#\n",
        "# -----------------------------------------------------------------------------#\n",
        "# If this is False, no fenestrations will be removed\n",
        "# If this is True, fenestrations will be removed based on the following parameters\n",
        "remove_fenestrations_based_on_params = False # @param {type:\"boolean\"}\n",
        "pixel_size_nm = 9.28 #@param {type:\"number\"}\n",
        "min_diameter_nm = 50 #@param {type:\"number\"}\n",
        "max_diameter_nm = 350 #@param {type:\"number\"}\n",
        "min_roundness = 0.4 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Roundness is computed as minor axis length/major axis length of a fitted ellipse.\n",
        "image_names = [f for f in sorted(os.listdir(input_folder)) if os.path.isfile(os.path.join(input_folder, f))]\n",
        "for image_name in image_names:\n",
        "    print(image_name)\n",
        "    image_path = os.path.join(input_folder, image_name)\n",
        "    out_mask = inference_on_image_with_overlap(model, image_path)\n",
        "    if remove_fenestrations_based_on_params:\n",
        "        out_mask = remove_fenestrations(out_mask, min_diameter_nm, max_diameter_nm, min_roundness, pixel_size_nm)\n",
        "    filename_ext = os.path.basename(image_name)\n",
        "    filename, ext = os.path.splitext(filename_ext)\n",
        "    out = os.path.join(output_folder, filename+'_mask'+ext)\n",
        "    cv.imwrite(out, out_mask)\n",
        "    print(f'Saving {out}')"
      ],
      "metadata": {
        "id": "OSv_W-oXFaKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Apply cell masks**\n",
        "---"
      ],
      "metadata": {
        "id": "XxPB1woAkX51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  { display-mode: \"form\" }\n",
        "#@markdown ##**Insert folder with cell masks:**\n",
        "#@markdown You can apply cell masks on the segmented masks, if you have them.\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------#\n",
        "# |                       CHANGE THESE PARAMETERS                             |#\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "#!!! If running locally on Windows, use / or \\\\ as folder separator !!! (not \\)\n",
        "\n",
        "cell_masks = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "#@markdown If you want to replace the old masks, check this box. If not, write the new output folder into **new_output_folder**.\n",
        "# If this is True, the masks will be rewritten\n",
        "# If this is False, write where to save the mask into new_output_folder\n",
        "rewrite_old_masks = False # @param {type:\"boolean\"}\n",
        "new_output_folder = './gdrive/MyDrive/' #@param {type:\"string\"}\n",
        "# -----------------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "\n",
        "cell_masks = cell_masks.strip()\n",
        "new_output_folder = new_output_folder.strip()\n",
        "\n",
        "image_names = [f for f in sorted(os.listdir(output_folder)) if os.path.isfile(os.path.join(output_folder, f))]\n",
        "mask_names = [f for f in sorted(os.listdir(cell_masks)) if os.path.isfile(os.path.join(cell_masks, f))]\n",
        "\n",
        "def apply_cell_mask(image_path, mask_path):\n",
        "    image = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
        "    cell_mask = cv.imread(mask_path, cv.IMREAD_GRAYSCALE)\n",
        "    image[cell_mask == 0] = 0\n",
        "    return image\n",
        "\n",
        "for image_name, mask_name in zip(image_names, mask_names):\n",
        "    print(f'{image_name} - {mask_name}')\n",
        "    image_path = os.path.join(output_folder, image_name)\n",
        "    mask_path = os.path.join(cell_masks, mask_name)\n",
        "    image_with_cell_mask = apply_cell_mask(image_path, mask_path)\n",
        "    if rewrite_old_masks:\n",
        "        cv.imwrite(output_folder, image_with_cell_mask)\n",
        "    else:\n",
        "        cv.imwrite(new_output_folder, image_with_cell_mask)\n"
      ],
      "metadata": {
        "id": "KtYndKbUkvs5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}